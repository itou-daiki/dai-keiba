"""
éå»3å¹´åˆ†ã®å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°

ç›®çš„: JRAãƒ‡ãƒ¼ã‚¿ã‚’656ä»¶â†’5000ä»¶ä»¥ä¸Šã«å¢—ã‚„ã™
æœŸé–“: 2023å¹´1æœˆ1æ—¥ - 2025å¹´12æœˆ31æ—¥
ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: netkeiba.com (JRAç«¶é¦¬)

ä½¿ç”¨æ–¹æ³•:
    python ml/scrape_historical_data.py [--start YYYY-MM-DD] [--end YYYY-MM-DD] [--dry-run]

    --start: é–‹å§‹æ—¥ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2023-01-01ï¼‰
    --end: çµ‚äº†æ—¥ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ä»Šæ—¥ï¼‰
    --dry-run: ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆå®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãªã—ï¼‰
"""

import os
import sys
from datetime import datetime, timedelta
import logging
import pandas as pd

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from scraper.auto_scraper import scrape_race_data

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ml/historical_scraping.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Constants
CSV_FILE_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), "database.csv")
TARGET_RECORDS = 5000  # ç›®æ¨™ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°


def get_existing_race_ids():
    """æ—¢å­˜ã®race_idã‚’å–å¾—ã—ã¦é‡è¤‡ã‚’é¿ã‘ã‚‹"""
    if not os.path.exists(CSV_FILE_PATH):
        return set()

    try:
        df = pd.read_csv(CSV_FILE_PATH)
        if 'race_id' in df.columns:
            return set(df['race_id'].astype(str))
    except Exception as e:
        logger.warning(f"Failed to load existing race IDs: {e}")

    return set()


def get_current_record_count():
    """ç¾åœ¨ã®ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã‚’å–å¾—"""
    if not os.path.exists(CSV_FILE_PATH):
        return 0

    try:
        df = pd.read_csv(CSV_FILE_PATH)
        return len(df)
    except:
        return 0


def save_race_data(df_new):
    """ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆè¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ï¼‰"""
    if df_new is None or df_new.empty:
        return

    if os.path.exists(CSV_FILE_PATH):
        try:
            existing_df = pd.read_csv(CSV_FILE_PATH)
            combined_df = pd.concat([existing_df, df_new], ignore_index=True)
        except Exception as e:
            logger.warning(f"Failed to read existing CSV: {e}")
            combined_df = df_new
    else:
        combined_df = df_new

    # é‡è¤‡æ’é™¤ (race_id + é¦¬å)
    subset_cols = ['race_id', 'é¦¬å']
    subset_cols = [c for c in subset_cols if c in combined_df.columns]

    if subset_cols:
        before_count = len(combined_df)
        combined_df.drop_duplicates(subset=subset_cols, keep='last', inplace=True)
        after_count = len(combined_df)
        if before_count != after_count:
            logger.info(f"  Removed {before_count - after_count} duplicates")

    # æ—¥ä»˜ã‚½ãƒ¼ãƒˆ
    try:
        combined_df['date_obj'] = pd.to_datetime(combined_df['æ—¥ä»˜'], format='%Yå¹´%mæœˆ%dæ—¥', errors='coerce')
        combined_df = combined_df.sort_values(['date_obj', 'race_id'])
        combined_df = combined_df.drop(columns=['date_obj'])
    except:
        pass

    combined_df.to_csv(CSV_FILE_PATH, index=False, encoding="utf-8-sig")
    logger.info(f"  Saved {len(df_new)} new rows. Total: {len(combined_df)}")

    return len(combined_df)


def scrape_historical_jra_data(start_date, end_date, dry_run=False):
    """
    JRAã®å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°

    Args:
        start_date: é–‹å§‹æ—¥ (datetime)
        end_date: çµ‚äº†æ—¥ (datetime)
        dry_run: ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆå®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãªã—ï¼‰

    æˆ¦ç•¥:
        - JRAã®10ç«¶é¦¬å ´ã‚’å…¨ã¦ã‚«ãƒãƒ¼
        - å„å¹´ã”ã¨ã«é–‹å‚¬å›ãƒ»æ—¥æ•°ã‚’æ¢ç´¢
        - æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ—
        - ç›®æ¨™5000ãƒ¬ã‚³ãƒ¼ãƒ‰ã«åˆ°é”ã—ãŸã‚‰å®Œäº†
    """
    logger.info("=" * 60)
    logger.info("JRA Historical Data Scraping Started")
    logger.info(f"Target Period: {start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')}")
    logger.info(f"Target Records: {TARGET_RECORDS}")
    logger.info(f"Dry Run: {dry_run}")
    logger.info("=" * 60)

    # ç¾åœ¨ã®ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
    initial_count = get_current_record_count()
    logger.info(f"Current records: {initial_count}")

    if initial_count >= TARGET_RECORDS:
        logger.info(f"âœ… Already reached target ({initial_count} >= {TARGET_RECORDS})")
        return

    # æ—¢å­˜ã®race_idã‚’å–å¾—
    existing_ids = get_existing_race_ids()
    logger.info(f"Existing race IDs: {len(existing_ids)}")

    # JRA 10ç«¶é¦¬å ´
    # Place codes: 01=æœ­å¹Œ, 02=å‡½é¤¨, 03=ç¦å³¶, 04=æ–°æ½Ÿ, 05=æ±äº¬,
    #              06=ä¸­å±±, 07=ä¸­äº¬, 08=äº¬éƒ½, 09=é˜ªç¥, 10=å°å€‰
    places = range(1, 11)

    # æ¢ç´¢ç¯„å›²
    years = range(start_date.year, end_date.year + 1)
    kais = range(1, 7)      # é–‹å‚¬å› (é€šå¸¸1-6å›)
    days = range(1, 13)     # é–‹å‚¬æ—¥æ•° (é€šå¸¸1-12æ—¥)
    races = range(1, 13)    # ãƒ¬ãƒ¼ã‚¹ç•ªå· (1-12R)

    total_scraped = 0
    total_skipped = 0
    total_errors = 0

    import time

    for year in years:
        logger.info(f"\n{'='*60}")
        logger.info(f"Year: {year}")
        logger.info(f"{'='*60}")

        for place in places:
            place_name_map = {
                1: "æœ­å¹Œ", 2: "å‡½é¤¨", 3: "ç¦å³¶", 4: "æ–°æ½Ÿ", 5: "æ±äº¬",
                6: "ä¸­å±±", 7: "ä¸­äº¬", 8: "äº¬éƒ½", 9: "é˜ªç¥", 10: "å°å€‰"
            }
            place_name = place_name_map.get(place, f"Place{place}")

            logger.info(f"\n{place_name} ({place:02})")

            for kai in kais:
                for day in days:
                    # 1Rã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦é–‹å‚¬æ—¥ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                    check_race_id = f"{year}{place:02}{kai:02}{day:02}01"

                    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
                    if check_race_id in existing_ids:
                        total_skipped += 1
                        continue

                    # Dry runãƒ¢ãƒ¼ãƒ‰
                    if dry_run:
                        logger.info(f"  [DRY RUN] Would scrape: {check_race_id}")
                        continue

                    logger.info(f"  Checking: {check_race_id} ({year}/{kai}å›{day}æ—¥) ... ", end="")

                    # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸› (0.5ç§’å¾…æ©Ÿ)
                    time.sleep(0.5)

                    try:
                        # 1Rã‚’å–å¾—
                        first_race_df = scrape_race_data(check_race_id, mode="JRA")

                        if first_race_df is None or first_race_df.empty:
                            logger.info("Miss")
                            # 1RãŒãªã„ = ã“ã®é–‹å‚¬æ—¥ã¯ãªã„
                            if day == 1:
                                # 1æ—¥ç›®ãŒãªã„ = ã“ã®é–‹å‚¬å›ã¯å­˜åœ¨ã—ãªã„
                                break
                            else:
                                # é€”ä¸­ã®æ—¥ãŒãªã„ = ã“ã®å›ã¯çµ‚äº†
                                break

                        # é–‹å‚¬æ—¥ãŒå­˜åœ¨ã™ã‚‹
                        race_date_str = first_race_df.iloc[0]["æ—¥ä»˜"]

                        try:
                            race_date = datetime.strptime(race_date_str, '%Yå¹´%mæœˆ%dæ—¥')
                        except:
                            race_date = datetime(year, 1, 1)

                        # å¯¾è±¡æœŸé–“ãƒã‚§ãƒƒã‚¯
                        if race_date < start_date:
                            logger.info(f"Skip (Old: {race_date_str})")
                            continue

                        if race_date > end_date:
                            logger.info(f"Skip (Future: {race_date_str})")
                            break

                        logger.info(f"Hit! ({race_date_str})")

                        # 1Rã‚’ä¿å­˜
                        total_count = save_race_data(first_race_df)
                        total_scraped += len(first_race_df)
                        existing_ids.add(check_race_id)

                        # 2R-12Rã‚’å–å¾—
                        for r in range(2, 13):
                            race_id = f"{year}{place:02}{kai:02}{day:02}{r:02}"

                            if race_id in existing_ids:
                                total_skipped += 1
                                continue

                            time.sleep(0.5)

                            try:
                                df = scrape_race_data(race_id, mode="JRA")
                                if df is not None and not df.empty:
                                    total_count = save_race_data(df)
                                    total_scraped += len(df)
                                    existing_ids.add(race_id)
                                    logger.info(f"    {r}R OK", end=" ")
                                else:
                                    # ã“ã®ãƒ¬ãƒ¼ã‚¹ã¯ãªã„ï¼ˆé–‹å‚¬çµ‚äº†ï¼‰
                                    break
                            except Exception as e:
                                logger.error(f"    {r}R Error: {e}")
                                total_errors += 1
                                break

                        logger.info("")

                        # ç›®æ¨™é”æˆãƒã‚§ãƒƒã‚¯
                        if total_count >= TARGET_RECORDS:
                            logger.info(f"\nğŸ‰ Target reached! {total_count} >= {TARGET_RECORDS}")
                            logger.info(f"Total scraped: {total_scraped} records")
                            logger.info(f"Skipped: {total_skipped}, Errors: {total_errors}")
                            return

                        # é€²æ—è¡¨ç¤º
                        if total_scraped > 0 and total_scraped % 100 == 0:
                            logger.info(f"\nğŸ“Š Progress: {total_count}/{TARGET_RECORDS} records ({total_count/TARGET_RECORDS*100:.1f}%)")

                    except Exception as e:
                        logger.error(f"Error: {e}")
                        total_errors += 1

    # æœ€çµ‚çµæœ
    final_count = get_current_record_count()
    logger.info("\n" + "=" * 60)
    logger.info("Scraping Completed")
    logger.info(f"Initial records: {initial_count}")
    logger.info(f"Final records: {final_count}")
    logger.info(f"New records: {final_count - initial_count}")
    logger.info(f"Total scraped: {total_scraped}")
    logger.info(f"Skipped: {total_skipped}")
    logger.info(f"Errors: {total_errors}")
    logger.info(f"Target: {TARGET_RECORDS}")

    if final_count >= TARGET_RECORDS:
        logger.info(f"âœ… Target reached!")
    else:
        logger.warning(f"âš ï¸ Target not reached. Need {TARGET_RECORDS - final_count} more records.")

    logger.info("=" * 60)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='JRAéå»3å¹´åˆ†ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°')
    parser.add_argument('--start', type=str, default='2023-01-01',
                       help='é–‹å§‹æ—¥ (YYYY-MM-DD, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2023-01-01)')
    parser.add_argument('--end', type=str, default=None,
                       help='çµ‚äº†æ—¥ (YYYY-MM-DD, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ä»Šæ—¥)')
    parser.add_argument('--dry-run', action='store_true',
                       help='ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆå®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãªã—ï¼‰')

    args = parser.parse_args()

    # Parse dates
    try:
        start_date = datetime.strptime(args.start, '%Y-%m-%d')
    except ValueError:
        logger.error(f"Invalid start date format: {args.start}. Use YYYY-MM-DD")
        sys.exit(1)

    if args.end:
        try:
            end_date = datetime.strptime(args.end, '%Y-%m-%d')
        except ValueError:
            logger.error(f"Invalid end date format: {args.end}. Use YYYY-MM-DD")
            sys.exit(1)
    else:
        end_date = datetime.now()

    # Validate date range
    if start_date > end_date:
        logger.error(f"Start date ({start_date}) is after end date ({end_date})")
        sys.exit(1)

    # Run scraping
    scrape_historical_jra_data(start_date, end_date, dry_run=args.dry_run)

    print("\nâœ… Historical data scraping script completed.")
    print(f"ğŸ“ Data saved to: {CSV_FILE_PATH}")
    print(f"ğŸ“‹ Log saved to: ml/historical_scraping.log")
