"""
ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ›²ç·šã®å¯è¦–åŒ–

ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡ãŒå®Ÿéš›ã®ç¢ºç‡ã¨ã©ã‚Œã ã‘ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
import os
from sklearn.calibration import calibration_curve
from sklearn.metrics import brier_score_loss
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def plot_calibration_curve(y_true, y_pred_proba, model_name="Model", n_bins=10, save_path=None):
    """
    ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆ

    Args:
        y_true: å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«
        y_pred_proba: äºˆæ¸¬ç¢ºç‡
        model_name: ãƒ¢ãƒ‡ãƒ«å
        n_bins: ãƒ“ãƒ³æ•°
        save_path: ä¿å­˜å…ˆãƒ‘ã‚¹

    Returns:
        matplotlib figure
    """
    # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ›²ç·šã®è¨ˆç®—
    fraction_of_positives, mean_predicted_value = calibration_curve(
        y_true, y_pred_proba, n_bins=n_bins, strategy='uniform'
    )

    # Brier Scoreã®è¨ˆç®—
    brier_score = brier_score_loss(y_true, y_pred_proba)

    # ãƒ—ãƒ­ãƒƒãƒˆ
    fig, ax = plt.subplots(figsize=(10, 10))

    # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ›²ç·š
    ax.plot(mean_predicted_value, fraction_of_positives, marker='o', linewidth=2,
            label=f'{model_name} (Brier: {brier_score:.4f})')

    # å®Œå…¨ã«ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ï¼ˆå¯¾è§’ç·šï¼‰
    ax.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')

    ax.set_xlabel('Mean Predicted Probability', fontsize=14)
    ax.set_ylabel('Fraction of Positives (True Probability)', fontsize=14)
    ax.set_title(f'Calibration Plot - {model_name}', fontsize=16)
    ax.legend(loc='upper left', fontsize=12)
    ax.grid(True, alpha=0.3)

    # å¯¾è§’ç·šã‹ã‚‰ã®è·é›¢ã‚’å¯è¦–åŒ–
    ax.fill_between(mean_predicted_value, fraction_of_positives, mean_predicted_value,
                     alpha=0.2, color='red', label='Calibration Error')

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        logger.info(f"Calibration plot saved to {save_path}")

    return fig


def evaluate_model_calibration(model_path, data_path, target_col='target_win', output_dir='ml/visualizations'):
    """
    ãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è©•ä¾¡ã—ã¦å¯è¦–åŒ–

    Args:
        model_path: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        data_path: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹
        target_col: ç›®æ¨™å¤‰æ•°ã®ã‚«ãƒ©ãƒ å
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

    Returns:
        calibration results dict
    """
    logger.info(f"Loading model from {model_path}")
    with open(model_path, 'rb') as f:
        model = pickle.load(f)

    logger.info(f"Loading data from {data_path}")
    df = pd.read_csv(data_path)

    if target_col not in df.columns:
        logger.error(f"Target column '{target_col}' not found")
        return None

    # ç‰¹å¾´é‡ã®æº–å‚™
    meta_cols = ['é¦¬å', 'horse_id', 'æ ', 'é¦¬ ç•ª', 'race_id', 'date', 'rank', 'ç€ é †']
    exclude_cols = ['target_top3', 'target_win', 'target_show']
    drop_cols = [c for c in df.columns if c in meta_cols or c in exclude_cols]

    X = df.drop(columns=drop_cols, errors='ignore').select_dtypes(include=['number'])
    y = df[target_col]

    # äºˆæ¸¬
    logger.info("Generating predictions...")
    y_pred_proba = model.predict(X)

    # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡
    logger.info("Evaluating calibration...")
    brier_score = brier_score_loss(y, y_pred_proba)

    # ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
    os.makedirs(output_dir, exist_ok=True)
    model_name = os.path.basename(model_path).replace('.pkl', '')
    save_path = os.path.join(output_dir, f'{model_name}_calibration.png')

    fig = plot_calibration_curve(y, y_pred_proba, model_name=model_name, save_path=save_path)

    # ãƒ“ãƒ³åˆ¥ã®è©³ç´°çµ±è¨ˆ
    logger.info("\n=== Calibration Details (by bin) ===")
    fraction_of_positives, mean_predicted_value = calibration_curve(y, y_pred_proba, n_bins=10)

    for i, (pred_prob, true_prob) in enumerate(zip(mean_predicted_value, fraction_of_positives)):
        error = abs(true_prob - pred_prob)
        logger.info(f"Bin {i+1}: Predicted={pred_prob:.3f}, Actual={true_prob:.3f}, Error={error:.3f}")

    results = {
        'brier_score': brier_score,
        'calibration_curve': (fraction_of_positives, mean_predicted_value),
        'plot_path': save_path
    }

    logger.info(f"\n=== Overall Calibration ===")
    logger.info(f"Brier Score: {brier_score:.4f} (lower is better, 0=perfect)")

    return results


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='ãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡')
    parser.add_argument('--model', default='ml/models/lgbm_model.pkl', help='ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--data', default='ml/processed_data.csv', help='ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--target', default='target_win', help='ç›®æ¨™å¤‰æ•°ã‚«ãƒ©ãƒ å')
    parser.add_argument('--output', default='ml/visualizations', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')

    args = parser.parse_args()

    results = evaluate_model_calibration(
        model_path=args.model,
        data_path=args.data,
        target_col=args.target,
        output_dir=args.output
    )

    if results:
        print(f"\nâœ… Calibration plot saved to: {results['plot_path']}")
        print(f"ğŸ“Š Brier Score: {results['brier_score']:.4f}")
