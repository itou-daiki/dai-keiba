{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 欠損データ補完スクリプト (Colab用)\n",
                "\n",
                "このノートブックは、`database.csv` や `database_nar.csv` の空のカラム（主に過去のレース成績や血統情報）をnetkeibaからスクレイピングして埋めるためのものです。\n",
                "\n",
                "## 手順\n",
                "1. 左側のファイルアイコンから、補完したい `database.csv` (または `_nar.csv`) をアップロードしてください。\n",
                "2. 以下のセルを順番に実行してください。\n",
                "3. 完了すると、補完されたファイル (例: `filled_database.csv`) が生成されるので、ダウンロードして元のファイルと差し替えてください。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install pandas requests beautifulsoup4 tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import time\n",
                "from tqdm import tqdm\n",
                "import re\n",
                "\n",
                "# === 設定 ===\n",
                "TARGET_FILE = \"database_nar.csv\" # 補完したいファイル名 (適宜変更してください)\n",
                "OUTPUT_FILE = \"filled_database_nar.csv\"\n",
                "\n",
                "# スクレイピングの間隔 (秒) - サーバー負荷軽減のため必ず1秒以上空ける\n",
                "INTERVAL = 1.0 \n",
                "\n",
                "def get_soup(url):\n",
                "    try:\n",
                "        r = requests.get(url)\n",
                "        r.encoding = r.apparent_encoding\n",
                "        time.sleep(INTERVAL)\n",
                "        return BeautifulSoup(r.text, 'html.parser')\n",
                "    except Exception as e:\n",
                "        print(f\"Error fetching {url}: {e}\")\n",
                "        return None\n",
                "\n",
                "def scrape_horse_detail(horse_id):\n",
                "    \"\"\"\n",
                "    馬のプロフィールページから血統と過去レース情報を取得\n",
                "    \"\"\"\n",
                "    url = f\"https://db.netkeiba.com/horse/{horse_id}\"\n",
                "    soup = get_soup(url)\n",
                "    if not soup:\n",
                "        return None\n",
                "\n",
                "    data = {}\n",
                "\n",
                "    # 1. 血統 (Pedigree)\n",
                "    # father: .blood_table tr:nth-child(1) td:nth-child(1)\n",
                "    try:\n",
                "        blood_table = soup.select_one(\".blood_table\")\n",
                "        if blood_table:\n",
                "            # 父\n",
                "            father_tag = blood_table.select_one(\"tr:nth-child(1) td:nth-child(1) a\")\n",
                "            if father_tag: data['father'] = father_tag.text.strip()\n",
                "            \n",
                "            # 母\n",
                "            mother_tag = blood_table.select_one(\"tr:nth-child(3) td:nth-child(1) a\")\n",
                "            if mother_tag: data['mother'] = mother_tag.text.strip()\n",
                "            \n",
                "            # 母父 (BMS) - 母の父\n",
                "            bms_tag = blood_table.select_one(\"tr:nth-child(3) td:nth-child(2) a\")\n",
                "            if bms_tag: data['bms'] = bms_tag.text.strip()\n",
                "    except Exception as e:\n",
                "        print(f\"Pedigree error for {horse_id}: {e}\")\n",
                "\n",
                "    # 2. 過去レース (Past Races)\n",
                "    # table class=\"db_h_race_results\"\n",
                "    # 最新5走を取得\n",
                "    try:\n",
                "        hist_table = soup.select_one(\"table.db_h_race_results\")\n",
                "        if hist_table:\n",
                "            rows = hist_table.select(\"tbody tr\")\n",
                "            past_idx = 1\n",
                "            # 時系列降順になっているはず\n",
                "            for row in rows:\n",
                "                if past_idx > 5: break\n",
                "                \n",
                "                # クラスによるフィルタリング等は一旦せず、単純に最近のものを取る\n",
                "                cols = row.select(\"td\")\n",
                "                if len(cols) < 15: continue\n",
                "\n",
                "                # 日付, 開催, ... 着順 ...\n",
                "                # 0:日付, 1:開催, 2:天気, 3:R, 4:レース名, ... 11:着順\n",
                "                try:\n",
                "                    date_str = cols[0].text.strip()\n",
                "                    rank = cols[11].text.strip()\n",
                "                    \n",
                "                    # rankが数字でない(取消など)場合はスキップするか、そのまま入れるか。\n",
                "                    # 学習に使うなら数字が望ましいが、一旦そのまま。\n",
                "                    \n",
                "                    prefix = f\"past_{past_idx}\"\n",
                "                    data[f\"{prefix}_date\"] = date_str\n",
                "                    data[f\"{prefix}_rank\"] = rank\n",
                "                    data[f\"{prefix}_race_name\"] = cols[4].text.strip()\n",
                "                    \n",
                "                    # 距離・馬場 (芝1600 etc)\n",
                "                    dist_raw = cols[14].text.strip() \n",
                "                    # e.g., 芝1600 or ダ1200\n",
                "                    if \"芝\" in dist_raw:\n",
                "                       data[f\"{prefix}_course_type\"] = \"芝\"\n",
                "                       data[f\"{prefix}_distance\"] = getattr(re.search(r'\\d+', dist_raw), 'group', lambda: '')()\n",
                "                    elif \"ダ\" in dist_raw:\n",
                "                       data[f\"{prefix}_course_type\"] = \"ダ\"\n",
                "                       data[f\"{prefix}_distance\"] = getattr(re.search(r'\\d+', dist_raw), 'group', lambda: '')()\n",
                "                    \n",
                "                    data[f\"{prefix}_time\"] = cols[17].text.strip()\n",
                "                    \n",
                "                    # 通過 (Run Style hint)\n",
                "                    pas = cols[20].text.strip()\n",
                "                    # 4角位置を簡易的にrun_styleとする (1なら逃げ、など)\n",
                "                    # 詳しくはfeatures.pyと合わせる必要があるが、簡易的に保存\n",
                "                    \n",
                "                    data[f\"{prefix}_last_3f\"] = cols[22].text.strip()\n",
                "                    data[f\"{prefix}_horse_weight\"] = cols[23].text.strip()\n",
                "\n",
                "                    past_idx += 1\n",
                "                except Exception as ex:\n",
                "                    print(f\"Row parse error: {ex}\")\n",
                "                    continue\n",
                "    except Exception as e:\n",
                "        print(f\"History error for {horse_id}: {e}\")\n",
                "\n",
                "    return data\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# メイン処理\n",
                "try:\n",
                "    df = pd.read_csv(TARGET_FILE)\n",
                "    print(f\"Loaded {len(df)} rows.\")\n",
                "\n",
                "    # 必要なカラムが存在しない場合は作成\n",
                "    cols_to_ensure = ['father', 'mother', 'bms']\n",
                "    for i in range(1, 6):\n",
                "        cols_to_ensure += [\n",
                "            f'past_{i}_date', f'past_{i}_rank', f'past_{i}_race_name', \n",
                "            f'past_{i}_distance', f'past_{i}_course_type', f'past_{i}_time', \n",
                "            f'past_{i}_last_3f', f'past_{i}_horse_weight'\n",
                "        ]\n",
                "    \n",
                "    for c in cols_to_ensure:\n",
                "        if c not in df.columns:\n",
                "            df[c] = None\n",
                "\n",
                "    # 進捗表示用\n",
                "    total_updated = 0\n",
                "    \n",
                "    # horse_idごとにグループ化して処理（同じ馬を何度もスクレイピングしないため）\n",
                "    # しかし、行ごとに異なる時点のデータを求めているわけではなく、馬自体の固定情報(血統)と最新履歴を取るならば\n",
                "    # 重複除去したhorse_idリストを作って辞書にするのが効率的\n",
                "    unique_horse_ids = df['horse_id'].dropna().unique()\n",
                "    print(f\"Unique horses to check: {len(unique_horse_ids)}\")\n",
                "\n",
                "    # 既にデータが埋まっているかチェックするロジックを入れるとさらに良いが、\n",
                "    # ここでは簡易に「まだ辞書にない馬」を取得する\n",
                "    horse_data_cache = {}\n",
                "\n",
                "    for hid in tqdm(unique_horse_ids):\n",
                "        # int float変換ケア\n",
                "        try:\n",
                "            hid_str = str(int(float(hid)))\n",
                "        except:\n",
                "            hid_str = str(hid)\n",
                "        \n",
                "        # Check if we assume fetching is needed (e.g., check first row of this horse in df)\n",
                "        # 全件チェックは重いので、無条件に取得して上書きするスタイルにする（確実性重視）\n",
                "        \n",
                "        scraped_data = scrape_horse_detail(hid_str)\n",
                "        \n",
                "        if scraped_data:\n",
                "            horse_data_cache[hid] = scraped_data\n",
                "            total_updated += 1\n",
                "        \n",
                "        # Colabの制限を避けるため適宜保存などを推奨\n",
                "        if total_updated % 100 == 0:\n",
                "            print(f\"Processed {total_updated} horses...\")\n",
                "\n",
                "    print(\"Applying data to dataframe...\")\n",
                "    \n",
                "    # DataFrameに反映\n",
                "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
                "        hid = row['horse_id']\n",
                "        if pd.isna(hid) or hid not in horse_data_cache:\n",
                "            continue\n",
                "            \n",
                "        h_data = horse_data_cache[hid]\n",
                "        # 血統は常に上書き\n",
                "        if 'father' in h_data: df.at[idx, 'father'] = h_data['father']\n",
                "        if 'mother' in h_data: df.at[idx, 'mother'] = h_data['mother']\n",
                "        if 'bms' in h_data: df.at[idx, 'bms'] = h_data['bms']\n",
                "        \n",
                "        # 過去データ\n",
                "        # 注意: database.csvの各行は「ある時点のレース」なので、\n",
                "        # 本来はそのレース日付より過去の戦績だけを入れるべきです（リーク防止）。\n",
                "        # しかし、この簡易スクリプトでは「最新5走」を取ってきて埋めるため、\n",
                "        # 未来のデータが入る可能性があります。\n",
                "        # **厳密な学習**を行う場合は、ここを「日付比較」するロジックにする必要があります。\n",
                "        \n",
                "        current_date = pd.to_datetime(row['日付'])\n",
                "        \n",
                "        # h_dataの中身 (past_1_date, past_1_rank...) は「最新順」になっている\n",
                "        # これを current_date より古いものだけフィルタして再割り当てする\n",
                "        \n",
                "        valid_past_races = []\n",
                "        # Check up to 10 past races scraped (if we scraped more) - currenly scraped 5\n",
                "        # Scraper function above scrapes top 5. \n",
                "        # Let's verify dates.\n",
                "        \n",
                "        # 簡易実装: 日付チェック付きで埋める\n",
                "        # 一旦 scraped_data からリスト形式でレースを復元\n",
                "        temp_races = []\n",
                "        for i in range(1, 6):\n",
                "            d_key = f\"past_{i}_date\"\n",
                "            if d_key in h_data and h_data[d_key]:\n",
                "                r = {k.replace(f\"past_{i}_\", \"\"): v for k, v in h_data.items() if k.startswith(f\"past_{i}_\")}\n",
                "                temp_races.append(r)\n",
                "        \n",
                "        # 日付比較\n",
                "        # date format in netkeiba usually YYYY/MM/DD, df usually YYYY-MM-DD or YYYY年...\n",
                "        filled_count = 0\n",
                "        for r in temp_races:\n",
                "            try:\n",
                "                # parse race date\n",
                "                p_date = pd.to_datetime(r['date'])\n",
                "                if p_date < current_date:\n",
                "                    filled_count += 1\n",
                "                    p_idx = filled_count\n",
                "                    if p_idx > 5: break\n",
                "                    \n",
                "                    # 埋める\n",
                "                    for k, v in r.items():\n",
                "                        col_name = f\"past_{p_idx}_{k}\"\n",
                "                        if col_name in df.columns:\n",
                "                            df.at[idx, col_name] = v\n",
                "            except:\n",
                "                continue\n",
                "\n",
                "    # 保存\n",
                "    df.to_csv(OUTPUT_FILE, index=False)\n",
                "    print(f\"Saved to {OUTPUT_FILE}\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"Fatal error: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}