{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# JRA Scraping Notebook\n",
                "\n",
                "中央競馬（JRA）のデータをスクレイピングし、Google Drive上のデータセットに追加します。"
            ],
            "metadata": {
                "id": "intro_md"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# 1. Google Driveのマウント\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# ★★★ 設定項目 ★★★\n",
                "# scraperフォルダが存在するパス (Google Drive上のパス)\n",
                "# 例: '/content/drive/MyDrive/dai-keiba'\n",
                "PROJECT_PATH = '/content/drive/MyDrive/dai-keiba'\n",
                "\n",
                "if not os.path.exists(PROJECT_PATH):\n",
                "    print(f\"Error: Path {PROJECT_PATH} does not exist. Please check your Drive structure.\")\n",
                "else:\n",
                "    print(f\"Project path found: {PROJECT_PATH}\")\n",
                "    os.chdir(PROJECT_PATH)\n",
                "    sys.path.append(PROJECT_PATH)\n"
            ],
            "metadata": {
                "id": "mount_drive"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 2. 必要なライブラリのインポート\n",
                "try:\n",
                "    import pandas as pd\n",
                "    import requests\n",
                "    import bs4\n",
                "except ImportError:\n",
                "    !pip install pandas requests beautifulsoup4\n",
                "    import pandas as pd\n",
                "    import requests\n",
                "    import bs4\n",
                "\n",
                "from datetime import datetime, date\n",
                "from scraper.jra_scraper import scrape_jra_year, JRA_MONTH_PARAMS\n",
                "import time\n"
            ],
            "metadata": {
                "id": "imports"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 3. スクレイピング実行関数の定義\n",
                "\n",
                "def jra_scrape_execution(year_str, start_date=None, end_date=None):\n",
                "    CSV_FILE_PATH = os.path.join(PROJECT_PATH, \"database.csv\")\n",
                "    print(f\"Using CSV Path: {CSV_FILE_PATH}\")\n",
                "\n",
                "    def save_chunk(df_chunk):\n",
                "        if os.path.exists(CSV_FILE_PATH):\n",
                "            try:\n",
                "                # Read types as string to prevent auto-float for IDs\n",
                "                existing_df = pd.read_csv(CSV_FILE_PATH, dtype={'race_id': str, 'horse_id': str})\n",
                "                combined_df = pd.concat([existing_df, df_chunk], ignore_index=True)\n",
                "            except Exception as e:\n",
                "                print(f\"Read Error: {e}, creating new.\")\n",
                "                combined_df = df_chunk\n",
                "        else:\n",
                "            combined_df = df_chunk\n",
                "\n",
                "        # Deduplicate\n",
                "        subset_cols = ['race_id', '馬名']\n",
                "        subset_cols = [c for c in subset_cols if c in combined_df.columns]\n",
                "        if subset_cols:\n",
                "            combined_df.drop_duplicates(subset=subset_cols, keep='last', inplace=True)\n",
                "\n",
                "        combined_df.to_csv(CSV_FILE_PATH, index=False, encoding=\"utf-8-sig\")\n",
                "        print(f\"  [Saved] Total rows: {len(combined_df)} (+{len(df_chunk)} new)\")\n",
                "\n",
                "    print(f\"Starting Scraping for {year_str} ({start_date} ~ {end_date})\")\n",
                "\n",
                "    # Load existing IDs to skip\n",
                "    existing_ids = set()\n",
                "    if os.path.exists(CSV_FILE_PATH):\n",
                "        try:\n",
                "             df_e = pd.read_csv(CSV_FILE_PATH, usecols=['race_id'], dtype={'race_id': str})\n",
                "             existing_ids = set(df_e['race_id'].astype(str))\n",
                "             print(f\"  Loaded {len(existing_ids)} existing race IDs to skip.\")\n",
                "        except:\n",
                "             pass\n",
                "\n",
                "    scrape_jra_year(year_str, start_date=start_date, end_date=end_date, save_callback=save_chunk, existing_race_ids=existing_ids)\n"
            ],
            "metadata": {
                "id": "def_exec"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 4. 実行パラメータの設定と開始\n",
                "# -----------------------------\n",
                "TARGET_YEAR = \"2024\"\n",
                "TARGET_MONTH = 1  # ★何月を取得するか指定 (Noneの場合は全期間、1〜12を指定)\n",
                "\n",
                "import calendar\n",
                "from datetime import date\n",
                "\n",
                "START_DATE = None\n",
                "END_DATE = None\n",
                "\n",
                "if TARGET_MONTH:\n",
                "    # 指定した月の1日〜末日を設定\n",
                "    _, last_day = calendar.monthrange(int(TARGET_YEAR), int(TARGET_MONTH))\n",
                "    START_DATE = date(int(TARGET_YEAR), int(TARGET_MONTH), 1)\n",
                "    END_DATE = date(int(TARGET_YEAR), int(TARGET_MONTH), last_day)\n",
                "    print(f\"Targeting specific month: {START_DATE} to {END_DATE}\")\n",
                "else:\n",
                "    # 自動判定ロジック (既存データの翌日から)\n",
                "    CSV_FILE_PATH = os.path.join(PROJECT_PATH, \"database.csv\")\n",
                "    if os.path.exists(CSV_FILE_PATH):\n",
                "        try:\n",
                "             df_exist = pd.read_csv(CSV_FILE_PATH)\n",
                "             if '日付' in df_exist.columns and not df_exist.empty:\n",
                "                 df_exist['date_obj'] = pd.to_datetime(df_exist['日付'], format='%Y年%m月%d日', errors='coerce')\n",
                "                 last_date = df_exist['date_obj'].max()\n",
                "                 if pd.notna(last_date):\n",
                "                     START_DATE = last_date.date()\n",
                "                     print(f\"既存データの最終日時: {last_date.date()} -> 続きから取得します\")\n",
                "        except Exception as e:\n",
                "            print(f\"既存データ確認エラー: {e}\")\n",
                "\n",
                "print(f\"Scraping Target: {TARGET_YEAR}, Start: {START_DATE}, End: {END_DATE}\")\n",
                "jra_scrape_execution(TARGET_YEAR, start_date=START_DATE, end_date=END_DATE)\n"
            ],
            "metadata": {
                "id": "run_cell"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}