{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# NAR Scraping Notebook\n",
                "\n",
                "地方競馬（NAR）のデータをスクレイピングし、Google Drive上のデータセットに追加します。"
            ],
            "metadata": {
                "id": "intro_md"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# 1. Google Driveのマウント\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# ★★★ 設定項目 ★★★\n",
                "# scraperフォルダが存在するパス (Google Drive上のパス)\n",
                "# 例: '/content/drive/MyDrive/dai-keiba'\n",
                "PROJECT_PATH = '/content/drive/MyDrive/dai-keiba'\n",
                "\n",
                "if not os.path.exists(PROJECT_PATH):\n",
                "    print(f\"Error: Path {PROJECT_PATH} does not exist. Please check your Drive structure.\")\n",
                "else:\n",
                "    print(f\"Project path found: {PROJECT_PATH}\")\n",
                "    os.chdir(PROJECT_PATH)\n",
                "    sys.path.append(PROJECT_PATH)\n"
            ],
            "metadata": {
                "id": "mount_drive"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 2. 必要なライブラリのインポート\n",
                "try:\n",
                "    import pandas as pd\n",
                "    import requests\n",
                "    import bs4\n",
                "except ImportError:\n",
                "    !pip install pandas requests beautifulsoup4\n",
                "    import pandas as pd\n",
                "    import requests\n",
                "    import bs4\n",
                "\n",
                "from datetime import datetime, date\n",
                "from scraper.auto_scraper import scrape_nar_year\n",
                "import time\n"
            ],
            "metadata": {
                "id": "imports"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 3. スクレイピング実行関数の定義\n",
                "\n",
                "def nar_scrape_execution(year_str, start_date=None, end_date=None):\n",
                "    CSV_FILE_PATH_NAR = os.path.join(PROJECT_PATH, \"database_nar.csv\")\n",
                "    print(f\"Using CSV Path: {CSV_FILE_PATH_NAR}\")\n",
                "\n",
                "    def save_callback(df_new):\n",
                "        if df_new is None or df_new.empty: return\n",
                "        \n",
                "        if os.path.exists(CSV_FILE_PATH_NAR):\n",
                "            try:\n",
                "                existing = pd.read_csv(CSV_FILE_PATH_NAR, dtype={'race_id': str, 'horse_id': str})\n",
                "                combined = pd.concat([existing, df_new], ignore_index=True)\n",
                "                # Deduplicate\n",
                "                if 'race_id' in combined.columns and '馬 番' in combined.columns:\n",
                "                    combined = combined.drop_duplicates(subset=['race_id', '馬 番'], keep='last')\n",
                "                combined.to_csv(CSV_FILE_PATH_NAR, index=False)\n",
                "                print(f\"  [Saved] {len(df_new)} rows added. Total: {len(combined)}\")\n",
                "            except Exception as e:\n",
                "                print(f\"Read Error: {e}, overwriting.\")\n",
                "                df_new.to_csv(CSV_FILE_PATH_NAR, index=False)\n",
                "        else:\n",
                "            df_new.to_csv(CSV_FILE_PATH_NAR, index=False)\n",
                "            print(f\"  [Created] {CSV_FILE_PATH_NAR} with {len(df_new)} rows.\")\n",
                "\n",
                "    print(f\"Starting NAR Scraping for {year_str} ({start_date} ~ {end_date})\")\n",
                "    scrape_nar_year(year_str, start_date=start_date, end_date=end_date, save_callback=save_callback)\n"
            ],
            "metadata": {
                "id": "def_exec"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 4. 実行パラメータの設定と開始\n",
                "# -----------------------------\n",
                "TARGET_YEAR = \"2025\"\n",
                "\n",
                "# 特定の期間だけ取得する場合\n",
                "START_DATE = None \n",
                "END_DATE = None\n",
                "\n",
                "# 自動判定ロジック (既存データの翌日から)\n",
                "CSV_FILE_PATH_NAR = os.path.join(PROJECT_PATH, \"database_nar.csv\")\n",
                "if os.path.exists(CSV_FILE_PATH_NAR) and START_DATE is None:\n",
                "    try:\n",
                "        df_exist = pd.read_csv(CSV_FILE_PATH_NAR)\n",
                "        if '日付' in df_exist.columns and not df_exist.empty:\n",
                "             df_exist['date_obj'] = pd.to_datetime(df_exist['日付'], format='%Y年%m月%d日', errors='coerce')\n",
                "             last_date = df_exist['date_obj'].max()\n",
                "             if pd.notna(last_date):\n",
                "                 START_DATE = last_date.date()\n",
                "                 print(f\"既存データの最終日時: {last_date.date()} -> この翌日から取得しますか？ (手動でSTART_DATEを設定して上書きも可能)\")\n",
                "                 pass\n",
                "    except Exception as e:\n",
                "        print(f\"既存データ確認エラー: {e}\")\n",
                "\n",
                "print(f\"Scraping Target: {TARGET_YEAR}, Start: {START_DATE}, End: {END_DATE}\")\n",
                "nar_scrape_execution(TARGET_YEAR, start_date=START_DATE, end_date=END_DATE)\n"
            ],
            "metadata": {
                "id": "run_cell"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}