#!/usr/bin/env python3
"""
ã‚«ãƒ©ãƒ å®Œå…¨æ€§æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å…¨94ã‚«ãƒ©ãƒ ã®ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ãå–å¾—ã§ãã‚‹ã‹ãƒ†ã‚¹ãƒˆ
"""

import pandas as pd
from pathlib import Path

# æœŸå¾…ã•ã‚Œã‚‹ã‚«ãƒ©ãƒ ãƒªã‚¹ãƒˆ(94ã‚«ãƒ©ãƒ )
EXPECTED_COLUMNS = [
    # åŸºæœ¬æƒ…å ± (26ã‚«ãƒ©ãƒ )
    "æ—¥ä»˜", "ä¼šå ´", "ãƒ¬ãƒ¼ã‚¹ç•ªå·", "ãƒ¬ãƒ¼ã‚¹å", "é‡è³", "ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—", "è·é›¢", "å›ã‚Š", "å¤©å€™", "é¦¬å ´çŠ¶æ…‹",
    "ç€é †", "æ ", "é¦¬ç•ª", "é¦¬å", "æ€§é½¢", "æ–¤é‡", "é¨æ‰‹", "ã‚¿ã‚¤ãƒ ", "ç€å·®", "äººæ°—", "å˜å‹ã‚ªãƒƒã‚º",
    "å¾Œ3F", "å©èˆ", "é¦¬ä½“é‡(å¢—æ¸›)", "race_id", "horse_id",
]

# éå»5èµ°ã‚«ãƒ©ãƒ  (65ã‚«ãƒ©ãƒ )
for i in range(1, 6):
    p = f"past_{i}"
    EXPECTED_COLUMNS.extend([
        f"{p}_date", f"{p}_rank", f"{p}_time", f"{p}_run_style", f"{p}_race_name",
        f"{p}_last_3f", f"{p}_horse_weight", f"{p}_jockey", f"{p}_condition",
        f"{p}_odds", f"{p}_weather", f"{p}_distance", f"{p}_course_type"
    ])

# è¡€çµ±ã‚«ãƒ©ãƒ  (3ã‚«ãƒ©ãƒ )
EXPECTED_COLUMNS.extend(["father", "mother", "bms"])


def verify_columns(csv_path: str) -> dict:
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚«ãƒ©ãƒ ã‚’æ¤œè¨¼
    
    Returns:
        æ¤œè¨¼çµæœã®è¾æ›¸
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“‹ ã‚«ãƒ©ãƒ å®Œå…¨æ€§æ¤œè¨¼: {Path(csv_path).name}")
    print(f"{'='*80}\n")
    
    if not Path(csv_path).exists():
        return {
            "status": "error",
            "message": f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {csv_path}"
        }
    
    try:
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã¿èª­ã¿è¾¼ã¿
        df = pd.read_csv(csv_path, nrows=0)
        actual_columns = df.columns.tolist()
        
        print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
        print(f"   æœŸå¾…ã‚«ãƒ©ãƒ æ•°: {len(EXPECTED_COLUMNS)}")
        print(f"   å®Ÿéš›ã‚«ãƒ©ãƒ æ•°: {len(actual_columns)}")
        
        # ã‚«ãƒ©ãƒ æ¯”è¼ƒ
        expected_set = set(EXPECTED_COLUMNS)
        actual_set = set(actual_columns)
        
        missing = expected_set - actual_set
        extra = actual_set - expected_set
        
        # é †åºãƒã‚§ãƒƒã‚¯
        order_match = (actual_columns == EXPECTED_COLUMNS)
        
        result = {
            "status": "success" if not missing and not extra and order_match else "warning",
            "total_expected": len(EXPECTED_COLUMNS),
            "total_actual": len(actual_columns),
            "missing_columns": list(missing),
            "extra_columns": list(extra),
            "order_match": order_match
        }
        
        # çµæœè¡¨ç¤º
        print(f"\n{'â”€'*80}")
        print("ğŸ“Š æ¤œè¨¼çµæœ")
        print(f"{'â”€'*80}\n")
        
        if not missing and not extra and order_match:
            print("âœ… å®Œå…¨ä¸€è‡´! ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ãŒæ­£ã—ãå­˜åœ¨ã—ã¾ã™")
        else:
            if missing:
                print(f"âŒ ä¸è¶³ã‚«ãƒ©ãƒ  ({len(missing)}å€‹):")
                for col in sorted(missing):
                    print(f"   - {col}")
            
            if extra:
                print(f"\nâ• ä½™åˆ†ãªã‚«ãƒ©ãƒ  ({len(extra)}å€‹):")
                for col in sorted(extra):
                    print(f"   - {col}")
            
            if not order_match:
                print(f"\nâš ï¸  ã‚«ãƒ©ãƒ ã®é †åºãŒç•°ãªã‚Šã¾ã™")
        
        # ãƒ‡ãƒ¼ã‚¿å†…å®¹ã®æ¤œè¨¼(ã‚µãƒ³ãƒ—ãƒ«)
        print(f"\n{'â”€'*80}")
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿å†…å®¹ã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼")
        print(f"{'â”€'*80}\n")
        
        df_sample = pd.read_csv(csv_path, nrows=10, dtype=str)
        
        if len(df_sample) > 0:
            print(f"âœ… ã‚µãƒ³ãƒ—ãƒ«è¡Œæ•°: {len(df_sample)}")
            
            # å„ã‚«ãƒ†ã‚´ãƒªã®æ¬ æç‡ã‚’ãƒã‚§ãƒƒã‚¯
            categories = {
                "åŸºæœ¬æƒ…å ±": EXPECTED_COLUMNS[:26],
                "éå»1èµ°": [c for c in EXPECTED_COLUMNS if c.startswith("past_1_")],
                "éå»2èµ°": [c for c in EXPECTED_COLUMNS if c.startswith("past_2_")],
                "éå»3èµ°": [c for c in EXPECTED_COLUMNS if c.startswith("past_3_")],
                "éå»4èµ°": [c for c in EXPECTED_COLUMNS if c.startswith("past_4_")],
                "éå»5èµ°": [c for c in EXPECTED_COLUMNS if c.startswith("past_5_")],
                "è¡€çµ±": ["father", "mother", "bms"]
            }
            
            print("\næ¬ æç‡:")
            for cat_name, cols in categories.items():
                available_cols = [c for c in cols if c in df_sample.columns]
                if available_cols:
                    missing_rate = df_sample[available_cols].isna().mean().mean() * 100
                    empty_rate = (df_sample[available_cols] == "").mean().mean() * 100
                    total_empty = missing_rate + empty_rate
                    
                    status = "âœ…" if total_empty < 10 else ("âš ï¸" if total_empty < 50 else "âŒ")
                    print(f"  {status} {cat_name:12}: {total_empty:5.1f}% ç©º")
        else:
            print("âš ï¸  ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
        
        return result
        
    except Exception as e:
        return {
            "status": "error",
            "message": f"ã‚¨ãƒ©ãƒ¼: {e}"
        }


def analyze_data_sources():
    """
    å„ã‚«ãƒ©ãƒ ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’åˆ†æ
    """
    print(f"\n{'='*80}")
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ†æ")
    print(f"{'='*80}\n")
    
    sources = {
        "ãƒ¬ãƒ¼ã‚¹çµæœãƒšãƒ¼ã‚¸": {
            "URL": "https://race.netkeiba.com/race/result.html?race_id=XXXX",
            "å–å¾—æ–¹æ³•": "BeautifulSoup + ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‘ãƒ¼ã‚¹",
            "ã‚«ãƒ©ãƒ æ•°": 26,
            "ã‚«ãƒ©ãƒ ": [
                "æ—¥ä»˜", "ä¼šå ´", "ãƒ¬ãƒ¼ã‚¹ç•ªå·", "ãƒ¬ãƒ¼ã‚¹å", "é‡è³", "ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—", "è·é›¢", "å›ã‚Š",
                "å¤©å€™", "é¦¬å ´çŠ¶æ…‹", "ç€é †", "æ ", "é¦¬ç•ª", "é¦¬å", "æ€§é½¢", "æ–¤é‡", "é¨æ‰‹",
                "ã‚¿ã‚¤ãƒ ", "ç€å·®", "äººæ°—", "å˜å‹ã‚ªãƒƒã‚º", "å¾Œ3F", "å©èˆ", "é¦¬ä½“é‡(å¢—æ¸›)",
                "race_id", "horse_id"
            ],
            "ãƒªã‚¹ã‚¯": "ä½ - ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ãŒå®‰å®š"
        },
        "é¦¬å±¥æ­´ãƒšãƒ¼ã‚¸": {
            "URL": "https://db.netkeiba.com/horse/result/HORSE_ID/",
            "å–å¾—æ–¹æ³•": "BeautifulSoup + pd.read_html",
            "ã‚«ãƒ©ãƒ æ•°": 65,
            "ã‚«ãƒ©ãƒ ": "past_1_* ~ past_5_* (å„13é …ç›®)",
            "ãƒªã‚¹ã‚¯": "ä¸­ - æ–°é¦¬ã®å ´åˆã¯ç©ºã€ãƒ¬ãƒ¼ã‚¹æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¿…é ˆ"
        },
        "è¡€çµ±ãƒšãƒ¼ã‚¸": {
            "URL": "https://db.netkeiba.com/horse/ped/HORSE_ID/",
            "å–å¾—æ–¹æ³•": "BeautifulSoup + CSSã‚»ãƒ¬ã‚¯ã‚¿",
            "ã‚«ãƒ©ãƒ æ•°": 3,
            "ã‚«ãƒ©ãƒ ": ["father", "mother", "bms"],
            "ãƒªã‚¹ã‚¯": "ä½ - ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ãŒå®‰å®š"
        }
    }
    
    for source_name, info in sources.items():
        print(f"ğŸ“Œ {source_name}")
        print(f"   URL: {info['URL']}")
        print(f"   å–å¾—æ–¹æ³•: {info['å–å¾—æ–¹æ³•']}")
        print(f"   ã‚«ãƒ©ãƒ æ•°: {info['ã‚«ãƒ©ãƒ æ•°']}")
        print(f"   ãƒªã‚¹ã‚¯: {info['ãƒªã‚¹ã‚¯']}")
        print()


def recommend_splitting_strategy():
    """
    ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯åˆ†å‰²æˆ¦ç•¥ã‚’ææ¡ˆ
    """
    print(f"\n{'='*80}")
    print("ğŸš€ ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯åˆ†å‰²æˆ¦ç•¥ã®ææ¡ˆ")
    print(f"{'='*80}\n")
    
    strategies = [
        {
            "name": "æˆ¦ç•¥1: 2æ®µéšåˆ†å‰²(åŸºæœ¬ + è©³ç´°)",
            "notebooks": [
                "Colab_JRA_Basic.ipynb - ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±ã®ã¿(26ã‚«ãƒ©ãƒ )",
                "Colab_JRA_Details.ipynb - å±¥æ­´+è¡€çµ±è¿½åŠ (68ã‚«ãƒ©ãƒ )"
            ],
            "pros": [
                "åŸºæœ¬æƒ…å ±ã¯é«˜é€Ÿå–å¾—(1ãƒ¬ãƒ¼ã‚¹=1ãƒªã‚¯ã‚¨ã‚¹ãƒˆ)",
                "è©³ç´°æƒ…å ±ã¯ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½",
                "å¤±æ•—æ™‚ã®å†å®Ÿè¡ŒãŒå®¹æ˜“"
            ],
            "cons": [
                "2å›å®Ÿè¡ŒãŒå¿…è¦",
                "ãƒ‡ãƒ¼ã‚¿çµåˆã®æ‰‹é–“"
            ],
            "time_reduction": "30-40%",
            "complexity": "ä¸­"
        },
        {
            "name": "æˆ¦ç•¥2: 3æ®µéšåˆ†å‰²(ãƒ¬ãƒ¼ã‚¹ + å±¥æ­´ + è¡€çµ±)",
            "notebooks": [
                "Colab_JRA_Race.ipynb - ãƒ¬ãƒ¼ã‚¹æƒ…å ±(26ã‚«ãƒ©ãƒ )",
                "Colab_JRA_History.ipynb - é¦¬å±¥æ­´(65ã‚«ãƒ©ãƒ )",
                "Colab_JRA_Pedigree.ipynb - è¡€çµ±(3ã‚«ãƒ©ãƒ )"
            ],
            "pros": [
                "å„æ®µéšãŒç‹¬ç«‹ã—ã¦å®Ÿè¡Œå¯èƒ½",
                "å±¥æ­´ã¨è¡€çµ±ã‚’ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½",
                "æœ€å¤§ã®æŸ”è»Ÿæ€§"
            ],
            "cons": [
                "3å›å®Ÿè¡ŒãŒå¿…è¦",
                "ãƒ‡ãƒ¼ã‚¿çµåˆãŒè¤‡é›‘"
            ],
            "time_reduction": "40-50%",
            "complexity": "é«˜"
        },
        {
            "name": "æˆ¦ç•¥3: ç¾çŠ¶ç¶­æŒ + æœ€é©åŒ–",
            "notebooks": [
                "Colab_JRA_Scraping.ipynb - å…¨ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬(94ã‚«ãƒ©ãƒ )"
            ],
            "pros": [
                "1å›ã®å®Ÿè¡Œã§å®Œäº†",
                "ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãŒä¿è¨¼ã•ã‚Œã‚‹",
                "ã‚·ãƒ³ãƒ—ãƒ«"
            ],
            "cons": [
                "å®Ÿè¡Œæ™‚é–“ãŒé•·ã„",
                "å¤±æ•—æ™‚ã®å†å®Ÿè¡Œã‚³ã‚¹ãƒˆãŒé«˜ã„"
            ],
            "time_reduction": "0% (ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³)",
            "complexity": "ä½"
        }
    ]
    
    for i, strategy in enumerate(strategies, 1):
        print(f"{'â”€'*80}")
        print(f"æˆ¦ç•¥{i}: {strategy['name']}")
        print(f"{'â”€'*80}")
        print(f"\nğŸ““ ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯æ§‹æˆ:")
        for nb in strategy['notebooks']:
            print(f"   - {nb}")
        
        print(f"\nâœ… ãƒ¡ãƒªãƒƒãƒˆ:")
        for pro in strategy['pros']:
            print(f"   + {pro}")
        
        print(f"\nâš ï¸  ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ:")
        for con in strategy['cons']:
            print(f"   - {con}")
        
        print(f"\nâ±ï¸  æ™‚é–“çŸ­ç¸®: {strategy['time_reduction']}")
        print(f"ğŸ”§ è¤‡é›‘åº¦: {strategy['complexity']}")
        print()
    
    print(f"{'='*80}")
    print("ğŸ’¡ æ¨å¥¨: æˆ¦ç•¥1 (2æ®µéšåˆ†å‰²)")
    print(f"{'='*80}")
    print("\nç†ç”±:")
    print("  1. åŸºæœ¬æƒ…å ±ã¯é«˜é€Ÿå–å¾—å¯èƒ½(Colabã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿)")
    print("  2. è©³ç´°æƒ…å ±ã¯åˆ¥ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§å®Ÿè¡Œå¯èƒ½")
    print("  3. å®Ÿè£…ãŒæ¯”è¼ƒçš„ã‚·ãƒ³ãƒ—ãƒ«")
    print("  4. 30-40%ã®æ™‚é–“çŸ­ç¸®ãŒè¦‹è¾¼ã‚ã‚‹")
    print()


if __name__ == "__main__":
    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ†æ
    analyze_data_sources()
    
    # åˆ†å‰²æˆ¦ç•¥ã®ææ¡ˆ
    recommend_splitting_strategy()
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼(å­˜åœ¨ã™ã‚‹å ´åˆ)
    csv_paths = [
        "/Users/itoudaiki/Program/dai-keiba/data/raw/database.csv",
        "/Users/itoudaiki/Program/dai-keiba/data/raw/database_nar.csv"
    ]
    
    for csv_path in csv_paths:
        if Path(csv_path).exists():
            verify_columns(csv_path)
    
    print(f"\n{'='*80}")
    print("âœ… æ¤œè¨¼å®Œäº†")
    print(f"{'='*80}\n")
