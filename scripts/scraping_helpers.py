"""
æœ€é©åŒ–ã•ã‚ŒãŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
Colab JRA/NAR ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ä½¿ç”¨
"""

import pandas as pd
import gc
import os
import shutil
from typing import Set, Optional


def deduplicate_in_chunks(csv_path: str, chunk_size: int = 10000) -> None:
    """
    ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§é‡è¤‡å‰Šé™¤ã‚’å®Ÿè¡Œ(ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„)
    
    Args:
        csv_path: é‡è¤‡å‰Šé™¤å¯¾è±¡ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        chunk_size: ä¸€åº¦ã«å‡¦ç†ã™ã‚‹è¡Œæ•°
    """
    if not os.path.exists(csv_path):
        print(f"  âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {csv_path}")
        return
    
    print(f"  ğŸ”„ ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§é‡è¤‡å‰Šé™¤ä¸­... (chunk_size={chunk_size})")
    
    seen: Set[str] = set()
    temp_path = csv_path + '.tmp'
    
    try:
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—
        headers = pd.read_csv(csv_path, nrows=0).columns.tolist()
        
        if 'race_id' not in headers or 'horse_id' not in headers:
            print("  âš ï¸  race_id ã¾ãŸã¯ horse_id ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«å‡¦ç†
        first_chunk = True
        total_rows = 0
        unique_rows = 0
        
        for chunk in pd.read_csv(csv_path, dtype=str, chunksize=chunk_size, low_memory=False):
            total_rows += len(chunk)
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã®ã‚­ãƒ¼ã‚’ä½œæˆ
            chunk['_key'] = chunk['race_id'].fillna('') + '_' + chunk['horse_id'].fillna('')
            
            # æ—¢ã«è¦‹ãŸã‚­ãƒ¼ã‚’é™¤å¤–
            chunk_dedup = chunk[~chunk['_key'].isin(seen)]
            
            # è¦‹ãŸã‚­ãƒ¼ã‚’è¨˜éŒ²
            seen.update(chunk_dedup['_key'].tolist())
            unique_rows += len(chunk_dedup)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
            chunk_dedup = chunk_dedup.drop('_key', axis=1)
            chunk_dedup.to_csv(temp_path, mode='a', header=first_chunk, index=False)
            first_chunk = False
            
            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            del chunk, chunk_dedup
            gc.collect()
        
        # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç½®ãæ›ãˆ
        shutil.move(temp_path, csv_path)
        
        duplicates = total_rows - unique_rows
        print(f"  âœ… é‡è¤‡å‰Šé™¤å®Œäº†: {total_rows} â†’ {unique_rows} rows ({duplicates} duplicates removed)")
        
    except Exception as e:
        print(f"  âŒ é‡è¤‡å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        if os.path.exists(temp_path):
            os.remove(temp_path)
        raise


def check_memory_usage() -> float:
    """
    ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç¢ºèª(MBå˜ä½)
    
    Returns:
        ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡(MB)
    """
    try:
        import psutil
        process = psutil.Process(os.getpid())
        mem_mb = process.memory_info().rss / 1024 / 1024
        return mem_mb
    except ImportError:
        # psutilãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        return 0.0


def log_memory(label: str = "", threshold_mb: float = 10000) -> None:
    """
    ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°å‡ºåŠ›ã—ã€é–¾å€¤ã‚’è¶…ãˆãŸã‚‰è­¦å‘Š
    
    Args:
        label: ãƒ­ã‚°ã®ãƒ©ãƒ™ãƒ«
        threshold_mb: è­¦å‘Šã‚’å‡ºã™ãƒ¡ãƒ¢ãƒªé–¾å€¤(MB)
    """
    mem_mb = check_memory_usage()
    
    if mem_mb > 0:
        status = "âš ï¸" if mem_mb > threshold_mb else "ğŸ’¾"
        print(f"  {status} Memory: {mem_mb:.1f} MB {label}")
        
        if mem_mb > threshold_mb:
            print(f"  âš ï¸  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒé«˜ã„! GCå®Ÿè¡Œ...")
            gc.collect()


class HorseHistoryCache:
    """
    é¦¬ã®å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã‚¯ãƒ©ã‚¹
    åŒã˜é¦¬ã®é‡è¤‡å–å¾—ã‚’å›é¿ã—ã¦ãƒ¡ãƒ¢ãƒªã¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç¯€ç´„
    """
    
    def __init__(self, max_size: int = 1000):
        """
        Args:
            max_size: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ€å¤§ã‚µã‚¤ã‚º
        """
        self.cache = {}
        self.max_size = max_size
        self.hits = 0
        self.misses = 0
    
    def get(self, horse_id: str, race_date: str) -> Optional[pd.DataFrame]:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å±¥æ­´ã‚’å–å¾—
        
        Args:
            horse_id: é¦¬ID
            race_date: ãƒ¬ãƒ¼ã‚¹æ—¥ä»˜
            
        Returns:
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå±¥æ­´DataFrameã€ãªã‘ã‚Œã°None
        """
        cache_key = f"{horse_id}_{race_date}"
        
        if cache_key in self.cache:
            self.hits += 1
            return self.cache[cache_key].copy()
        
        self.misses += 1
        return None
    
    def put(self, horse_id: str, race_date: str, df: pd.DataFrame) -> None:
        """
        å±¥æ­´ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        
        Args:
            horse_id: é¦¬ID
            race_date: ãƒ¬ãƒ¼ã‚¹æ—¥ä»˜
            df: å±¥æ­´DataFrame
        """
        cache_key = f"{horse_id}_{race_date}"
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™
        if len(self.cache) >= self.max_size:
            # æœ€ã‚‚å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤(FIFO)
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
        
        self.cache[cache_key] = df.copy()
    
    def stats(self) -> str:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’è¿”ã™
        
        Returns:
            çµ±è¨ˆæ–‡å­—åˆ—
        """
        total = self.hits + self.misses
        hit_rate = (self.hits / total * 100) if total > 0 else 0
        return f"Cache: {len(self.cache)} entries, Hit rate: {hit_rate:.1f}% ({self.hits}/{total})"


def fetch_with_retry(url: str, headers: dict, max_retries: int = 3, timeout: int = 15) -> Optional[object]:
    """
    ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ä»˜ãã§HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    
    Args:
        url: ãƒªã‚¯ã‚¨ã‚¹ãƒˆURL
        headers: HTTPãƒ˜ãƒƒãƒ€ãƒ¼
        max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
        timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ(ç§’)
        
    Returns:
        ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€å¤±æ•—æ™‚ã¯None
    """
    import requests
    import time
    
    for attempt in range(max_retries):
        try:
            resp = requests.get(url, headers=headers, timeout=timeout)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ¤œçŸ¥
            if resp.status_code == 403 or resp.status_code == 429:
                wait_time = (2 ** attempt) * 5  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•: 5ç§’, 10ç§’, 20ç§’
                print(f"  âš ï¸  ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ¤œçŸ¥ (Status {resp.status_code}). {wait_time}ç§’å¾…æ©Ÿ...")
                time.sleep(wait_time)
                continue
            
            if resp.status_code == 200:
                return resp
            
            # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
            print(f"  âš ï¸  HTTP Error {resp.status_code} (attempt {attempt + 1}/{max_retries})")
            
        except requests.exceptions.Timeout:
            print(f"  âš ï¸  Timeout (attempt {attempt + 1}/{max_retries})")
        except requests.exceptions.RequestException as e:
            print(f"  âš ï¸  Request Error: {e} (attempt {attempt + 1}/{max_retries})")
        
        # ãƒªãƒˆãƒ©ã‚¤å‰ã«å¾…æ©Ÿ
        if attempt < max_retries - 1:
            time.sleep(2)
    
    print(f"  âŒ Failed after {max_retries} attempts: {url}")
    return None


def safe_append_csv_optimized(df_chunk: pd.DataFrame, path: str) -> None:
    """
    æœ€é©åŒ–ã•ã‚ŒãŸCSVè¿½è¨˜é–¢æ•°
    
    Args:
        df_chunk: è¿½è¨˜ã™ã‚‹DataFrame
        path: CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    if not os.path.exists(path):
        # æ–°è¦ä½œæˆ
        df_chunk.to_csv(path, index=False)
    else:
        try:
            # ã‚«ãƒ©ãƒ é †åºã‚’æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆã‚ã›ã‚‹
            headers = pd.read_csv(path, nrows=0).columns.tolist()
            df_aligned = df_chunk.reindex(columns=headers, fill_value='')
            df_aligned.to_csv(path, mode='a', header=False, index=False)
        except Exception as e:
            print(f"  âš ï¸  ã‚«ãƒ©ãƒ æ•´åˆ—å¤±æ•—ã€ãã®ã¾ã¾è¿½è¨˜: {e}")
            df_chunk.to_csv(path, mode='a', header=False, index=False)
