#!/usr/bin/env python3
"""
NAR Basic v2ã®tableså¤‰æ•°ã‚¹ã‚³ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£
"""

import json

notebook_path = "/Users/itoudaiki/Program/dai-keiba/notebooks/Colab_NAR_Basic_v2.ipynb"

with open(notebook_path, 'r', encoding='utf-8') as f:
    nb = json.load(f)

for i, cell in enumerate(nb['cells']):
    if cell['cell_type'] == 'code':
        source = ''.join(cell['source'])
        
        if 'def scrape_race_basic(' in source:
            print(f"âœ… ã‚»ãƒ«{i}: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°ã‚’ç™ºè¦‹")
            
            # tablesã®å®šç¾©ã‚’ã€ã‚³ãƒ¼ãƒŠãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—ã®å‰ã«ç§»å‹•
            old_code = """        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        metadata = extract_metadata(soup, url)
        
        # ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ãƒ†ãƒ¼ãƒ–ãƒ«å–å¾—(NAR)
        corner_data = {}
        for table in tables:
            if 'ã‚³ãƒ¼ãƒŠãƒ¼' in table.text:
                headers_cells = table.find_all('th')
                corner_names = [th.text.strip() for th in headers_cells]
                corner_rows = table.find_all('tr')
                for j, row in enumerate(corner_rows):
                    cells = row.find_all('td')
                    if cells and j < len(corner_names):
                        corner_data[corner_names[j]] = cells[0].text.strip()
                break
        
        # ãƒ¬ãƒ¼ã‚¹çµæœãƒ†ãƒ¼ãƒ–ãƒ«å–å¾—
        tables = soup.find_all('table')"""
            
            new_code = """        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        metadata = extract_metadata(soup, url)
        
        # å…¨ãƒ†ãƒ¼ãƒ–ãƒ«å–å¾—(ã‚³ãƒ¼ãƒŠãƒ¼ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¬ãƒ¼ã‚¹çµæœã®ä¸¡æ–¹ã§ä½¿ç”¨)
        tables = soup.find_all('table')
        
        # ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ãƒ†ãƒ¼ãƒ–ãƒ«å–å¾—(NAR)
        corner_data = {}
        for table in tables:
            if 'ã‚³ãƒ¼ãƒŠãƒ¼' in table.text:
                headers_cells = table.find_all('th')
                corner_names = [th.text.strip() for th in headers_cells]
                corner_rows = table.find_all('tr')
                for j, row in enumerate(corner_rows):
                    cells = row.find_all('td')
                    if cells and j < len(corner_names):
                        corner_data[corner_names[j]] = cells[0].text.strip()
                break
        
        # ãƒ¬ãƒ¼ã‚¹çµæœãƒ†ãƒ¼ãƒ–ãƒ«å–å¾—"""
            
            source = source.replace(old_code, new_code)
            
            cell['source'] = [line + '\n' for line in source.split('\n')]
            if cell['source'] and cell['source'][-1] == '\n':
                cell['source'][-1] = cell['source'][-1].rstrip('\n')
            
            print("  âœ… tableså¤‰æ•°ã®å®šç¾©ã‚’ã‚³ãƒ¼ãƒŠãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—ã®å‰ã«ç§»å‹•")
            break

with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(nb, f, ensure_ascii=False, indent=1)

print(f"\nğŸ’¾ ä¿å­˜å®Œäº†: {notebook_path}")
print("\nğŸ“ ä¿®æ­£å†…å®¹:")
print("  - tables = soup.find_all('table') ã‚’ã‚³ãƒ¼ãƒŠãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—ã®å‰ã«ç§»å‹•")
print("  - ã‚¹ã‚³ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼ã‚’è§£æ±º")
