#!/usr/bin/env python3
"""
NAR Basic v2ã®è¾æ›¸åˆæœŸåŒ–ã‚’ä¿®æ­£
ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ã¨corner_1~4ã‚’è¿½åŠ 
"""

import json

notebook_path = "/Users/itoudaiki/Program/dai-keiba/notebooks/Colab_NAR_Basic_v2.ipynb"

with open(notebook_path, 'r', encoding='utf-8') as f:
    nb = json.load(f)

for i, cell in enumerate(nb['cells']):
    if cell['cell_type'] == 'code':
        source = ''.join(cell['source'])
        
        if 'def scrape_race_basic(' in source:
            print(f"âœ… ã‚»ãƒ«{i}: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°ã‚’ç™ºè¦‹\n")
            
            # è¾æ›¸åˆæœŸåŒ–ã®'å¾Œ3F'ã®å¾Œã«'ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †'ã¨corner_1~4ã‚’æŒ¿å…¥
            old_dict = """                'å¾Œ3F': cells[11].text.strip() if len(cells) > 11 else '',
                'ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †': '',  # NARã«ã¯å­˜åœ¨ã—ãªã„ãŸã‚ç©ºæ¬„
                'å©èˆ': cells[12].text.strip() if len(cells) > 12 else '',"""
            
            new_dict = """                'å¾Œ3F': cells[11].text.strip() if len(cells) > 11 else '',
                'ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †': '',  # ç”Ÿãƒ‡ãƒ¼ã‚¿(NARã¯åˆ¥ãƒ†ãƒ¼ãƒ–ãƒ«)
                'corner_1': '',  # æœ€çµ‚ã‚³ãƒ¼ãƒŠãƒ¼
                'corner_2': '',  # æœ€çµ‚-1
                'corner_3': '',  # æœ€çµ‚-2
                'corner_4': '',  # æœ€çµ‚-3
                'å©èˆ': cells[12].text.strip() if len(cells) > 12 else '',"""
            
            source = source.replace(old_dict, new_dict)
            print("âœ… è¾æ›¸åˆæœŸåŒ–ã«'ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †'ã¨corner_1~4ã‚’è¿½åŠ \n")
            
            cell['source'] = [line + '\n' for line in source.split('\n')]
            if cell['source'] and cell['source'][-1] == '\n':
                cell['source'][-1] = cell['source'][-1].rstrip('\n')
            
            break

with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(nb, f, ensure_ascii=False, indent=1)

print(f"ğŸ’¾ ä¿å­˜å®Œäº†: {notebook_path}")
