#!/usr/bin/env python3
"""
JRA/NARã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¬ æã‚’ä¿®æ­£
"""

import json
import re

def fix_notebook_metadata(notebook_path):
    """
    force_race_idä½¿ç”¨æ™‚ã§ã‚‚ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹ã‚ˆã†ã«ä¿®æ­£
    """
    print(f"\nğŸ”§ ä¿®æ­£ä¸­: {notebook_path}")
    
    with open(notebook_path, 'r', encoding='utf-8') as f:
        nb = json.load(f)
    
    for i, cell in enumerate(nb['cells']):
        if cell['cell_type'] != 'code':
            continue
        
        source = ''.join(cell['source'])
        
        # scrape_race_riché–¢æ•°ã‚’æ¢ã™
        if 'def scrape_race_rich(' not in source:
            continue
        
        if 'if force_race_id:' not in source:
            continue
        
        print(f"  âœ… ã‚»ãƒ« {i}: scrape_race_riché–¢æ•°ã‚’ç™ºè¦‹")
        
        # ä¿®æ­£: elseãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å¸¸ã«æŠ½å‡º
        lines = source.split('\n')
        new_lines = []
        in_else_block = False
        else_indent = 0
        
        for line in lines:
            # force_race_idã®elseãƒ–ãƒ­ãƒƒã‚¯ã‚’è¦‹ã¤ã‘ã‚‹
            if re.match(r'\s+else:\s*$', line) and 'venues_str' in source[source.index(line):source.index(line)+500]:
                # ã“ã®elseã¯force_race_idã®else
                in_else_block = True
                else_indent = len(line) - len(line.lstrip())
                # elseã‚’ã‚³ãƒ¡ãƒ³ãƒˆã«å¤‰æ›´
                new_lines.append(line.replace('else:', '# Always extract metadata (even with force_race_id)'))
                continue
            
            # elseãƒ–ãƒ­ãƒƒã‚¯å†…ã®ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’èª¿æ•´
            if in_else_block:
                current_indent = len(line) - len(line.lstrip())
                if line.strip() and current_indent <= else_indent:
                    # elseãƒ–ãƒ­ãƒƒã‚¯çµ‚äº†
                    in_else_block = False
                    new_lines.append(line)
                else:
                    # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’1ãƒ¬ãƒ™ãƒ«æ¸›ã‚‰ã™
                    if line.strip():
                        new_line = line[4:] if len(line) > 4 else line
                        new_lines.append(new_line)
                    else:
                        new_lines.append(line)
            else:
                new_lines.append(line)
        
        # ã‚»ãƒ«ã‚’æ›´æ–°
        new_source = '\n'.join(new_lines)
        cell['source'] = [line + '\n' for line in new_source.split('\n')]
        if cell['source'] and cell['source'][-1].endswith('\n\n'):
            cell['source'][-1] = cell['source'][-1].rstrip('\n') + '\n'
        elif cell['source'] and cell['source'][-1] == '\n':
            cell['source'][-1] = ''
        
        print(f"  âœ… ä¿®æ­£å®Œäº†")
        break
    
    # ä¿å­˜
    with open(notebook_path, 'w', encoding='utf-8') as f:
        json.dump(nb, f, ensure_ascii=False, indent=1)
    
    print(f"  ğŸ’¾ ä¿å­˜å®Œäº†\n")

if __name__ == "__main__":
    print("="*80)
    print("ğŸ”§ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¬ æã®ä¿®æ­£")
    print("="*80)
    
    notebooks = [
        "/Users/itoudaiki/Program/dai-keiba/notebooks/Colab_JRA_Scraping.ipynb",
        "/Users/itoudaiki/Program/dai-keiba/notebooks/Colab_NAR_Scraping.ipynb"
    ]
    
    for nb in notebooks:
        fix_notebook_metadata(nb)
    
    print("="*80)
    print("âœ… ä¿®æ­£å®Œäº†")
    print("="*80)
    print("\nğŸ“ å¤‰æ›´å†…å®¹:")
    print("  - force_race_idä½¿ç”¨æ™‚ã§ã‚‚ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿(æ—¥ä»˜ã€ä¼šå ´ç­‰)ã‚’æŠ½å‡º")
    print("  - åŸºæœ¬æƒ…å ±ã‚«ãƒ©ãƒ ã®æ¬ æã‚’è§£æ¶ˆ")
    print("\nâš ï¸  æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("  1. æ—¢å­˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã¾ãŸã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—")
    print("  2. ä¿®æ­£å¾Œã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å†å®Ÿè¡Œ")
    print("  3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ãå–å¾—ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª")
