#!/usr/bin/env python3
"""
Stage 1 Basic ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ãƒ†ã‚¹ãƒˆ(1ãƒ¬ãƒ¼ã‚¹)
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time

# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–¢æ•°(ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¨åŒã˜)
def extract_metadata(soup, url):
    metadata = {
        'æ—¥ä»˜': '',
        'ä¼šå ´': '',
        'ãƒ¬ãƒ¼ã‚¹ç•ªå·': '',
        'ãƒ¬ãƒ¼ã‚¹å': '',
        'é‡è³': '',
        'ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—': '',
        'è·é›¢': '',
        'å›ã‚Š': '',
        'å¤©å€™': '',
        'é¦¬å ´çŠ¶æ…‹': '',
        'race_id': ''
    }
    
    try:
        # ãƒ¬ãƒ¼ã‚¹å
        race_name_elem = soup.select_one('div.race_name')
        if race_name_elem:
            metadata['ãƒ¬ãƒ¼ã‚¹å'] = race_name_elem.text.strip()
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±
        header_elem = soup.select_one('div.header_line')
        if header_elem:
            header_text = header_elem.text.strip()
        else:
            header_text = soup.text
        
        # æ—¥ä»˜
        date_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', header_text)
        if date_match:
            year = date_match.group(1)
            month = f"{int(date_match.group(2)):02d}"
            day = f"{int(date_match.group(3)):02d}"
            metadata['æ—¥ä»˜'] = f"{year}/{month}/{day}"
        
        # ä¼šå ´ã€å›ã€æ—¥
        venues = ['æœ­å¹Œ', 'å‡½é¤¨', 'ç¦å³¶', 'æ–°æ½Ÿ', 'æ±äº¬', 'ä¸­å±±', 'ä¸­äº¬', 'äº¬éƒ½', 'é˜ªç¥', 'å°å€‰']
        venue_pattern = '|'.join(venues)
        meta_match = re.search(rf'(\d+)å›({venue_pattern})(\d+)æ—¥', header_text)
        
        kai = '01'
        nichi = '01'
        
        if meta_match:
            kai = f"{int(meta_match.group(1)):02d}"
            metadata['ä¼šå ´'] = meta_match.group(2)
            nichi = f"{int(meta_match.group(3)):02d}"
        
        # ãƒ¬ãƒ¼ã‚¹ç•ªå·
        race_num_match = re.search(r'(\d+)R', header_text)
        if race_num_match:
            race_num = f"{int(race_num_match.group(1)):02d}"
            metadata['ãƒ¬ãƒ¼ã‚¹ç•ªå·'] = f"{race_num_match.group(1)}R"
        else:
            race_num = '10'
        
        # race_idç”Ÿæˆ
        place_map = {
            'æœ­å¹Œ': '01', 'å‡½é¤¨': '02', 'ç¦å³¶': '03', 'æ–°æ½Ÿ': '04', 'æ±äº¬': '05',
            'ä¸­å±±': '06', 'ä¸­äº¬': '07', 'äº¬éƒ½': '08', 'é˜ªç¥': '09', 'å°å€‰': '10'
        }
        place_code = place_map.get(metadata['ä¼šå ´'], '00')
        
        if date_match:
            metadata['race_id'] = f"{year}{place_code}{kai}{nichi}{race_num}"
        
        # é‡è³
        if 'G1' in metadata['ãƒ¬ãƒ¼ã‚¹å'] or 'Gâ… ' in metadata['ãƒ¬ãƒ¼ã‚¹å']:
            metadata['é‡è³'] = 'G1'
        elif 'G2' in metadata['ãƒ¬ãƒ¼ã‚¹å'] or 'Gâ…¡' in metadata['ãƒ¬ãƒ¼ã‚¹å']:
            metadata['é‡è³'] = 'G2'
        elif 'G3' in metadata['ãƒ¬ãƒ¼ã‚¹å'] or 'Gâ…¢' in metadata['ãƒ¬ãƒ¼ã‚¹å']:
            metadata['é‡è³'] = 'G3'
        
        # ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ãƒ»è·é›¢
        course_match = re.search(r'(èŠ|ãƒ€ãƒ¼ãƒˆ|ãƒ€|éšœå®³)[^0-9]*(\d+)m', header_text)
        if course_match:
            course_type = course_match.group(1)
            if 'èŠ' in course_type:
                metadata['ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—'] = 'èŠ'
            elif 'ãƒ€' in course_type:
                metadata['ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—'] = 'ãƒ€ãƒ¼ãƒˆ'
            elif 'éšœ' in course_type:
                metadata['ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—'] = 'éšœå®³'
            metadata['è·é›¢'] = course_match.group(2)
        
        # å›ã‚Š
        if 'å³' in header_text:
            metadata['å›ã‚Š'] = 'å³'
        elif 'å·¦' in header_text:
            metadata['å›ã‚Š'] = 'å·¦'
        elif 'ç›´ç·š' in header_text or 'ç›´' in header_text:
            metadata['å›ã‚Š'] = 'ç›´ç·š'
        
        # å¤©å€™
        weather_match = re.search(r'å¤©å€™\s*[:ï¼š]\s*(\S+)', soup.text)
        if weather_match:
            metadata['å¤©å€™'] = weather_match.group(1)
        
        # é¦¬å ´çŠ¶æ…‹
        condition_match = re.search(r'(?:èŠ|ãƒ€ãƒ¼ãƒˆ)\s*[:ï¼š]\s*(\S+)', soup.text)
        if condition_match:
            metadata['é¦¬å ´çŠ¶æ…‹'] = condition_match.group(1)
    
    except Exception as e:
        print(f"  âš ï¸ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
    
    return metadata

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
def test_scraping(race_id):
    """1ãƒ¬ãƒ¼ã‚¹ã®ãƒ†ã‚¹ãƒˆ"""
    print(f"\n{'='*80}")
    print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆ: {race_id}")
    print(f"{'='*80}\n")
    
    url = f"https://race.netkeiba.com/race/result.html?race_id={race_id}"
    
    # ãƒšãƒ¼ã‚¸å–å¾—
    headers = {'User-Agent': 'Mozilla/5.0'}
    resp = requests.get(url, headers=headers, timeout=15)
    resp.encoding = 'EUC-JP'
    soup = BeautifulSoup(resp.text, 'html.parser')
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
    metadata = extract_metadata(soup, url)
    
    print("ğŸ“Š ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿:")
    for key, value in metadata.items():
        status = "âœ…" if value else "âŒ"
        print(f"  {status} {key}: {value}")
    
    # æ¬ æãƒã‚§ãƒƒã‚¯
    missing = [k for k, v in metadata.items() if not v]
    if missing:
        print(f"\nâš ï¸  æ¬ æ: {missing}")
    else:
        print(f"\nâœ… å…¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ")
    
    return metadata

if __name__ == "__main__":
    # JRAãƒ†ã‚¹ãƒˆ
    print("ğŸ‡ JRA ãƒ†ã‚¹ãƒˆ")
    jra_metadata = test_scraping("202001010101")
    
    # NARãƒ†ã‚¹ãƒˆ
    print("\n\nğŸ‡ NAR ãƒ†ã‚¹ãƒˆ")
    nar_metadata = test_scraping("202030041501")
    
    print(f"\n{'='*80}")
    print("âœ… ãƒ†ã‚¹ãƒˆå®Œäº†")
    print(f"{'='*80}")
