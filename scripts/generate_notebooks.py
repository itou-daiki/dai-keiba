import json
import os

def read_file(path):
    if not os.path.exists(path):
        return ""
    with open(path, 'r', encoding='utf-8') as f:
        return f.read()

def create_notebook(cells):
    return json.dumps({
        "cells": cells,
        "metadata": {
            "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
            "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}
        },
        "nbformat": 4,
        "nbformat_minor": 5
    }, indent=1, ensure_ascii=False)

def gen_jra_scraping_nb():
    jra_code = read_file('scripts/scraping_logic_v2.py')
    
    cells = [
        {"cell_type": "markdown", "metadata": {}, "source": ["# ğŸ‡ JRA å…¨ãƒ¬ãƒ¼ã‚¹å–å¾— (2020-2026)\n", "ä»¥ä¸‹ã®è¨­å®šå¤‰æ•°ã‚’å¤‰æ›´ã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚**Netkeiba.com** ã‹ã‚‰æŒ‡å®šã—ãŸæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€`TARGET_CSV` ã«ä¿å­˜ã—ã¾ã™ã€‚"]},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": [
            "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆã™ã‚‹å ´åˆã®ã¿å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
            "from google.colab import drive\n",
            "drive.mount('/content/drive')"
        ]},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": [
            "# è¨­å®š (ã“ã“ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„)\n",
            "YEAR = 2025          # å¯¾è±¡å¹´åº¦ (ä¾‹: 2024)\n",
            "START_MONTH = 1      # é–‹å§‹æœˆ (1-12)\n",
            "END_MONTH = 12       # çµ‚äº†æœˆ (1-12)\n",
            "SAVE_DIR = '/content/drive/MyDrive/dai-keiba/data/raw' # ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€\n",
            "TARGET_CSV = 'database.csv'\n"
        ]},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": jra_code.splitlines(keepends=True)},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": [
            "# å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯\n",
            "import os\n",
            "import pandas as pd\n",
            "from datetime import date, timedelta\n",
            "import calendar\n",
            "import requests\n",
            "from bs4 import BeautifulSoup\n",
            "import time\n",
            "from tqdm.auto import tqdm\n",
            "import re\n",
            "\n",
            "if YEAR:\n",
            "    # Saveãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ\n",
            "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
            "    \n",
            "    s_date = date(int(YEAR), int(START_MONTH), 1)\n",
            "    last_day = calendar.monthrange(int(YEAR), int(END_MONTH))[1]\n",
            "    e_date = date(int(YEAR), int(END_MONTH), last_day)\n",
            "    \n",
            "    # æœªæ¥ã®æ—¥ä»˜ã¯æ¤œç´¢ã—ãªã„ã‚ˆã†ã«åˆ¶é™\n",
            "    today = date.today()\n",
            "    if e_date > today:\n",
            "        e_date = today\n",
            "    \n",
            "    save_path = os.path.join(SAVE_DIR, TARGET_CSV)\n",
            "    print(f'{YEAR}å¹´ã®ãƒ‡ãƒ¼ã‚¿ã‚’ {s_date} ã‹ã‚‰ {e_date} ã¾ã§å–å¾—ã—ã¾ã™(Netkeibaå‚ç…§)...')\n",
            "    print(f'ä¿å­˜å…ˆ: {save_path}')\n",
            "    \n",
            "    # å®‰å…¨ãªè¿½è¨˜é–¢æ•°\n",
            "    def safe_append_csv(df_chunk, path):\n",
            "        if not os.path.exists(path):\n",
            "            df_chunk.to_csv(path, index=False)\n",
            "        else:\n",
            "            try:\n",
            "                existing_cols = pd.read_csv(path, nrows=0).columns.tolist()\n",
            "                df_aligned = df_chunk.reindex(columns=existing_cols)\n",
            "                df_aligned.to_csv(path, mode='a', header=False, index=False)\n",
            "            except Exception as e:\n",
            "                print(f\"Save Error: {e}\")\n",
            "\n",
            "    # æœˆã”ã¨ã«ãƒ«ãƒ¼ãƒ—\n",
            "    for m in range(int(START_MONTH), int(END_MONTH) + 1):\n",
            "        m_start = date(int(YEAR), m, 1)\n",
            "        m_last = calendar.monthrange(int(YEAR), m)[1]\n",
            "        m_end = date(int(YEAR), m, m_last)\n",
            "        \n",
            "        if m_end < s_date or m_start > e_date:\n",
            "            continue\n",
            "            \n",
            "        curr_s = max(m_start, s_date)\n",
            "        curr_e = min(m_end, e_date)\n",
            "        if curr_s > curr_e: continue\n",
            "        \n",
            "        days = []\n",
            "        c = curr_s\n",
            "        while c <= curr_e:\n",
            "            days.append(c)\n",
            "            c += timedelta(days=1)\n",
            "            \n",
            "        print(f'\\nğŸ“… {YEAR}/{m:02} ã‚’å–å¾—ä¸­...')\n",
            "        print(f'  {len(days)} æ—¥åˆ†ã®æ—¥ä»˜å¯¾è±¡')\n",
            "\n",
            "        for d in tqdm(days, desc=f'  {YEAR}/{m:02}'):\n",
            "            d_str = d.strftime('%Y%m%d')\n",
            "            # JRA (Netkeiba) URL\n",
            "            url = f'https://race.netkeiba.com/top/race_list.html?kaisai_date={d_str}'\n",
            "            daily_data = []\n",
            "            try:\n",
            "                time.sleep(0.5)\n",
            "                headers = {'User-Agent': 'Mozilla/5.0'}\n",
            "                resp = requests.get(url, headers=headers)\n",
            "                resp.encoding = 'EUC-JP'\n",
            "                soup = BeautifulSoup(resp.text, 'html.parser')\n",
            "                \n",
            "                links = soup.select('a[href*=\"race/result.html\"]')\n",
            "                if links:\n",
            "                    for link in links:\n",
            "                        href = link.get('href')\n",
            "                        if href.startswith('../'):\n",
            "                             full_url = f'https://race.netkeiba.com/{href.replace(\"../\", \"\")}'\n",
            "                        elif href.startswith('http'):\n",
            "                             full_url = href\n",
            "                        else:\n",
            "                             full_url = f'https://race.netkeiba.com{href}'\n",
            "\n",
            "                        rid_match = re.search(r'race_id=(\d+)', full_url)\n",
            "                        if rid_match:\n",
            "                            rid = rid_match.group(1)\n",
            "                            # JRA race IDs are 12 digits (YYYYJJRRDDNN) same as Netkeiba DB\n",
            "                            db_url = f\"https://db.netkeiba.com/race/{rid}/\"\n",
            "                            try:\n",
            "                                df = scrape_race_rich(db_url, existing_race_ids=None)\n",
            "                                if df is not None and not df.empty:\n",
            "                                    daily_data.append(df)\n",
            "                                time.sleep(1)\n",
            "                            except Exception as e_race:\n",
            "                                pass\n",
            "\n",
            "                if daily_data:\n",
            "                    df_day = pd.concat(daily_data, ignore_index=True)\n",
            "                    safe_append_csv(df_day, save_path)\n",
            "                    del daily_data\n",
            "                    del df_day\n",
            "                    import gc\n",
            "                    gc.collect()\n",
            "            \n",
            "            except Exception as e_day:\n",
            "                print(f'  æ—¥ä»˜å‡¦ç†ã‚¨ãƒ©ãƒ¼ {d}: {e_day}')\n",
            "\n",
            "    print('å®Œäº†ã—ã¾ã—ãŸã€‚')\n",
            "else:\n",
            "    print('å¹´åº¦ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚')\n"
        ]},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": [
             "# ãƒ‡ãƒ¼ã‚¿æ•´ç†ãƒ»é‡è¤‡å‰Šé™¤ãƒ»ã‚«ãƒ©ãƒ é †åºä¿è¨¼\n",
             "import pandas as pd\n",
             "import os\n",
             "\n",
             "save_path = os.path.join(SAVE_DIR, TARGET_CSV)\n",
             "if os.path.exists(save_path):\n",
             "    print('ãƒ‡ãƒ¼ã‚¿ã®æ•´ç†ã‚’è¡Œã£ã¦ã„ã¾ã™...')\n",
             "    try:\n",
             "        # å…¨ã‚«ãƒ©ãƒ ã‚’æ–‡å­—åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã¿ï¼ˆå‹ãšã‚Œé˜²æ­¢ï¼‰\n",
             "        df_final = pd.read_csv(save_path, dtype=str)\n",
             "        before_len = len(df_final)\n",
             "        if 'race_id' in df_final.columns and 'horse_id' in df_final.columns:\n",
             "            df_final.drop_duplicates(subset=['race_id', 'horse_id'], keep='last', inplace=True)\n",
             "        after_len = len(df_final)\n",
             "        print(f'é‡è¤‡å‰Šé™¤: {before_len} -> {after_len} ({before_len - after_len}ä»¶å‰Šé™¤)')\n",
             "        df_final.to_csv(save_path, index=False)\n",
             "        print('å®Œäº†: ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã‚’ç¢ºèªã—ä¿å­˜ã—ã¾ã—ãŸã€‚')\n",
             "    except Exception as e:\n",
             "        print(f'ãƒ‡ãƒ¼ã‚¿æ•´ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}')\n"
        ]}
    ]
    return create_notebook(cells)

def gen_jra_backfill_nb():
    cleanup_code = read_file('scripts/colab_backfill_helper.py')
    race_scraper_code = read_file('scraper/race_scraper.py')
    
    helper_lines = cleanup_code.splitlines()
    filtered_helper = []
    for line in helper_lines:
        if "from scraper.race_scraper import RaceScraper" in line:
            filtered_helper.append("    pass # Replaced import")
            continue
        if "sys.path.append" in line: continue
        filtered_helper.append(line)
        
    cells = [
        {"cell_type": "markdown", "metadata": {}, "source": ["# ğŸ› ï¸ JRA ãƒ‡ãƒ¼ã‚¿è£œå®Œãƒ„ãƒ¼ãƒ«\n", "æ¬ æã—ã¦ã„ã‚‹è¡€çµ±æƒ…å ±ãŠã‚ˆã³éå»èµ°å±¥æ­´ã‚’è£œå®Œã—ã¾ã™ã€‚"]},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": [
            "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆã™ã‚‹å ´åˆã®ã¿å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
            "from google.colab import drive\n",
            "drive.mount('/content/drive')"
        ]},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": race_scraper_code.splitlines(keepends=True)},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": [l + "\n" for l in filtered_helper]},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": [
            "# è¨­å®š\n",
            "DATA_DIR = '/content/drive/MyDrive/dai-keiba/data/raw' # CSVãŒã‚ã‚‹ãƒ•ã‚©ãƒ«ãƒ€\n",
            "\n",
            "# å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯\n",
            "csv_path = os.path.join(DATA_DIR, 'database.csv')\n",
            "if os.path.exists(csv_path):\n",
            "    print(f'å‡¦ç†å¯¾è±¡: {csv_path}')\n",
            "    fill_bloodline_data(csv_path, mode='JRA')\n",
            "    fill_history_data(csv_path, mode='JRA')\n",
            "    fill_race_metadata(csv_path, mode='JRA')\n",
            "else:\n",
            "    print(f'{csv_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚')\n",
            "    print(f'ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}')\n",
            "    if os.path.exists(DATA_DIR):\n",
            "        print(f'{DATA_DIR} ã®ä¸­èº«: {os.listdir(DATA_DIR)}')\n",
            "    else:\n",
            "        print(f'{DATA_DIR} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè‡ªä½“ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚')"
        ]}
    ]
    return create_notebook(cells)

def gen_nar_scraping_nb():
    # Similar to JRA, but using NAR logic and separate saving logic
    # We will reuse the code style
    
    race_scraper_code = read_file('scraper/race_scraper.py')
    jra_code = read_file('scripts/scraping_logic_v2.py') # Use V2 for NAR too
    
    # We define run_nar_scraping with month support
    nar_execution_logic = """
def run_nar_scraping(year, start_month=1, end_month=12, save_dir='data/raw', target_csv='database_nar.csv'):
    # This function is now embedded in the notebook directly in the NAR execution cell below
    pass
""" 

    # Since we can't easily inline the full logic without a clean file, 
    # and previous step used a placeholder, I will update the placeholder 
    # to accept the month inputs and print them, or use the iter logic previously defined but bounded.
    
    cells = [
         {"cell_type": "markdown", "metadata": {}, "source": ["# ğŸ‡ NAR å…¨ãƒ¬ãƒ¼ã‚¹å–å¾—\n", "ä»¥ä¸‹ã®è¨­å®šå¤‰æ•°ã‚’å¤‰æ›´ã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚NARï¼ˆåœ°æ–¹ç«¶é¦¬ï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ—¥ä»˜é †ã«å–å¾—ã—ã¾ã™ã€‚"]},
         {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": [
             "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆã™ã‚‹å ´åˆã®ã¿å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
             "from google.colab import drive\n",
             "drive.mount('/content/drive')"
         ]},
         {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": [
            "# è¨­å®š (ã“ã“ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„)\n",
            "YEAR = 2025          # å¯¾è±¡å¹´åº¦\n",
            "START_MONTH = 1      # é–‹å§‹æœˆ\n",
            "END_MONTH = 12       # çµ‚äº†æœˆ\n",
            "SAVE_DIR = '/content/drive/MyDrive/dai-keiba/data/raw' # ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€\n",
            "TARGET_CSV = 'database_nar.csv'\n"
         ]},
         {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": race_scraper_code.splitlines(keepends=True)},
         {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": jra_code.splitlines(keepends=True)},
         {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": [
             "# NAR ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯\n",
             "import requests\n",
             "from bs4 import BeautifulSoup\n",
             "import pandas as pd\n",
             "import re\n",
             "from datetime import date, timedelta\n",
             "import calendar\n",
             "import time\n",
             "import os\n",
             "\n",
             "def run_nar_scraping(year, start_month=1, end_month=12, save_dir='data/raw', target_csv='database_nar.csv'):\n",
             "    from tqdm.auto import tqdm\n",
             "    \n",
             "    s_date = date(int(year), int(start_month), 1)\n",
             "    last_day_e = calendar.monthrange(int(year), int(end_month))[1]\n",
             "    e_date = date(int(year), int(end_month), last_day_e)\n",
             "    \n",
             "    today = date.today()\n",
             "    if e_date > today: e_date = today\n",
             "    \n",
             "    print(f'NARãƒ‡ãƒ¼ã‚¿ã‚’ {s_date} ã‹ã‚‰ {e_date} ã¾ã§å–å¾—ã—ã¾ã™...')\n",
             "    print(f'ä¿å­˜å…ˆ: {os.path.join(save_dir, target_csv)}')\n",
             "    \n",
             "    # æœˆã”ã¨ã«ãƒ«ãƒ¼ãƒ—\n",
             "    for m in range(int(start_month), int(end_month) + 1):\n",
             "        # ãã®æœˆã®æ—¥ä»˜ç¯„å›²ã‚’æ±ºå®š\n",
             "        m_start = date(int(year), m, 1)\n",
             "        m_last = calendar.monthrange(int(year), m)[1]\n",
             "        m_end = date(int(year), m, m_last)\n",
             "        \n",
             "        # ç¯„å›²å¤–ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—\n",
             "        if m_end < s_date or m_start > e_date:\n",
             "            continue\n",
             "            \n",
             "        # å®Ÿéš›ã®é–‹å§‹ãƒ»çµ‚äº†ï¼ˆã‚¯ãƒ©ãƒ³ãƒ—ï¼‰\n",
             "        curr_s = max(m_start, s_date)\n",
             "        curr_e = min(m_end, e_date)\n",
             "        \n",
             "        if curr_s > curr_e: continue\n",
             "        \n",
             "        # æ—¥ä»˜ãƒªã‚¹ãƒˆä½œæˆ\n",
             "        days = []\n",
             "        c = curr_s\n",
             "        while c <= curr_e:\n",
             "            days.append(c)\n",
             "            c += timedelta(days=1)\n",
             "            \n",
             "        print(f'\\nğŸ“… {year}/{m:02} ã‚’å–å¾—ä¸­...')\n",
             "        print(f'  {len(days)} æ—¥åˆ†ã®æ—¥ä»˜å¯¾è±¡')\n",
             "        \n",
             "        for d in tqdm(days, desc=f'  {year}/{m:02}'):\n",
             "            d_str = d.strftime('%Y%m%d')\n",
             "            url = f'https://nar.netkeiba.com/top/race_list_sub.html?kaisai_date={d_str}'\n",
             "            \n",
             "            daily_data = [] # 1æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’è²¯ã‚ã‚‹\n",
             "            \n",
             "            try:\n",
             "                 time.sleep(0.5)\n",
             "                 headers = {'User-Agent': 'Mozilla/5.0'}\n",
             "                 resp = requests.get(url, headers=headers)\n",
             "                 resp.encoding = 'EUC-JP'\n",
             "                 soup = BeautifulSoup(resp.text, 'html.parser')\n",
             "                 links = soup.select('a[href*=\"race/result.html\"]')\n",
             "                 \n",
             "                 if links:\n",
             "                     # print(f'  {d}: {len(links)} ãƒ¬ãƒ¼ã‚¹') # ãƒ­ã‚°éå¤šé˜²æ­¢ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ\n",
             "                     for link in links:\n",
             "                         href = link.get('href')\n",
             "                         if href.startswith('../'):\n",
             "                             full_url = f'https://nar.netkeiba.com/{href.replace(\"../\", \"\")}'\n",
             "                         elif href.startswith('http'):\n",
             "                             full_url = href\n",
             "                         else:\n",
             "                             full_url = f'https://nar.netkeiba.com{href}'\n",
             "                         \n",
             "                         try:\n",
             "                             df = scrape_race_rich(full_url, existing_race_ids=None)\n",
             "                             if df is not None and not df.empty:\n",
             "                                 daily_data.append(df)\n",
             "                             time.sleep(1)\n",
             "                         except Exception as e_race:\n",
             "                             pass # ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ã—ã¦æ¬¡ã¸\n",
             "                 \n",
             "                 # 1æ—¥åˆ†ã®ãƒ«ãƒ¼ãƒ—çµ‚äº†å¾Œã€ã¾ã¨ã‚ã¦ä¿å­˜\n",
             "                 if daily_data:\n",
             "                     os.makedirs(save_dir, exist_ok=True)\n",
             "                     csv_file = os.path.join(save_dir, target_csv)\n",
             "                     try:\n",
             "                         df_day = pd.concat(daily_data, ignore_index=True)\n",
             "                         \n",
             "                         if not os.path.exists(csv_file):\n",
             "                             df_day.to_csv(csv_file, index=False)\n",
             "                         else:\n",
             "                             existing_cols = pd.read_csv(csv_file, nrows=0).columns.tolist()\n",
             "                             df_aligned = df_day.reindex(columns=existing_cols)\n",
             "                             df_aligned.to_csv(csv_file, mode='a', header=False, index=False)\n",
             "                     except Exception as e_save:\n",
             "                          print(f\"  ä¿å­˜ã‚¨ãƒ©ãƒ¼ ({d}): {e_save}\")\n",
             "                     \n",
             "                     # ãƒ¡ãƒ¢ãƒªè§£æ”¾\n",
             "                     del daily_data\n",
             "                     import gc\n",
             "                     gc.collect()\n",
             "            \n",
             "            except Exception as e_day:\n",
             "                print(f'  æ—¥ä»˜å‡¦ç†ã‚¨ãƒ©ãƒ¼ {d}: {e_day}')\n",
             "    \n",
             "    print('å®Œäº†ã—ã¾ã—ãŸã€‚')\n"
         ]},
         {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": [
             "# å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯\n",
             "if YEAR:\n",
             "    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
             "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
             "    run_nar_scraping(YEAR, START_MONTH, END_MONTH, save_dir=SAVE_DIR, target_csv=TARGET_CSV)\n"
         ]},
         {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": [
             "# ãƒ‡ãƒ¼ã‚¿æ•´ç†ãƒ»é‡è¤‡å‰Šé™¤ãƒ»ã‚«ãƒ©ãƒ é †åºä¿è¨¼ (NAR)\n",
             "import pandas as pd\n",
             "import os\n",
             "\n",
             "csv_path = os.path.join(SAVE_DIR, TARGET_CSV)\n",
             "if os.path.exists(csv_path):\n",
             "    print('ãƒ‡ãƒ¼ã‚¿ã®æ•´ç†ã‚’è¡Œã£ã¦ã„ã¾ã™...')\n",
             "    try:\n",
             "        # å…¨ã‚«ãƒ©ãƒ ã‚’æ–‡å­—åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã¿ï¼ˆå‹ãšã‚Œé˜²æ­¢ï¼‰\n",
             "        df_final = pd.read_csv(csv_path, dtype=str)\n",
             "        before_len = len(df_final)\n",
             "        if 'race_id' in df_final.columns and 'horse_id' in df_final.columns:\n",
             "            df_final.drop_duplicates(subset=['race_id', 'horse_id'], keep='last', inplace=True)\n",
             "        after_len = len(df_final)\n",
             "        print(f'é‡è¤‡å‰Šé™¤: {before_len} -> {after_len} ({before_len - after_len}ä»¶å‰Šé™¤)')\n",
             "        df_final.to_csv(save_path, index=False)\n",
             "        print('å®Œäº†: ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã‚’ç¢ºèªã—ä¿å­˜ã—ã¾ã—ãŸã€‚')\n",
             "    except Exception as e:\n",
             "        print(f'ãƒ‡ãƒ¼ã‚¿æ•´ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}')\n"
         ]}
    ]
    return create_notebook(cells)

def gen_nar_backfill_nb():
    # Similar to JRA backfill but NAR file
    cleanup_code = read_file('scripts/colab_backfill_helper.py')
    race_scraper_code = read_file('scraper/race_scraper.py')
    
    helper_lines = cleanup_code.splitlines()
    filtered_helper = []
    for line in helper_lines:
        if "from scraper.race_scraper import RaceScraper" in line:
            filtered_helper.append("    pass # Replaced import")
            continue
        if "sys.path.append" in line: continue
        filtered_helper.append(line)
        
    cells = [
        {"cell_type": "markdown", "metadata": {}, "source": ["# ğŸ› ï¸ NAR ãƒ‡ãƒ¼ã‚¿è£œå®Œãƒ„ãƒ¼ãƒ«"]},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": [
            "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆã™ã‚‹å ´åˆã®ã¿å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
            "from google.colab import drive\n",
            "drive.mount('/content/drive')"
        ]},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": race_scraper_code.splitlines(keepends=True)},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": [l + "\n" for l in filtered_helper]},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": [
            "# è¨­å®š\n",
            "DATA_DIR = '/content/drive/MyDrive/dai-keiba/data/raw' # CSVãŒã‚ã‚‹ãƒ•ã‚©ãƒ«ãƒ€\n",
            "\n",
            "# å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯\n",
            "csv_path = os.path.join(DATA_DIR, 'database_nar.csv')\n",
            "if os.path.exists(csv_path):\n",
            "    print(f'å‡¦ç†å¯¾è±¡: {csv_path}')\n",
            "    fill_bloodline_data(csv_path, mode='NAR')\n",
            "    fill_history_data(csv_path, mode='NAR')\n",
            "    fill_race_metadata(csv_path, mode='NAR')\n",
            "else:\n",
            "    print(f'{csv_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚')\n",
            "    print(f'ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}')\n",
            "    if os.path.exists(DATA_DIR):\n",
            "        print(f'{DATA_DIR} ã®ä¸­èº«: {os.listdir(DATA_DIR)}')\n",
            "    else:\n",
            "        print(f'{DATA_DIR} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè‡ªä½“ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚')"
        ]}
    ]
    return create_notebook(cells)

if __name__ == "__main__":
    os.makedirs('notebooks', exist_ok=True)
    
    with open('notebooks/Colab_JRA_Scraping.ipynb', 'w') as f:
        f.write(gen_jra_scraping_nb())
        
    with open('notebooks/Colab_JRA_Backfill.ipynb', 'w') as f:
        f.write(gen_jra_backfill_nb())
        
    with open('notebooks/Colab_NAR_Backfill.ipynb', 'w') as f:
        f.write(gen_nar_backfill_nb())
        
    # NAR Scraping logic reuse
    with open('notebooks/Colab_NAR_Scraping.ipynb', 'w') as f:
        f.write(gen_nar_scraping_nb())
