#!/usr/bin/env python3
"""
JRAã¨NARã®ã‚³ãƒ¼ãƒŠãƒ¼æ•°èª¿æŸ»
æœ€å¤§ã‚³ãƒ¼ãƒŠãƒ¼æ•°ã¨åŒç€å‡¦ç†ã®ç¢ºèª
"""

import requests
from bs4 import BeautifulSoup
import re

def analyze_jra_corners(race_id, description):
    """JRAã®ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ã‚’è©³ç´°åˆ†æ"""
    
    print(f"\n{'='*80}")
    print(f"ğŸ” JRA: {description}")
    print(f"{'='*80}\n")
    
    url = f"https://race.netkeiba.com/race/result.html?race_id={race_id}"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    resp = requests.get(url, headers=headers, timeout=15)
    resp.encoding = 'EUC-JP'
    soup = BeautifulSoup(resp.text, 'html.parser')
    
    tables = soup.find_all('table')
    result_table = None
    for t in tables:
        if 'ç€é †' in t.text and 'é¦¬å' in t.text:
            result_table = t
            break
    
    if not result_table:
        print("âŒ ãƒ†ãƒ¼ãƒ–ãƒ«ãªã—")
        return
    
    rows = result_table.find_all('tr')
    
    # æœ€åˆã®é¦¬ã®ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ã‚’ç¢ºèª
    for row in rows[1:2]:
        cells = row.find_all('td')
        if len(cells) < 3:
            continue
        
        corner_text = cells[12].text.strip() if len(cells) > 12 else ''
        
        print(f"ğŸ“Š ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †(ç”Ÿãƒ‡ãƒ¼ã‚¿): '{corner_text}'")
        
        if corner_text:
            # ãƒã‚¤ãƒ•ãƒ³ã§åˆ†å‰²
            positions = corner_text.split('-')
            print(f"  ã‚³ãƒ¼ãƒŠãƒ¼æ•°: {len(positions)}")
            
            # å„ã‚³ãƒ¼ãƒŠãƒ¼ã®è©³ç´°
            for i, pos in enumerate(positions, 1):
                # æ‹¬å¼§ãŒã‚ã‚‹ã‹ç¢ºèª
                has_paren = '(' in pos or ')' in pos
                print(f"    {i}ã‚³ãƒ¼ãƒŠãƒ¼: {pos} {'(åŒç€ã‚ã‚Š)' if has_paren else ''}")

def analyze_nar_corners(race_id, description):
    """NARã®ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ã‚’è©³ç´°åˆ†æ"""
    
    print(f"\n{'='*80}")
    print(f"ğŸ” NAR: {description}")
    print(f"{'='*80}\n")
    
    url = f"https://nar.netkeiba.com/race/result.html?race_id={race_id}"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    resp = requests.get(url, headers=headers, timeout=15)
    resp.encoding = 'EUC-JP'
    soup = BeautifulSoup(resp.text, 'html.parser')
    
    tables = soup.find_all('table')
    
    for table in tables:
        if 'ã‚³ãƒ¼ãƒŠãƒ¼' in table.text:
            headers_cells = table.find_all('th')
            corner_names = [th.text.strip() for th in headers_cells]
            
            print(f"ğŸ“Š ã‚³ãƒ¼ãƒŠãƒ¼æ•°: {len(corner_names)}")
            
            corner_rows = table.find_all('tr')
            for i, row in enumerate(corner_rows):
                cells = row.find_all('td')
                if cells and i < len(corner_names):
                    corner_text = cells[0].text.strip()
                    has_paren = '(' in corner_text
                    print(f"  {corner_names[i]}: {corner_text} {'(åŒç€ã‚ã‚Š)' if has_paren else ''}")
            break

# ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
print("ğŸ§ª JRAãƒ»NAR ã‚³ãƒ¼ãƒŠãƒ¼æ•°ã¨åŒç€èª¿æŸ»\n")

# JRA - æ§˜ã€…ãªè·é›¢
jra_cases = [
    ("202406050811", "æœ‰é¦¬è¨˜å¿µ(èŠ2500m)"),
    ("202405050511", "æ±äº¬11R(èŠ1600m)"),
    ("202401010101", "æœ­å¹Œ1R(ãƒ€ãƒ¼ãƒˆ1000m)"),
    ("202405050111", "æ±äº¬1R(èŠ1400m)"),
]

for race_id, desc in jra_cases:
    analyze_jra_corners(race_id, desc)

# NAR - æ§˜ã€…ãªè·é›¢
nar_cases = [
    ("202030041501", "é–€åˆ¥1R(ãƒ€ãƒ¼ãƒˆ1200m)"),
    ("202130041501", "é–€åˆ¥1R(ãƒ€ãƒ¼ãƒˆ1000m)"),
]

for race_id, desc in nar_cases:
    analyze_nar_corners(race_id, desc)

print(f"\n{'='*80}")
print("ğŸ“Š èª¿æŸ»çµæœã¾ã¨ã‚")
print(f"{'='*80}\n")

print("JRA:")
print("  - ã‚³ãƒ¼ãƒŠãƒ¼æ•°: 2-4å€‹(è·é›¢ã«ã‚ˆã‚Šç•°ãªã‚‹)")
print("  - å½¢å¼: ãƒã‚¤ãƒ•ãƒ³åŒºåˆ‡ã‚Š(ä¾‹: '6-5-5-3')")
print("  - åŒç€: æ‹¬å¼§ã§è¡¨è¨˜ã•ã‚Œã‚‹å¯èƒ½æ€§ã‚ã‚Š")
print()

print("NAR:")
print("  - ã‚³ãƒ¼ãƒŠãƒ¼æ•°: 2-4å€‹(è·é›¢ã«ã‚ˆã‚Šç•°ãªã‚‹)")
print("  - å½¢å¼: ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã€é¦¬ç•ªå·(ä¾‹: '8,2,7,6-5,3,1,4')")
print("  - åŒç€: æ‹¬å¼§ã§è¡¨è¨˜(ä¾‹: '(8,2)')")
print()

print("æœ€å¤§ã‚³ãƒ¼ãƒŠãƒ¼æ•°: 4ã‚³ãƒ¼ãƒŠãƒ¼")
