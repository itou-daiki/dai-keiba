#!/usr/bin/env python3
"""
JRA Basic v2ã«å …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’è¿½åŠ 
"""

import json

notebook_path = "/Users/itoudaiki/Program/dai-keiba/notebooks/Colab_JRA_Basic_v2.ipynb"

with open(notebook_path, 'r', encoding='utf-8') as f:
    nb = json.load(f)

for i, cell in enumerate(nb['cells']):
    if cell['cell_type'] == 'code':
        source = ''.join(cell['source'])
        
        if 'def scrape_race_basic(' in source:
            print(f"âœ… ã‚»ãƒ«{i}: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°ã‚’ç™ºè¦‹")
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«æ¤œå‡ºå¤±æ•—æ™‚ã®ãƒ­ã‚°ã‚’æ”¹å–„
            old_error_msg = """        if not result_table:
            print(f"  âš ï¸ ãƒ¬ãƒ¼ã‚¹çµæœãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {race_id}")
            return None"""
            
            new_error_msg = """        if not result_table:
            print(f"  âš ï¸ ãƒ¬ãƒ¼ã‚¹çµæœãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {race_id}")
            print(f"     URL: {url}")
            print(f"     ç·ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables)}")
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ä¸€éƒ¨ã‚’è¡¨ç¤º(ãƒ‡ãƒãƒƒã‚°ç”¨)
            if len(soup.text) < 100:
                print(f"     ãƒšãƒ¼ã‚¸å†…å®¹ãŒå°‘ãªã™ãã¾ã™(ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å¯èƒ½æ€§)")
            return None"""
            
            source = source.replace(old_error_msg, new_error_msg)
            
            # HTTPã‚¨ãƒ©ãƒ¼ã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’è¿½åŠ 
            old_try_block = """    try:
        # ãƒšãƒ¼ã‚¸å–å¾—
        time.sleep(0.5)
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(url, headers=headers, timeout=15)
        resp.encoding = 'EUC-JP'
        soup = BeautifulSoup(resp.text, 'html.parser')"""
            
            new_try_block = """    try:
        # ãƒšãƒ¼ã‚¸å–å¾—
        time.sleep(0.5)
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(url, headers=headers, timeout=15)
        
        # HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒã‚§ãƒƒã‚¯
        if resp.status_code != 200:
            print(f"  âš ï¸ HTTP {resp.status_code}: {race_id}")
            return None
        
        resp.encoding = 'EUC-JP'
        soup = BeautifulSoup(resp.text, 'html.parser')"""
            
            source = source.replace(old_try_block, new_try_block)
            
            cell['source'] = [line + '\n' for line in source.split('\n')]
            if cell['source'] and cell['source'][-1] == '\n':
                cell['source'][-1] = cell['source'][-1].rstrip('\n')
            
            print("  âœ… ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å¼·åŒ–")
            print("  âœ… HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒã‚§ãƒƒã‚¯ã‚’è¿½åŠ ")
            print("  âœ… ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ ")
            break

with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(nb, f, ensure_ascii=False, indent=1)

print(f"\nğŸ’¾ ä¿å­˜å®Œäº†: {notebook_path}")
print("\nğŸ“ æ”¹å–„å†…å®¹:")
print("  - HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯")
print("  - ãƒ†ãƒ¼ãƒ–ãƒ«æ¤œå‡ºå¤±æ•—æ™‚ã®è©³ç´°ãƒ­ã‚°")
print("  - ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ¤œå‡º")
