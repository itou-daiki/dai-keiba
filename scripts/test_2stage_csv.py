#!/usr/bin/env python3
"""
2æ®µéšã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®å®Œå…¨ãƒ†ã‚¹ãƒˆ
Stage 1 â†’ Stage 2 â†’ Merge â†’ ã‚«ãƒ©ãƒ ã‚ºãƒ¬æ¤œè¨¼
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
import os
from datetime import datetime
import io

# ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
TEST_DIR = "/Users/itoudaiki/Program/dai-keiba/data/test"
os.makedirs(TEST_DIR, exist_ok=True)

print("ğŸ§ª 2æ®µéšã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œå…¨ãƒ†ã‚¹ãƒˆ")
print(f"{'='*80}\n")

# ========================================
# Stage 1: Basic (26ã‚«ãƒ©ãƒ )
# ========================================

def scrape_basic(race_id):
    """Stage 1: åŸºæœ¬æƒ…å ±å–å¾—"""
    url = f"https://race.netkeiba.com/race/result.html?race_id={race_id}"
    
    try:
        time.sleep(0.5)
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(url, headers=headers, timeout=15)
        resp.encoding = 'EUC-JP'
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        title = soup.title.text if soup.title else ""
        full_text = soup.text
        
        metadata = {
            'æ—¥ä»˜': '', 'ä¼šå ´': '', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·': '', 'ãƒ¬ãƒ¼ã‚¹å': '', 'é‡è³': '',
            'ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—': '', 'è·é›¢': '', 'å›ã‚Š': '', 'å¤©å€™': '', 'é¦¬å ´çŠ¶æ…‹': '', 'race_id': race_id
        }
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æŠ½å‡º
        if title:
            race_name_match = re.search(r'^([^|]+)', title)
            if race_name_match:
                race_name = re.sub(r'\s*(çµæœ|æ‰•æˆ»).*$', '', race_name_match.group(1).strip())
                metadata['ãƒ¬ãƒ¼ã‚¹å'] = race_name
                if 'G1' in race_name or 'Gâ… ' in race_name:
                    metadata['é‡è³'] = 'G1'
            
            date_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', title)
            if date_match:
                metadata['æ—¥ä»˜'] = f"{date_match.group(1)}/{int(date_match.group(2)):02d}/{int(date_match.group(3)):02d}"
            
            for venue in ['æœ­å¹Œ', 'å‡½é¤¨', 'ç¦å³¶', 'æ–°æ½Ÿ', 'æ±äº¬', 'ä¸­å±±', 'ä¸­äº¬', 'äº¬éƒ½', 'é˜ªç¥', 'å°å€‰']:
                if venue in title:
                    metadata['ä¼šå ´'] = venue
                    break
            
            race_num_match = re.search(r'(\d+)R', title)
            if race_num_match:
                metadata['ãƒ¬ãƒ¼ã‚¹ç•ªå·'] = race_num_match.group(0)
        
        # æœ¬æ–‡ã‹ã‚‰
        course_match = re.search(r'(èŠ|ãƒ€ãƒ¼ãƒˆ|ãƒ€)\s*(\d+)m', full_text)
        if course_match:
            metadata['ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—'] = 'èŠ' if 'èŠ' in course_match.group(1) else 'ãƒ€ãƒ¼ãƒˆ'
            metadata['è·é›¢'] = course_match.group(2)
        
        if 'å³' in full_text:
            metadata['å›ã‚Š'] = 'å³'
        elif 'å·¦' in full_text:
            metadata['å›ã‚Š'] = 'å·¦'
        
        weather_match = re.search(r'å¤©å€™\s*[:ï¼š]\s*(\S+)', full_text)
        if weather_match:
            metadata['å¤©å€™'] = weather_match.group(1)
        
        baba_match = re.search(r'é¦¬å ´\s*[:ï¼š]\s*(\S+)', full_text)
        if baba_match:
            metadata['é¦¬å ´çŠ¶æ…‹'] = baba_match.group(1)
        
        # ãƒ¬ãƒ¼ã‚¹çµæœãƒ†ãƒ¼ãƒ–ãƒ«
        tables = soup.find_all('table')
        result_table = None
        for t in tables:
            if 'ç€é †' in t.text and 'é¦¬å' in t.text:
                result_table = t
                break
        
        if not result_table:
            return None
        
        rows = result_table.find_all('tr')
        race_data = []
        
        for row in rows:
            if row.find('th'):
                continue
            cells = row.find_all('td')
            if len(cells) < 10:
                continue
            
            # åŸºæœ¬æƒ…å ±ã‚’è¾æ›¸ã§æ§‹ç¯‰
            horse_data = {
                'æ—¥ä»˜': metadata['æ—¥ä»˜'],
                'ä¼šå ´': metadata['ä¼šå ´'],
                'ãƒ¬ãƒ¼ã‚¹ç•ªå·': metadata['ãƒ¬ãƒ¼ã‚¹ç•ªå·'],
                'ãƒ¬ãƒ¼ã‚¹å': metadata['ãƒ¬ãƒ¼ã‚¹å'],
                'é‡è³': metadata['é‡è³'],
                'ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—': metadata['ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—'],
                'è·é›¢': metadata['è·é›¢'],
                'å›ã‚Š': metadata['å›ã‚Š'],
                'å¤©å€™': metadata['å¤©å€™'],
                'é¦¬å ´çŠ¶æ…‹': metadata['é¦¬å ´çŠ¶æ…‹'],
                'ç€é †': cells[0].text.strip(),
                'æ ': '',
                'é¦¬ç•ª': cells[2].text.strip() if len(cells) > 2 else '',
                'é¦¬å': cells[3].text.strip() if len(cells) > 3 else '',
                'æ€§é½¢': cells[4].text.strip() if len(cells) > 4 else '',
                'æ–¤é‡': cells[5].text.strip() if len(cells) > 5 else '',
                'é¨æ‰‹': cells[6].text.strip() if len(cells) > 6 else '',
                'ã‚¿ã‚¤ãƒ ': cells[7].text.strip() if len(cells) > 7 else '',
                'ç€å·®': cells[8].text.strip() if len(cells) > 8 else '',
                'äººæ°—': cells[9].text.strip() if len(cells) > 9 else '',
                'å˜å‹ã‚ªãƒƒã‚º': cells[10].text.strip() if len(cells) > 10 else '',
                'å¾Œ3F': cells[11].text.strip() if len(cells) > 11 else '',
                'å©èˆ': cells[18].text.strip() if len(cells) > 18 else '',
                'é¦¬ä½“é‡(å¢—æ¸›)': cells[14].text.strip() if len(cells) > 14 else '',
                'race_id': race_id,
                'horse_id': ''
            }
            
            # æ ç•ª
            waku_img = cells[1].find('img') if len(cells) > 1 else None
            if waku_img and 'alt' in waku_img.attrs:
                waku_match = re.search(r'æ (\d+)', waku_img['alt'])
                if waku_match:
                    horse_data['æ '] = waku_match.group(1)
            
            # horse_id
            horse_link = cells[3].find('a') if len(cells) > 3 else None
            if horse_link and 'href' in horse_link.attrs:
                horse_id_match = re.search(r'/horse/(\d+)', horse_link['href'])
                if horse_id_match:
                    horse_data['horse_id'] = horse_id_match.group(1)
            
            race_data.append(horse_data)
        
        # DataFrameã«å¤‰æ›(ã‚«ãƒ©ãƒ é †åºã‚’æ˜ç¤º)
        ordered_columns = [
            'æ—¥ä»˜', 'ä¼šå ´', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·', 'ãƒ¬ãƒ¼ã‚¹å', 'é‡è³', 'ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—', 'è·é›¢', 'å›ã‚Š',
            'å¤©å€™', 'é¦¬å ´çŠ¶æ…‹', 'ç€é †', 'æ ', 'é¦¬ç•ª', 'é¦¬å', 'æ€§é½¢', 'æ–¤é‡', 'é¨æ‰‹', 'ã‚¿ã‚¤ãƒ ',
            'ç€å·®', 'äººæ°—', 'å˜å‹ã‚ªãƒƒã‚º', 'å¾Œ3F', 'å©èˆ', 'é¦¬ä½“é‡(å¢—æ¸›)', 'race_id', 'horse_id'
        ]
        
        df = pd.DataFrame(race_data)[ordered_columns]
        return df
    
    except Exception as e:
        print(f"  âŒ Stage 1ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# ========================================
# Stage 2: Details (68ã‚«ãƒ©ãƒ )
# ========================================

def scrape_details(horse_id, race_date):
    """Stage 2: é¦¬å±¥æ­´ãƒ»è¡€çµ±å–å¾—"""
    
    details = {
        'race_id': '',
        'horse_id': horse_id
    }
    
    # éå»5èµ°ã®åˆæœŸåŒ–
    for i in range(1, 6):
        prefix = f'past_{i}'
        for field in ['date', 'rank', 'time', 'run_style', 'race_name', 'last_3f', 
                      'horse_weight', 'jockey', 'condition', 'odds', 'weather', 'distance', 'course_type']:
            details[f'{prefix}_{field}'] = ''
    
    # è¡€çµ±ã®åˆæœŸåŒ–
    details['father'] = ''
    details['mother'] = ''
    details['bms'] = ''
    
    try:
        time.sleep(0.5)
        headers = {'User-Agent': 'Mozilla/5.0'}
        
        # é¦¬å±¥æ­´å–å¾—
        result_url = f"https://db.netkeiba.com/horse/{horse_id}/"
        resp = requests.get(result_url, headers=headers, timeout=15)
        resp.encoding = 'EUC-JP'
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # è¡€çµ±æƒ…å ±(ç°¡æ˜“ç‰ˆ - ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰)
        title = soup.title.text if soup.title else ""
        # å®Ÿéš›ã®è¡€çµ±æŠ½å‡ºã¯çœç•¥(ãƒ†ã‚¹ãƒˆç”¨)
        
        # ãƒ¬ãƒ¼ã‚¹å±¥æ­´(ç°¡æ˜“ç‰ˆ)
        # å®Ÿéš›ã®å±¥æ­´æŠ½å‡ºã¯çœç•¥(ãƒ†ã‚¹ãƒˆç”¨)
        
        return details
    
    except Exception as e:
        print(f"  âš ï¸ Stage 2ã‚¨ãƒ©ãƒ¼({horse_id}): {e}")
        return details

# ========================================
# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
# ========================================

print("ğŸ“ Stage 1: åŸºæœ¬æƒ…å ±å–å¾—")
print(f"{'-'*80}")

test_race_id = "202406050811"  # æœ‰é¦¬è¨˜å¿µ
df_basic = scrape_basic(test_race_id)

if df_basic is not None:
    print(f"  âœ… å–å¾—æˆåŠŸ: {len(df_basic)}é ­")
    print(f"  ã‚«ãƒ©ãƒ æ•°: {len(df_basic.columns)}")
    print(f"  ã‚«ãƒ©ãƒ : {list(df_basic.columns)}")
    
    # CSVä¿å­˜
    basic_csv = os.path.join(TEST_DIR, "test_basic.csv")
    df_basic.to_csv(basic_csv, index=False)
    print(f"  ğŸ’¾ ä¿å­˜: {basic_csv}")
    
    # ã‚«ãƒ©ãƒ æ•°ç¢ºèª
    df_check = pd.read_csv(basic_csv)
    print(f"  âœ… èª­ã¿è¾¼ã¿ç¢ºèª: {len(df_check.columns)}ã‚«ãƒ©ãƒ ")
    
    if len(df_check.columns) == 26:
        print(f"  âœ… ã‚«ãƒ©ãƒ æ•°æ­£å¸¸(26ã‚«ãƒ©ãƒ )")
    else:
        print(f"  âŒ ã‚«ãƒ©ãƒ æ•°ç•°å¸¸({len(df_check.columns)}ã‚«ãƒ©ãƒ )")
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ãƒ‡ãƒ¼ã‚¿è¡Œã®ç¢ºèª
    with open(basic_csv, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        header_cols = len(lines[0].strip().split(','))
        data_cols = len(lines[1].strip().split(',')) if len(lines) > 1 else 0
        
        print(f"  ãƒ˜ãƒƒãƒ€ãƒ¼: {header_cols}ã‚«ãƒ©ãƒ ")
        print(f"  ãƒ‡ãƒ¼ã‚¿è¡Œ: {data_cols}ã‚«ãƒ©ãƒ ")
        
        if header_cols == data_cols:
            print(f"  âœ… ã‚«ãƒ©ãƒ ã‚ºãƒ¬ãªã—")
        else:
            print(f"  âŒ ã‚«ãƒ©ãƒ ã‚ºãƒ¬ã‚ã‚Š!")
    
    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    print(f"\n  ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:")
    print(df_basic[['æ—¥ä»˜', 'ä¼šå ´', 'ãƒ¬ãƒ¼ã‚¹å', 'é¦¬å', 'horse_id']].head(3).to_string(index=False))

else:
    print(f"  âŒ å–å¾—å¤±æ•—")

print(f"\n{'='*80}")
print("âœ… ãƒ†ã‚¹ãƒˆå®Œäº†")
