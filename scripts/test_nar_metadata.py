#!/usr/bin/env python3
"""
NARç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–¢æ•°ã®ãƒ†ã‚¹ãƒˆ
"""

import requests
from bs4 import BeautifulSoup
import re

def extract_nar_metadata(soup, url):
    """NARç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
    metadata = {
        'æ—¥ä»˜': '', 'ä¼šå ´': '', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·': '', 'ãƒ¬ãƒ¼ã‚¹å': '', 'é‡è³': '',
        'ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—': '', 'è·é›¢': '', 'å›ã‚Š': '', 'å¤©å€™': '', 'é¦¬å ´çŠ¶æ…‹': '', 'race_id': ''
    }
    
    try:
        title = soup.title.text if soup.title else ""
        full_text = soup.text
        
        print(f"ã‚¿ã‚¤ãƒˆãƒ«: {title}\n")
        
        if title:
            # ãƒ¬ãƒ¼ã‚¹å(ã‚¿ã‚¤ãƒˆãƒ«ã®æœ€åˆã®éƒ¨åˆ†)
            race_name_match = re.search(r'^([^|]+)', title)
            if race_name_match:
                race_name_full = race_name_match.group(1).strip()
                race_name = re.sub(r'\s*(çµæœ|æ‰•æˆ»|æ‰•ã„æˆ»ã—).*$', '', race_name_full).strip()
                metadata['ãƒ¬ãƒ¼ã‚¹å'] = race_name
                print(f"âœ… ãƒ¬ãƒ¼ã‚¹å: {race_name}")
            
            # æ—¥ä»˜(ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰)
            date_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', title)
            if date_match:
                year = date_match.group(1)
                month = f"{int(date_match.group(2)):02d}"
                day = f"{int(date_match.group(3)):02d}"
                metadata['æ—¥ä»˜'] = f"{year}/{month}/{day}"
                print(f"âœ… æ—¥ä»˜: {metadata['æ—¥ä»˜']}")
            
            # ä¼šå ´(NARä¼šå ´ãƒªã‚¹ãƒˆ)
            nar_venues = [
                'é–€åˆ¥', 'ç››å²¡', 'æ°´æ²¢', 'æµ¦å’Œ', 'èˆ¹æ©‹', 'å¤§äº•', 'å·å´', 'é‡‘æ²¢', 
                'ç¬ æ¾', 'åå¤å±‹', 'åœ’ç”°', 'å§«è·¯', 'é«˜çŸ¥', 'ä½è³€', 'ã°ã‚“ãˆã„å¸¯åºƒ'
            ]
            for venue in nar_venues:
                if venue in title:
                    metadata['ä¼šå ´'] = venue
                    print(f"âœ… ä¼šå ´: {venue}")
                    break
            
            # ãƒ¬ãƒ¼ã‚¹ç•ªå·
            race_num_match = re.search(r'(\d+)R', title)
            if race_num_match:
                metadata['ãƒ¬ãƒ¼ã‚¹ç•ªå·'] = race_num_match.group(0)
                print(f"âœ… ãƒ¬ãƒ¼ã‚¹ç•ªå·: {metadata['ãƒ¬ãƒ¼ã‚¹ç•ªå·']}")
        
        # æœ¬æ–‡ã‹ã‚‰
        # ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ãƒ»è·é›¢
        course_match = re.search(r'(èŠ|ãƒ€ãƒ¼ãƒˆ|ãƒ€)\s*(\d+)m', full_text)
        if course_match:
            course_type = course_match.group(1)
            metadata['ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—'] = 'èŠ' if 'èŠ' in course_type else 'ãƒ€ãƒ¼ãƒˆ'
            metadata['è·é›¢'] = course_match.group(2)
            print(f"âœ… ã‚³ãƒ¼ã‚¹: {metadata['ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—']} {metadata['è·é›¢']}m")
        else:
            # NARç‰¹æœ‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
            # ã€Œãƒ€1200mã€ã®ã‚ˆã†ãªè¡¨è¨˜ã‚’æ¢ã™
            course_match2 = re.search(r'(ãƒ€)(\d{3,4})', full_text)
            if course_match2:
                metadata['ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—'] = 'ãƒ€ãƒ¼ãƒˆ'
                metadata['è·é›¢'] = course_match2.group(2)
                print(f"âœ… ã‚³ãƒ¼ã‚¹(NAR): {metadata['ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—']} {metadata['è·é›¢']}m")
        
        # å¤©å€™
        weather_match = re.search(r'å¤©å€™\s*[:ï¼š]?\s*(\S+)', full_text)
        if weather_match:
            metadata['å¤©å€™'] = weather_match.group(1)
            print(f"âœ… å¤©å€™: {metadata['å¤©å€™']}")
        
        # é¦¬å ´çŠ¶æ…‹
        baba_match = re.search(r'é¦¬å ´\s*[:ï¼š]?\s*(\S+)', full_text)
        if baba_match:
            metadata['é¦¬å ´çŠ¶æ…‹'] = baba_match.group(1)
            print(f"âœ… é¦¬å ´çŠ¶æ…‹: {metadata['é¦¬å ´çŠ¶æ…‹']}")
        
        # race_id(URLã‹ã‚‰)
        race_id_match = re.search(r'race_id=(\d+)', url)
        if race_id_match:
            metadata['race_id'] = race_id_match.group(1)
            print(f"âœ… race_id: {metadata['race_id']}")
        
        return metadata
    
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return metadata

# ãƒ†ã‚¹ãƒˆ
if __name__ == "__main__":
    race_id = '202030041501'
    url = f'https://nar.netkeiba.com/race/result.html?race_id={race_id}'
    
    print(f"ğŸ§ª NAR ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ†ã‚¹ãƒˆ")
    print(f"{'='*80}\n")
    
    headers = {'User-Agent': 'Mozilla/5.0'}
    resp = requests.get(url, headers=headers, timeout=15)
    resp.encoding = 'EUC-JP'
    soup = BeautifulSoup(resp.text, 'html.parser')
    
    metadata = extract_nar_metadata(soup, url)
    
    print(f"\n{'='*80}")
    print(f"ğŸ“Š æŠ½å‡ºçµæœ:")
    for key, value in metadata.items():
        status = "âœ…" if value else "âŒ"
        print(f"  {status} {key}: {value}")
    
    filled = sum(1 for v in metadata.values() if v)
    print(f"\næˆåŠŸç‡: {filled}/11 ({filled/11*100:.0f}%)")
