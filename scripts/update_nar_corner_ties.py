#!/usr/bin/env python3
"""
NAR Basic v2ã®ã‚³ãƒ¼ãƒŠãƒ¼æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ã‚’åŒç€å¯¾å¿œç‰ˆã«æ›´æ–°
"""

import json

notebook_path = "/Users/itoudaiki/Program/dai-keiba/notebooks/Colab_NAR_Basic_v2.ipynb"

with open(notebook_path, 'r', encoding='utf-8') as f:
    nb = json.load(f)

for i, cell in enumerate(nb['cells']):
    if cell['cell_type'] == 'code':
        source = ''.join(cell['source'])
        
        if 'def scrape_race_basic(' in source:
            print(f"âœ… ã‚»ãƒ«{i}: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°ã‚’ç™ºè¦‹")
            
            # ã‚³ãƒ¼ãƒŠãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«å–å¾—éƒ¨åˆ†ã‚’ä¿®æ­£
            old_corner_table = """        # ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ãƒ†ãƒ¼ãƒ–ãƒ«å–å¾—(NAR)
        corner_data = {}
        for table in tables:
            if 'ã‚³ãƒ¼ãƒŠãƒ¼' in table.text and 'é€šé' in table.text:
                corner_rows = table.find_all('tr')
                for row in corner_rows:
                    cells = row.find_all('td')
                    if len(cells) >= 2:
                        corner_name = cells[0].text.strip()
                        corner_text = cells[1].text.strip()
                        corner_data[corner_name] = corner_text
                break"""
            
            new_corner_table = """        # ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ãƒ†ãƒ¼ãƒ–ãƒ«å–å¾—(NAR)
        corner_data = {}
        for table in tables:
            if 'ã‚³ãƒ¼ãƒŠãƒ¼' in table.text:
                headers_cells = table.find_all('th')
                corner_names = [th.text.strip() for th in headers_cells]
                corner_rows = table.find_all('tr')
                for j, row in enumerate(corner_rows):
                    cells = row.find_all('td')
                    if cells and j < len(corner_names):
                        corner_data[corner_names[j]] = cells[0].text.strip()
                break"""
            
            source = source.replace(old_corner_table, new_corner_table)
            
            # ã‚³ãƒ¼ãƒŠãƒ¼é †ä½æŠ½å‡ºéƒ¨åˆ†ã‚’åŒç€å¯¾å¿œç‰ˆã«ä¿®æ­£
            old_corner_extract = """            # ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ä½ã®æŠ½å‡º(NAR: é¦¬ç•ªå·å½¢å¼)
            umaban = horse_data['é¦¬ç•ª']
            for corner_num in range(1, 5):
                corner_key = f'{corner_num}ã‚³ãƒ¼ãƒŠãƒ¼'
                if corner_key in corner_data:
                    corner_text = corner_data[corner_key]
                    # é¦¬ç•ªå·å½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹
                    corner_text_clean = corner_text.replace('(', '').replace(')', '').replace('-', ',')
                    horses = [h.strip() for h in corner_text_clean.split(',') if h.strip()]
                    for j, horse_num in enumerate(horses, 1):
                        if horse_num == umaban:
                            horse_data[f'corner_{corner_num}'] = str(j)
                            break"""
            
            new_corner_extract = """            # ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ä½ã®æŠ½å‡º(NAR: é¦¬ç•ªå·å½¢å¼ã€åŒç€è€ƒæ…®)
            umaban = horse_data['é¦¬ç•ª']
            for corner_num in range(1, 5):
                corner_key = f'{corner_num}ã‚³ãƒ¼ãƒŠãƒ¼'
                if corner_key in corner_data:
                    corner_text = corner_data[corner_key]
                    # æ‹¬å¼§(åŒç€)ã‚’è€ƒæ…®ã—ã¦ãƒ‘ãƒ¼ã‚¹
                    corner_text = corner_text.replace('-', ',')
                    parts = []
                    current = ''
                    paren_depth = 0
                    for char in corner_text:
                        if char == '(':
                            paren_depth += 1
                            current += char
                        elif char == ')':
                            paren_depth -= 1
                            current += char
                        elif char == ',' and paren_depth == 0:
                            if current.strip():
                                parts.append(current.strip())
                            current = ''
                        else:
                            current += char
                    if current.strip():
                        parts.append(current.strip())
                    
                    # å„ãƒ‘ãƒ¼ãƒˆã‹ã‚‰é †ä½ã‚’è¨ˆç®—
                    current_position = 1
                    for part in parts:
                        if part.startswith('(') and part.endswith(')'):
                            horses_in_group = part[1:-1].split(',')
                            for horse_num in horses_in_group:
                                if horse_num.strip() == umaban:
                                    horse_data[f'corner_{corner_num}'] = str(current_position)
                                    break
                            current_position += len([h for h in horses_in_group if h.strip()])
                        else:
                            if part.strip() == umaban:
                                horse_data[f'corner_{corner_num}'] = str(current_position)
                            current_position += 1"""
            
            source = source.replace(old_corner_extract, new_corner_extract)
            
            cell['source'] = [line + '\n' for line in source.split('\n')]
            if cell['source'] and cell['source'][-1] == '\n':
                cell['source'][-1] = cell['source'][-1].rstrip('\n')
            
            print("  âœ… ã‚³ãƒ¼ãƒŠãƒ¼æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ã‚’åŒç€å¯¾å¿œç‰ˆã«æ›´æ–°")
            break

with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(nb, f, ensure_ascii=False, indent=1)

print(f"\nğŸ’¾ ä¿å­˜å®Œäº†: {notebook_path}")
print("\nğŸ“ æ›´æ–°å†…å®¹:")
print("  - æ‹¬å¼§å†…ã®åŒç€é¦¬ã‚’æ­£ã—ãå‡¦ç†")
print("  - åŒç€é¦¬ã¯åŒã˜é †ä½ã€æ¬¡ã®é¦¬ã¯æ‹¬å¼§å†…ã®é¦¬æ•°åˆ†åŠ ç®—")
