{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ‡ JRA åŸºæœ¬æƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° (Stage 1/2) v2\n",
    "\n",
    "## ğŸ“Š å–å¾—ãƒ‡ãƒ¼ã‚¿\n",
    "- **26ã‚«ãƒ©ãƒ **: æ—¥ä»˜ã€ä¼šå ´ã€ãƒ¬ãƒ¼ã‚¹ç•ªå·ã€ãƒ¬ãƒ¼ã‚¹åã€é‡è³ã€ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã€è·é›¢ã€å›ã‚Šã€å¤©å€™ã€é¦¬å ´çŠ¶æ…‹ã€ç€é †ã€æ ã€é¦¬ç•ªã€é¦¬åã€æ€§é½¢ã€æ–¤é‡ã€é¨æ‰‹ã€ã‚¿ã‚¤ãƒ ã€ç€å·®ã€äººæ°—ã€å˜å‹ã‚ªãƒƒã‚ºã€å¾Œ3Fã€å©èˆã€é¦¬ä½“é‡(å¢—æ¸›)ã€race_idã€horse_id\n",
    "\n",
    "## âœ… æ”¹å–„ç‚¹\n",
    "- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚’ç¢ºå®Ÿã«\n",
    "- ã‚«ãƒ©ãƒ ã‚ºãƒ¬ã‚’å®Œå…¨ã«é˜²æ­¢\n",
    "- ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚’å®Ÿè£…\n",
    "\n",
    "## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "Stage 2ã§é¦¬å±¥æ­´ãƒ»è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive Mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ›¡ï¸ Keep-Alive (ã‚¢ã‚¤ãƒ‰ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿)\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "display(Javascript('''\n",
    "function ClickConnect(){\n",
    "    console.log(\"Keep-alive: Working\");\n",
    "    var buttons = document.querySelectorAll(\"colab-connect-button\");\n",
    "    buttons.forEach(function(btn){\n",
    "        btn.click();\n",
    "    });\n",
    "}\n",
    "setInterval(ClickConnect, 60000);\n",
    "console.log(\"Keep-alive script started - clicks every 60 seconds\");\n",
    "'''))\n",
    "\n",
    "print(\"âœ… Keep-alive activated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "SAVE_DIR = '/content/drive/MyDrive/dai-keiba/data/raw'\n",
    "TARGET_CSV = 'database_basic.csv'\n",
    "MASTER_ID_CSV = 'race_ids.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"âœ… Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–¢æ•°(å®Œå…¨ç‰ˆ - 11/11ã‚«ãƒ©ãƒ å–å¾—)\n",
    "\n",
    "def extract_metadata(soup, url):\n",
    "    \"\"\"å…¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºå®Ÿã«æŠ½å‡º\"\"\"\n",
    "    metadata = {\n",
    "        'æ—¥ä»˜': '', 'ä¼šå ´': '', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·': '', 'ãƒ¬ãƒ¼ã‚¹å': '', 'é‡è³': '',\n",
    "        'ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—': '', 'è·é›¢': '', 'å›ã‚Š': '', 'å¤©å€™': '', 'é¦¬å ´çŠ¶æ…‹': '', 'race_id': ''\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        title = soup.title.text if soup.title else \"\"\n",
    "        full_text = soup.text\n",
    "        \n",
    "        if title:\n",
    "            # ãƒ¬ãƒ¼ã‚¹å\n",
    "            race_name_match = re.search(r'^([^|]+)', title)\n",
    "            if race_name_match:\n",
    "                race_name_full = race_name_match.group(1).strip()\n",
    "                race_name = re.sub(r'\\s*(çµæœ|æ‰•æˆ»|æ‰•ã„æˆ»ã—).*$', '', race_name_full).strip()\n",
    "                metadata['ãƒ¬ãƒ¼ã‚¹å'] = race_name\n",
    "                \n",
    "                # é‡è³åˆ¤å®š\n",
    "                if 'G1' in race_name or 'Gâ… ' in race_name or 'GI' in race_name:\n",
    "                    metadata['é‡è³'] = 'G1'\n",
    "                elif 'G2' in race_name or 'Gâ…¡' in race_name or 'GII' in race_name:\n",
    "                    metadata['é‡è³'] = 'G2'\n",
    "                elif 'G3' in race_name or 'Gâ…¢' in race_name or 'GIII' in race_name:\n",
    "                    metadata['é‡è³'] = 'G3'\n",
    "            \n",
    "            # æ—¥ä»˜\n",
    "            date_match = re.search(r'(\\d{4})å¹´(\\d{1,2})æœˆ(\\d{1,2})æ—¥', title)\n",
    "            if date_match:\n",
    "                year = date_match.group(1)\n",
    "                month = f\"{int(date_match.group(2)):02d}\"\n",
    "                day = f\"{int(date_match.group(3)):02d}\"\n",
    "                metadata['æ—¥ä»˜'] = f\"{year}/{month}/{day}\"\n",
    "            \n",
    "            # ä¼šå ´\n",
    "            venues = ['æœ­å¹Œ', 'å‡½é¤¨', 'ç¦å³¶', 'æ–°æ½Ÿ', 'æ±äº¬', 'ä¸­å±±', 'ä¸­äº¬', 'äº¬éƒ½', 'é˜ªç¥', 'å°å€‰']\n",
    "            for venue in venues:\n",
    "                if venue in title:\n",
    "                    metadata['ä¼šå ´'] = venue\n",
    "                    break\n",
    "            \n",
    "            # ãƒ¬ãƒ¼ã‚¹ç•ªå·\n",
    "            race_num_match = re.search(r'(\\d+)R', title)\n",
    "            if race_num_match:\n",
    "                metadata['ãƒ¬ãƒ¼ã‚¹ç•ªå·'] = race_num_match.group(0)\n",
    "        \n",
    "        # ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ãƒ»è·é›¢\n",
    "        course_match = re.search(r'(èŠ|ãƒ€ãƒ¼ãƒˆ|ãƒ€|éšœå®³)\\s*(\\d+)m', full_text)\n",
    "        if course_match:\n",
    "            course_type = course_match.group(1)\n",
    "            if 'èŠ' in course_type:\n",
    "                metadata['ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—'] = 'èŠ'\n",
    "            elif 'ãƒ€' in course_type:\n",
    "                metadata['ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—'] = 'ãƒ€ãƒ¼ãƒˆ'\n",
    "            elif 'éšœ' in course_type:\n",
    "                metadata['ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—'] = 'éšœå®³'\n",
    "            metadata['è·é›¢'] = course_match.group(2)\n",
    "        \n",
    "        # å›ã‚Š\n",
    "        if 'å³' in full_text:\n",
    "            metadata['å›ã‚Š'] = 'å³'\n",
    "        elif 'å·¦' in full_text:\n",
    "            metadata['å›ã‚Š'] = 'å·¦'\n",
    "        elif 'ç›´ç·š' in full_text or 'ç›´' in full_text:\n",
    "            metadata['å›ã‚Š'] = 'ç›´ç·š'\n",
    "        \n",
    "        # å¤©å€™\n",
    "        weather_match = re.search(r'å¤©å€™\\s*[:ï¼š]\\s*(\\S+)', full_text)\n",
    "        if weather_match:\n",
    "            metadata['å¤©å€™'] = weather_match.group(1)\n",
    "        \n",
    "        # é¦¬å ´çŠ¶æ…‹(ç¢ºå®Ÿã«å–å¾—)\n",
    "        baba_match = re.search(r'é¦¬å ´\\s*[:ï¼š]\\s*(\\S+)', full_text)\n",
    "        if baba_match:\n",
    "            metadata['é¦¬å ´çŠ¶æ…‹'] = baba_match.group(1)\n",
    "        else:\n",
    "            if metadata['ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—'] == 'èŠ':\n",
    "                condition_match = re.search(r'èŠ\\s*[:ï¼š]\\s*(\\S+)', full_text)\n",
    "                if condition_match:\n",
    "                    metadata['é¦¬å ´çŠ¶æ…‹'] = condition_match.group(1)\n",
    "            elif metadata['ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—'] == 'ãƒ€ãƒ¼ãƒˆ':\n",
    "                condition_match = re.search(r'ãƒ€ãƒ¼ãƒˆ\\s*[:ï¼š]\\s*(\\S+)', full_text)\n",
    "                if condition_match:\n",
    "                    metadata['é¦¬å ´çŠ¶æ…‹'] = condition_match.group(1)\n",
    "        \n",
    "        # race_idç”Ÿæˆ\n",
    "        if metadata['æ—¥ä»˜'] and metadata['ä¼šå ´'] and metadata['ãƒ¬ãƒ¼ã‚¹ç•ªå·']:\n",
    "            year = metadata['æ—¥ä»˜'][:4]\n",
    "            place_map = {\n",
    "                'æœ­å¹Œ': '01', 'å‡½é¤¨': '02', 'ç¦å³¶': '03', 'æ–°æ½Ÿ': '04', 'æ±äº¬': '05',\n",
    "                'ä¸­å±±': '06', 'ä¸­äº¬': '07', 'äº¬éƒ½': '08', 'é˜ªç¥': '09', 'å°å€‰': '10'\n",
    "            }\n",
    "            place_code = place_map.get(metadata['ä¼šå ´'], '00')\n",
    "            race_num = metadata['ãƒ¬ãƒ¼ã‚¹ç•ªå·'].replace('R', '')\n",
    "            race_num_padded = f\"{int(race_num):02d}\"\n",
    "            \n",
    "            kai = '01'\n",
    "            nichi = '01'\n",
    "            kai_match = re.search(rf'(\\d+)å›{metadata[\"ä¼šå ´\"]}(\\d+)æ—¥', title + full_text)\n",
    "            if kai_match:\n",
    "                kai = f\"{int(kai_match.group(1)):02d}\"\n",
    "                nichi = f\"{int(kai_match.group(2)):02d}\"\n",
    "            \n",
    "            metadata['race_id'] = f\"{year}{place_code}{kai}{nichi}{race_num_padded}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "print(\"âœ… Metadata extraction function loaded (Complete - 11/11 columns)\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¬ãƒ¼ã‚¹çµæœã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°\n",
    "\n",
    "def scrape_race_basic(race_id):\n",
    "    \"\"\"\n",
    "    1ãƒ¬ãƒ¼ã‚¹ã®åŸºæœ¬æƒ…å ±(26ã‚«ãƒ©ãƒ )ã‚’å–å¾—\n",
    "    \"\"\"\n",
    "    url = f\"https://race.netkeiba.com/race/result.html?race_id={race_id}\"\n",
    "    \n",
    "    try:\n",
    "        # ãƒšãƒ¼ã‚¸å–å¾—\n",
    "        time.sleep(0.5)\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        resp = requests.get(url, headers=headers, timeout=15)\n",
    "        \n",
    "        # HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒã‚§ãƒƒã‚¯\n",
    "        if resp.status_code != 200:\n",
    "            print(f\"  âš ï¸ HTTP {resp.status_code}: {race_id}\")\n",
    "            return None\n",
    "        \n",
    "        resp.encoding = 'EUC-JP'\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        \n",
    "        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º\n",
    "        metadata = extract_metadata(soup, url)\n",
    "        \n",
    "        # ãƒ¬ãƒ¼ã‚¹çµæœãƒ†ãƒ¼ãƒ–ãƒ«å–å¾—\n",
    "        tables = soup.find_all('table')\n",
    "        result_table = None\n",
    "        \n",
    "        for table in tables:\n",
    "            if 'ç€é †' in table.text and 'é¦¬å' in table.text:\n",
    "                result_table = table\n",
    "                break\n",
    "        \n",
    "        if not result_table:\n",
    "            print(f\"  âš ï¸ ãƒ¬ãƒ¼ã‚¹çµæœãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {race_id}\")\n",
    "            print(f\"     URL: {url}\")\n",
    "            print(f\"     ç·ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables)}\")\n",
    "            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ä¸€éƒ¨ã‚’è¡¨ç¤º(ãƒ‡ãƒãƒƒã‚°ç”¨)\n",
    "            if len(soup.text) < 100:\n",
    "                print(f\"     ãƒšãƒ¼ã‚¸å†…å®¹ãŒå°‘ãªã™ãã¾ã™(ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å¯èƒ½æ€§)\")\n",
    "            return None\n",
    "        \n",
    "        # å„é¦¬ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "        rows = result_table.find_all('tr')\n",
    "        race_data = []\n",
    "        \n",
    "        for row in rows:\n",
    "            if row.find('th'):\n",
    "                continue  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚¹ã‚­ãƒƒãƒ—\n",
    "            \n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) < 10:\n",
    "                continue\n",
    "            \n",
    "            # åŸºæœ¬æƒ…å ±ã‚’è¾æ›¸ã§æ§‹ç¯‰(ã‚«ãƒ©ãƒ ã‚ºãƒ¬é˜²æ­¢)\n",
    "            horse_data = {\n",
    "                'æ—¥ä»˜': metadata['æ—¥ä»˜'],\n",
    "                'ä¼šå ´': metadata['ä¼šå ´'],\n",
    "                'ãƒ¬ãƒ¼ã‚¹ç•ªå·': metadata['ãƒ¬ãƒ¼ã‚¹ç•ªå·'],\n",
    "                'ãƒ¬ãƒ¼ã‚¹å': metadata['ãƒ¬ãƒ¼ã‚¹å'],\n",
    "                'é‡è³': metadata['é‡è³'],\n",
    "                'ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—': metadata['ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—'],\n",
    "                'è·é›¢': metadata['è·é›¢'],\n",
    "                'å›ã‚Š': metadata['å›ã‚Š'],\n",
    "                'å¤©å€™': metadata['å¤©å€™'],\n",
    "                'é¦¬å ´çŠ¶æ…‹': metadata['é¦¬å ´çŠ¶æ…‹'],\n",
    "                'ç€é †': cells[0].text.strip(),\n",
    "                'æ ': '',\n",
    "                'é¦¬ç•ª': cells[2].text.strip() if len(cells) > 2 else '',\n",
    "                'é¦¬å': cells[3].text.strip() if len(cells) > 3 else '',\n",
    "                'æ€§é½¢': cells[4].text.strip() if len(cells) > 4 else '',\n",
    "                'æ–¤é‡': cells[5].text.strip() if len(cells) > 5 else '',\n",
    "                'é¨æ‰‹': cells[6].text.strip() if len(cells) > 6 else '',\n",
    "                'ã‚¿ã‚¤ãƒ ': cells[7].text.strip() if len(cells) > 7 else '',\n",
    "                'ç€å·®': cells[8].text.strip() if len(cells) > 8 else '',\n",
    "                'äººæ°—': cells[9].text.strip() if len(cells) > 9 else '',\n",
    "                'å˜å‹ã‚ªãƒƒã‚º': cells[10].text.strip() if len(cells) > 10 else '',\n",
    "                'å¾Œ3F': cells[11].text.strip() if len(cells) > 11 else '',\n",
    "                'ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †': cells[12].text.strip() if len(cells) > 12 else '',\n",
    "                'corner_1': '',\n",
    "                'corner_2': '',\n",
    "                'corner_3': '',\n",
    "                'corner_4': '',\n",
    "                'å©èˆ': cells[13].text.strip() if len(cells) > 13 else '',\n",
    "                'é¦¬ä½“é‡(å¢—æ¸›)': cells[14].text.strip() if len(cells) > 14 else '',\n",
    "                'race_id': metadata['race_id'],\n",
    "                'horse_id': ''\n",
    "            }\n",
    "            \n",
    "            # æ ç•ª(ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥å–å¾—)\n",
    "            if len(cells) > 1:\n",
    "                horse_data['æ '] = cells[1].text.strip()\n",
    "            \n",
    "            # horse_id(ãƒªãƒ³ã‚¯ã‹ã‚‰æŠ½å‡º)\n",
    "            horse_link = cells[3].find('a') if len(cells) > 3 else None\n",
    "            if horse_link and 'href' in horse_link.attrs:\n",
    "                horse_id_match = re.search(r'/horse/(\\d+)', horse_link['href'])\n",
    "                if horse_id_match:\n",
    "                    horse_data['horse_id'] = horse_id_match.group(1)\n",
    "            \n",
    "            # ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ã®å€‹åˆ¥ã‚«ãƒ©ãƒ æŠ½å‡º(LightGBMç”¨)\n",
    "            corner_text = cells[12].text.strip() if len(cells) > 12 else ''\n",
    "            if corner_text and '-' in corner_text:\n",
    "                positions = corner_text.split('-')\n",
    "                for j, pos in enumerate(positions[:4], 1):\n",
    "                    horse_data[f'corner_{j}'] = pos.strip()\n",
    "            \n",
    "            # ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ã®å€‹åˆ¥ã‚«ãƒ©ãƒ æŠ½å‡º(æœ€çµ‚ã‹ã‚‰é€†é †)\n",
    "\n",
    "            \n",
    "            corner_text = cells[12].text.strip() if len(cells) > 12 else ''\n",
    "\n",
    "            \n",
    "            if corner_text and '-' in corner_text:\n",
    "\n",
    "            \n",
    "                positions = corner_text.split('-')\n",
    "\n",
    "            \n",
    "                for j, pos in enumerate(reversed(positions)):\n",
    "\n",
    "            \n",
    "                    if j < 4:\n",
    "\n",
    "            \n",
    "                        horse_data[f'corner_{j+1}'] = pos.strip()\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            race_data.append(horse_data)\n",
    "        \n",
    "        if not race_data:\n",
    "            return None\n",
    "        \n",
    "        # DataFrameã«å¤‰æ›(ã‚«ãƒ©ãƒ é †åºã‚’æ˜ç¤ºçš„ã«æŒ‡å®š)\n",
    "        ordered_columns = [\n",
    "            'æ—¥ä»˜', 'ä¼šå ´', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·', 'ãƒ¬ãƒ¼ã‚¹å', 'é‡è³', 'ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—', 'è·é›¢', 'å›ã‚Š',\n",
    "            'å¤©å€™', 'é¦¬å ´çŠ¶æ…‹', 'ç€é †', 'æ ', 'é¦¬ç•ª', 'é¦¬å', 'æ€§é½¢', 'æ–¤é‡', 'é¨æ‰‹', 'ã‚¿ã‚¤ãƒ ',\n",
    "            'ç€å·®', 'äººæ°—', 'å˜å‹ã‚ªãƒƒã‚º', 'å¾Œ3F', 'ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †', 'corner_1', 'corner_2', 'corner_3', 'corner_4',\n",
    "            'å©èˆ', 'é¦¬ä½“é‡(å¢—æ¸›)', 'race_id', 'horse_id'\n",
    "        ]\n",
    "        \n",
    "        df = pd.DataFrame(race_data)[ordered_columns]\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ ã‚¨ãƒ©ãƒ¼: {race_id} - {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"âœ… Scraping function loaded\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ\n",
    "\n",
    "def run_basic_scraping():\n",
    "    \"\"\"\n",
    "    å·®åˆ†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° + ãƒãƒ£ãƒ³ã‚¯ä¿å­˜\n",
    "    \"\"\"\n",
    "    csv_path = os.path.join(SAVE_DIR, TARGET_CSV)\n",
    "    master_path = os.path.join(SAVE_DIR, MASTER_ID_CSV)\n",
    "    \n",
    "    # ãƒã‚¹ã‚¿ãƒ¼IDèª­ã¿è¾¼ã¿\n",
    "    if not os.path.exists(master_path):\n",
    "        print(f\"âŒ ãƒã‚¹ã‚¿ãƒ¼IDãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {master_path}\")\n",
    "        return\n",
    "    \n",
    "    master_df = pd.read_csv(master_path, dtype=str)\n",
    "    master_ids = set(master_df['race_id'].dropna().unique())\n",
    "    print(f\"ğŸ“‹ ãƒã‚¹ã‚¿ãƒ¼IDæ•°: {len(master_ids)}\")\n",
    "    \n",
    "    # æ—¢å­˜IDèª­ã¿è¾¼ã¿\n",
    "    existing_ids = set()\n",
    "    if os.path.exists(csv_path):\n",
    "        existing_df = pd.read_csv(csv_path, dtype=str, usecols=['race_id'])\n",
    "        existing_ids = set(existing_df['race_id'].dropna().unique())\n",
    "        print(f\"ğŸ’¾ æ—¢å­˜IDæ•°: {len(existing_ids)}\")\n",
    "    \n",
    "    # å·®åˆ†è¨ˆç®—\n",
    "    target_ids = sorted(list(master_ids - existing_ids))\n",
    "    print(f\"ğŸš€ ä»Šå›å–å¾—: {len(target_ids)} ãƒ¬ãƒ¼ã‚¹\\n\")\n",
    "    \n",
    "    if not target_ids:\n",
    "        print(\"âœ… å…¨ã¦å–å¾—æ¸ˆã¿ã§ã™\")\n",
    "        return\n",
    "    \n",
    "    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°\n",
    "    buffer = []\n",
    "    chunk_size = 50\n",
    "    \n",
    "    for i, race_id in enumerate(tqdm(target_ids)):\n",
    "        df = scrape_race_basic(race_id)\n",
    "        \n",
    "        if df is not None and not df.empty:\n",
    "            buffer.append(df)\n",
    "        \n",
    "        # ãƒãƒ£ãƒ³ã‚¯ä¿å­˜\n",
    "        if len(buffer) >= chunk_size or (i == len(target_ids) - 1 and buffer):\n",
    "            df_chunk = pd.concat(buffer, ignore_index=True)\n",
    "            \n",
    "            if not os.path.exists(csv_path):\n",
    "                df_chunk.to_csv(csv_path, index=False)\n",
    "            else:\n",
    "                df_chunk.to_csv(csv_path, mode='a', header=False, index=False)\n",
    "            \n",
    "            print(f\"  ğŸ’¾ Saved {len(buffer)} races\")\n",
    "            buffer = []\n",
    "            gc.collect()\n",
    "    \n",
    "    print(\"\\nâœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†\")\n",
    "\n",
    "print(\"âœ… Main function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Ÿè¡Œ\n",
    "run_basic_scraping()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}