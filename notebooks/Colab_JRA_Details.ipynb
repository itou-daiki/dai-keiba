{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ‡ JRA è©³ç´°æƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° (Stage 2/2)\n",
                "\n",
                "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯**é¦¬å±¥æ­´+è¡€çµ±æƒ…å ±**ã‚’å–å¾—ã—ã¾ã™ã€‚\n",
                "\n",
                "## ğŸ“Š å‰ææ¡ä»¶\n",
                "- `Colab_JRA_Basic.ipynb` ã‚’å®Ÿè¡Œæ¸ˆã¿\n",
                "- `database_basic.csv` ãŒå­˜åœ¨ã™ã‚‹\n",
                "\n",
                "## ğŸ“Š å–å¾—ãƒ‡ãƒ¼ã‚¿\n",
                "- é¦¬å±¥æ­´ãƒšãƒ¼ã‚¸ã‹ã‚‰65ã‚«ãƒ©ãƒ (past_1_* ~ past_5_*)\n",
                "- è¡€çµ±ãƒšãƒ¼ã‚¸ã‹ã‚‰3ã‚«ãƒ©ãƒ (father, mother, bms)\n",
                "\n",
                "## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
                "`merge_jra_data.py` ã§ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ â†’ `database.csv`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Google Drive Mount\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ğŸ›¡ï¸ Keep-Alive (ã‚¢ã‚¤ãƒ‰ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿)\n",
                "from IPython.display import display, Javascript\n",
                "\n",
                "display(Javascript('''\n",
                "function ClickConnect(){\n",
                "    console.log(\"Keep-alive: Working\");\n",
                "    var buttons = document.querySelectorAll(\"colab-connect-button\");\n",
                "    buttons.forEach(function(btn){\n",
                "        btn.click();\n",
                "    });\n",
                "}\n",
                "setInterval(ClickConnect, 60000);\n",
                "console.log(\"Keep-alive script started - clicks every 60 seconds\");\n",
                "'''))\n",
                "\n",
                "print(\"âœ… Keep-alive activated (auto-clicks every 60 seconds)\")\n",
                "print(\"ğŸ’¡ This prevents idle timeout during long scraping sessions\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Settings\n",
                "SAVE_DIR = '/content/drive/MyDrive/dai-keiba/data/raw'\n",
                "BASIC_CSV = 'database_basic.csv'      # Input: Basic data\n",
                "DETAILS_CSV = 'database_details.csv'  # Output: Details data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# RaceScraper Class (Horse History + Pedigree)\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import pandas as pd\n",
                "import io\n",
                "import re\n",
                "from datetime import datetime\n",
                "import time\n",
                "import os\n",
                "import gc\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "class RaceScraper:\n",
                "    def __init__(self):\n",
                "        self.headers = {\n",
                "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
                "        }\n",
                "    \n",
                "    def _get_soup(self, url):\n",
                "        try:\n",
                "            time.sleep(1)\n",
                "            response = requests.get(url, headers=self.headers, timeout=10)\n",
                "            response.encoding = response.apparent_encoding\n",
                "            if response.status_code == 200:\n",
                "                return BeautifulSoup(response.text, 'html.parser')\n",
                "        except Exception as e:\n",
                "            print(f\"Error fetching {url}: {e}\")\n",
                "        return None\n",
                "    \n",
                "    def get_horse_profile(self, horse_id):\n",
                "        \"\"\"è¡€çµ±æƒ…å ±ã‚’å–å¾—\"\"\"\n",
                "        url = f\"https://db.netkeiba.com/horse/ped/{horse_id}/\"\n",
                "        soup = self._get_soup(url)\n",
                "        data = {\"father\": \"\", \"mother\": \"\", \"bms\": \"\"}\n",
                "        if not soup: return data\n",
                "        \n",
                "        try:\n",
                "            table = soup.select_one(\"table.blood_table\")\n",
                "            if table:\n",
                "                rows = table.find_all(\"tr\")\n",
                "                if len(rows) >= 17:\n",
                "                    r0 = rows[0].find_all(\"td\")\n",
                "                    if r0:\n",
                "                        data[\"father\"] = r0[0].text.strip().split('\\n')[0].strip()\n",
                "                    r16 = rows[16].find_all(\"td\")\n",
                "                    if len(r16) >= 2:\n",
                "                        data[\"mother\"] = r16[0].text.strip().split('\\n')[0].strip()\n",
                "                        data[\"bms\"] = r16[1].text.strip().split('\\n')[0].strip()\n",
                "        except Exception as e:\n",
                "            pass\n",
                "        return data\n",
                "    \n",
                "    def extract_run_style(self, passing_str):\n",
                "        if not isinstance(passing_str, str): return 3\n",
                "        try:\n",
                "            cleaned = re.sub(r'[^0-9-]', '', passing_str)\n",
                "            parts = [int(p) for p in cleaned.split('-') if p]\n",
                "            if not parts: return 3\n",
                "            first_corner = parts[0]\n",
                "            if first_corner == 1: return 1\n",
                "            elif first_corner <= 4: return 2\n",
                "            elif first_corner <= 9: return 3\n",
                "            else: return 4\n",
                "        except: return 3\n",
                "    \n",
                "    def get_past_races(self, horse_id, current_race_date, n_samples=5):\n",
                "        \"\"\"éå»ãƒ¬ãƒ¼ã‚¹å±¥æ­´ã‚’å–å¾—\"\"\"\n",
                "        url = f\"https://db.netkeiba.com/horse/result/{horse_id}/\"\n",
                "        soup = self._get_soup(url)\n",
                "        if not soup: return pd.DataFrame()\n",
                "        \n",
                "        table = soup.select_one(\"table.db_h_race_results\")\n",
                "        if not table:\n",
                "            tables = soup.find_all(\"table\")\n",
                "            for t in tables:\n",
                "                if \"ç€é †\" in t.text:\n",
                "                    table = t\n",
                "                    break\n",
                "        if not table: return pd.DataFrame()\n",
                "        \n",
                "        try:\n",
                "            df = pd.read_html(io.StringIO(str(table)))[0]\n",
                "            df = df.dropna(how='all')\n",
                "            df.columns = df.columns.astype(str).str.replace(r'\\s+', '', regex=True)\n",
                "            \n",
                "            if 'æ—¥ä»˜' in df.columns:\n",
                "                df['date_obj'] = pd.to_datetime(df['æ—¥ä»˜'], format='%Y/%m/%d', errors='coerce')\n",
                "                df = df.dropna(subset=['date_obj'])\n",
                "                current_date = pd.to_datetime(current_race_date)\n",
                "                df = df[df['date_obj'] < current_date]\n",
                "                df = df.sort_values('date_obj', ascending=False)\n",
                "            \n",
                "            if df.empty: return df\n",
                "            df = df.head(n_samples)\n",
                "            \n",
                "            if 'é€šé' in df.columns:\n",
                "                df['run_style_val'] = df['é€šé'].apply(self.extract_run_style)\n",
                "            else:\n",
                "                df['run_style_val'] = 3\n",
                "            \n",
                "            column_map = {\n",
                "                'æ—¥ä»˜': 'date', 'é–‹å‚¬': 'venue', 'å¤©æ°—': 'weather', 'ãƒ¬ãƒ¼ã‚¹å': 'race_name',\n",
                "                'ç€é †': 'rank', 'æ ç•ª': 'waku', 'é¦¬ç•ª': 'umaban', 'é¨æ‰‹': 'jockey',\n",
                "                'æ–¤é‡': 'weight_carried', 'é¦¬å ´': 'condition', 'ã‚¿ã‚¤ãƒ ': 'time',\n",
                "                'ç€å·®': 'margin', 'ä¸Šã‚Š': 'last_3f', 'é€šé': 'passing', 'é¦¬ä½“é‡': 'horse_weight',\n",
                "                'run_style_val': 'run_style', 'å˜å‹': 'odds', 'ã‚ªãƒƒã‚º': 'odds', 'è·é›¢': 'raw_distance'\n",
                "            }\n",
                "            df.rename(columns=column_map, inplace=True)\n",
                "            \n",
                "            if 'raw_distance' in df.columns:\n",
                "                def parse_dist(x):\n",
                "                    if not isinstance(x, str): return None, None\n",
                "                    surf = None; dist = None\n",
                "                    if 'èŠ' in x: surf = 'èŠ'\n",
                "                    elif 'ãƒ€' in x: surf = 'ãƒ€'\n",
                "                    elif 'éšœ' in x: surf = 'éšœ'\n",
                "                    match = re.search(r'(\\d+)', x)\n",
                "                    if match: dist = int(match.group(1))\n",
                "                    return surf, dist\n",
                "                \n",
                "                parsed = df['raw_distance'].apply(parse_dist)\n",
                "                df['course_type'] = parsed.apply(lambda x: x[0])\n",
                "                df['distance'] = parsed.apply(lambda x: x[1])\n",
                "            else:\n",
                "                df['course_type'] = None\n",
                "                df['distance'] = None\n",
                "            \n",
                "            return df\n",
                "        except Exception as e:\n",
                "            return pd.DataFrame()\n",
                "\n",
                "print(\"âœ… RaceScraper loaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Main Execution: Fetch Details for each horse\n",
                "def run_details_scraping():\n",
                "    basic_path = os.path.join(SAVE_DIR, BASIC_CSV)\n",
                "    details_path = os.path.join(SAVE_DIR, DETAILS_CSV)\n",
                "    \n",
                "    # Load basic data\n",
                "    if not os.path.exists(basic_path):\n",
                "        print(f\"âŒ Basic CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {basic_path}\")\n",
                "        print(\"   å…ˆã«Colab_JRA_Basic.ipynbã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
                "        return\n",
                "    \n",
                "    print(\"ğŸ“– Basic CSVã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
                "    basic_df = pd.read_csv(basic_path, dtype=str)\n",
                "    print(f\"   {len(basic_df)} è¡Œèª­ã¿è¾¼ã¿\")\n",
                "    \n",
                "    # Extract unique horses\n",
                "    horse_race_pairs = basic_df[['race_id', 'horse_id', 'æ—¥ä»˜']].drop_duplicates()\n",
                "    horse_race_pairs = horse_race_pairs[horse_race_pairs['horse_id'].notna()]\n",
                "    horse_race_pairs = horse_race_pairs[horse_race_pairs['horse_id'] != '']\n",
                "    \n",
                "    print(f\"ğŸ“‹ ãƒ¦ãƒ‹ãƒ¼ã‚¯é¦¬: {len(horse_race_pairs)} ä»¶\")\n",
                "    \n",
                "    # Check existing\n",
                "    existing_pairs = set()\n",
                "    if os.path.exists(details_path):\n",
                "        existing_df = pd.read_csv(details_path, dtype=str, usecols=['race_id', 'horse_id'])\n",
                "        existing_pairs = set(zip(existing_df['race_id'], existing_df['horse_id']))\n",
                "        print(f\"ğŸ’¾ æ—¢å­˜å–å¾—æ¸ˆã¿: {len(existing_pairs)} ä»¶\")\n",
                "    \n",
                "    # Calculate diff\n",
                "    target_pairs = []\n",
                "    for _, row in horse_race_pairs.iterrows():\n",
                "        pair = (row['race_id'], row['horse_id'])\n",
                "        if pair not in existing_pairs:\n",
                "            target_pairs.append(row)\n",
                "    \n",
                "    print(f\"ğŸš€ ä»Šå›å–å¾—å¯¾è±¡: {len(target_pairs)} ä»¶\")\n",
                "    \n",
                "    if not target_pairs:\n",
                "        print(\"âœ… å…¨ã¦å–å¾—æ¸ˆã¿ã§ã™ã€‚\")\n",
                "        return\n",
                "    \n",
                "    # Scrape\n",
                "    scraper = RaceScraper()\n",
                "    buffer = []\n",
                "    chunk_size = 50\n",
                "    \n",
                "    for i, row in enumerate(tqdm(target_pairs)):\n",
                "        race_id = row['race_id']\n",
                "        horse_id = row['horse_id']\n",
                "        race_date = row['æ—¥ä»˜']\n",
                "        \n",
                "        try:\n",
                "            # Pedigree\n",
                "            pedigree = scraper.get_horse_profile(horse_id)\n",
                "            \n",
                "            # History\n",
                "            history_df = scraper.get_past_races(horse_id, race_date, n_samples=5)\n",
                "            \n",
                "            # Build row\n",
                "            row_data = {\n",
                "                'race_id': race_id,\n",
                "                'horse_id': horse_id,\n",
                "                'father': pedigree['father'],\n",
                "                'mother': pedigree['mother'],\n",
                "                'bms': pedigree['bms']\n",
                "            }\n",
                "            \n",
                "            # Add past races\n",
                "            for j in range(5):\n",
                "                prefix = f\"past_{j+1}\"\n",
                "                if j < len(history_df):\n",
                "                    r = history_df.iloc[j]\n",
                "                    row_data[f\"{prefix}_date\"] = r.get('date', '')\n",
                "                    row_data[f\"{prefix}_rank\"] = r.get('rank', '')\n",
                "                    row_data[f\"{prefix}_time\"] = r.get('time', '')\n",
                "                    row_data[f\"{prefix}_run_style\"] = r.get('run_style', '')\n",
                "                    row_data[f\"{prefix}_race_name\"] = r.get('race_name', '')\n",
                "                    row_data[f\"{prefix}_last_3f\"] = r.get('last_3f', '')\n",
                "                    row_data[f\"{prefix}_horse_weight\"] = r.get('horse_weight', '')\n",
                "                    row_data[f\"{prefix}_jockey\"] = r.get('jockey', '')\n",
                "                    row_data[f\"{prefix}_condition\"] = r.get('condition', '')\n",
                "                    row_data[f\"{prefix}_odds\"] = r.get('odds', '')\n",
                "                    row_data[f\"{prefix}_weather\"] = r.get('weather', '')\n",
                "                    row_data[f\"{prefix}_distance\"] = r.get('distance', '')\n",
                "                    row_data[f\"{prefix}_course_type\"] = r.get('course_type', '')\n",
                "                else:\n",
                "                    for col in ['date','rank','time','run_style','race_name','last_3f',\n",
                "                               'horse_weight','jockey','condition','odds','weather','distance','course_type']:\n",
                "                        row_data[f\"{prefix}_{col}\"] = ''\n",
                "            \n",
                "            buffer.append(row_data)\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"  Error: {e}\")\n",
                "        \n",
                "        # Save chunk\n",
                "        if len(buffer) >= chunk_size or (i == len(target_pairs) - 1 and buffer):\n",
                "            df_chunk = pd.DataFrame(buffer)\n",
                "            \n",
                "            if not os.path.exists(details_path):\n",
                "                df_chunk.to_csv(details_path, index=False)\n",
                "            else:\n",
                "                df_chunk.to_csv(details_path, mode='a', header=False, index=False)\n",
                "            \n",
                "            print(f\"  Saved {len(buffer)} horses\")\n",
                "            buffer = []\n",
                "            gc.collect()\n",
                "    \n",
                "    print(\"âœ… è©³ç´°æƒ…å ±å–å¾—å®Œäº†\")\n",
                "    print(f\"\\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: merge_jra_data.py ã‚’å®Ÿè¡Œã—ã¦database.csvã‚’ä½œæˆ\")\n",
                "\n",
                "print(\"âœ… Main function ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run\n",
                "run_details_scraping()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}