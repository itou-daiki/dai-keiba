{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# JRA Scraping Notebook\n",
                "\n",
                "‰∏≠Â§ÆÁ´∂È¶¨ÔºàJRAÔºâ„ÅÆ„Éá„Éº„Çø„Çí„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„Åó„ÄÅGoogle Drive‰∏ä„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´ËøΩÂä†„Åó„Åæ„Åô„ÄÇ"
            ],
            "metadata": {
                "id": "intro_md"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# 1. Google Drive„ÅÆ„Éû„Ç¶„É≥„Éà\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# ‚òÖ‚òÖ‚òÖ Ë®≠ÂÆöÈ†ÖÁõÆ ‚òÖ‚òÖ‚òÖ\n",
                "# scraper„Éï„Ç©„É´„ÉÄ„ÅåÂ≠òÂú®„Åô„Çã„Éë„Çπ (Google Drive‰∏ä„ÅÆ„Éë„Çπ)\n",
                "# ‰æã: '/content/drive/MyDrive/dai-keiba'\n",
                "PROJECT_PATH = '/content/drive/MyDrive/dai-keiba'\n",
                "\n",
                "if not os.path.exists(PROJECT_PATH):\n",
                "    print(f\"Error: Path {PROJECT_PATH} does not exist. Please check your Drive structure.\")\n",
                "else:\n",
                "    print(f\"Project path found: {PROJECT_PATH}\")\n",
                "    os.chdir(PROJECT_PATH)\n",
                "    sys.path.append(PROJECT_PATH)\n"
            ],
            "metadata": {
                "id": "mount_drive"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 2. ÂøÖË¶Å„Å™„É©„Ç§„Éñ„É©„É™„ÅÆ„Ç§„É≥„Éù„Éº„Éà\n",
                "try:\n",
                "    import pandas as pd\n",
                "    import requests\n",
                "    import bs4\n",
                "except ImportError:\n",
                "    !pip install pandas requests beautifulsoup4\n",
                "    import pandas as pd\n",
                "    import requests\n",
                "    import bs4\n",
                "\n",
                "from datetime import datetime, date\n",
                "from scraper.jra_scraper import scrape_jra_year, JRA_MONTH_PARAMS\n",
                "import time\n",
                "\n",
                "# ==========================================\n",
                "# üö® 2023Âπ¥/2022Âπ¥„Éë„É©„É°„Éº„Çø‰∏çË∂≥„ÅÆ‰øÆÊ≠£„Éë„ÉÉ„ÉÅ\n",
                "# ==========================================\n",
                "# „Éë„É©„É°„Éº„Çø„Éû„ÉÉ„Éó„Çí‰∏äÊõ∏„ÅçÊõ¥Êñ∞\n",
                "try:\n",
                "    JRA_MONTH_PARAMS.update({\n",
                "        \"2023\": { \"01\": \"27\", \"02\": \"F5\", \"03\": \"C3\", \"04\": \"91\", \"05\": \"5F\", \"06\": \"2D\", \"07\": \"FB\", \"08\": \"C9\", \"09\": \"97\", \"10\": \"06\", \"11\": \"D4\", \"12\": \"A2\" },\n",
                "        \"2022\": { \"01\": \"9B\", \"02\": \"69\", \"03\": \"37\", \"04\": \"05\", \"05\": \"D3\", \"06\": \"A1\", \"07\": \"6F\", \"08\": \"3D\", \"09\": \"0B\", \"10\": \"7A\", \"11\": \"48\", \"12\": \"16\" }\n",
                "    })\n",
                "    print(\"‚úÖ JRA 2023/2022 Parameters have been patched successfully.\")\n",
                "except NameError:\n",
                "    print(\"‚ö†Ô∏è JRA_MONTH_PARAMS not found in scope. Ensure imports are correct.\")\n"
            ],
            "metadata": {
                "id": "imports"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.1 „Çπ„ÇØ„É¨„Ç§„Éë„Éº„ÅÆ‰øÆÊ≠£„Éë„ÉÉ„ÉÅ („Ç≥„Éº„ÇπË©≥Á¥∞„ÉªÂ§©ÂÄôÂèñÂæó)\n",
                "import re\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import pandas as pd\n",
                "import scraper.jra_scraper\n",
                "\n",
                "def patched_scrape_jra_race(url, existing_race_ids=None):\n",
                "    print(f'Accessing JRA URL (Patched): {url}...')\n",
                "    headers = {\n",
                "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
                "    }\n",
                "    try:\n",
                "        response = requests.get(url, headers=headers, timeout=10)\n",
                "        response.encoding = 'cp932'\n",
                "        if response.status_code != 200: return None\n",
                "        soup = BeautifulSoup(response.text, 'html.parser')\n",
                "        \n",
                "        # (Skipping full redundant parse code for brevity in this patch, assuming main logic matches jra_scraper.py)\n",
                "        # Ideally we should just RELOAD the module if we updated the file in Drive.\n",
                "        # If the user updates the file in Drive (scraper/jra_scraper.py), they just need to reload.\n",
                "        # But to be safe, we can force reload.\n",
                "        pass\n",
                "    except: pass\n",
                "    return scraper.jra_scraper.scrape_jra_race(url, existing_race_ids)\n",
                "\n",
                "import importlib\n",
                "importlib.reload(scraper.jra_scraper)\n",
                "from scraper.jra_scraper import scrape_jra_race, scrape_jra_year, JRA_MONTH_PARAMS\n",
                "print('‚úÖ Loaded updated scraper/jra_scraper.py from Drive')\n"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# 3. „Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÂÆüË°åÈñ¢Êï∞„ÅÆÂÆöÁæ©\n",
                "\n",
                "def jra_scrape_execution(year_str, start_date=None, end_date=None):\n",
                "    CSV_FILE_PATH = os.path.join(PROJECT_PATH, \"database.csv\")\n",
                "    print(f\"Using CSV Path: {CSV_FILE_PATH}\")\n",
                "\n",
                "    def save_chunk(df_chunk):\n",
                "        if os.path.exists(CSV_FILE_PATH):\n",
                "            try:\n",
                "                # Read types as string to prevent auto-float for IDs\n",
                "                existing_df = pd.read_csv(CSV_FILE_PATH, dtype={'race_id': str, 'horse_id': str})\n",
                "                combined_df = pd.concat([existing_df, df_chunk], ignore_index=True)\n",
                "            except Exception as e:\n",
                "                print(f\"Read Error: {e}, creating new.\")\n",
                "                combined_df = df_chunk\n",
                "        else:\n",
                "            combined_df = df_chunk\n",
                "\n",
                "        # Deduplicate\n",
                "        subset_cols = ['race_id', 'È¶¨Âêç']\n",
                "        subset_cols = [c for c in subset_cols if c in combined_df.columns]\n",
                "        if subset_cols:\n",
                "            combined_df.drop_duplicates(subset=subset_cols, keep='last', inplace=True)\n",
                "\n",
                "        combined_df.to_csv(CSV_FILE_PATH, index=False, encoding=\"utf-8-sig\")\n",
                "        print(f\"  [Saved] Total rows: {len(combined_df)} (+{len(df_chunk)} new)\")\n",
                "\n",
                "    print(f\"Starting Scraping for {year_str} ({start_date} ~ {end_date})\")\n",
                "\n",
                "    # Load existing IDs to skip\n",
                "    existing_ids = set()\n",
                "    if os.path.exists(CSV_FILE_PATH):\n",
                "        try:\n",
                "             df_e = pd.read_csv(CSV_FILE_PATH, usecols=['race_id'], dtype={'race_id': str})\n",
                "             existing_ids = set(df_e['race_id'].astype(str))\n",
                "             print(f\"  Loaded {len(existing_ids)} existing race IDs to skip.\")\n",
                "        except:\n",
                "             pass\n",
                "\n",
                "    scrape_jra_year(year_str, start_date=start_date, end_date=end_date, save_callback=save_chunk, existing_race_ids=existing_ids)\n"
            ],
            "metadata": {
                "id": "def_exec"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 4. ÂÆüË°å„Éë„É©„É°„Éº„Çø„ÅÆË®≠ÂÆö„Å®ÈñãÂßã\n",
                "# -----------------------------\n",
                "TARGET_YEAR = \"2024\"\n",
                "TARGET_MONTH = 1  # ‚òÖ‰ΩïÊúà„ÇíÂèñÂæó„Åô„Çã„ÅãÊåáÂÆö (None„ÅÆÂ†¥Âêà„ÅØÂÖ®ÊúüÈñì„ÄÅ1„Äú12„ÇíÊåáÂÆö)\n",
                "\n",
                "import calendar\n",
                "from datetime import date\n",
                "\n",
                "START_DATE = None\n",
                "END_DATE = None\n",
                "\n",
                "if TARGET_MONTH:\n",
                "    # ÊåáÂÆö„Åó„ÅüÊúà„ÅÆ1Êó•„ÄúÊú´Êó•„ÇíË®≠ÂÆö\n",
                "    _, last_day = calendar.monthrange(int(TARGET_YEAR), int(TARGET_MONTH))\n",
                "    START_DATE = date(int(TARGET_YEAR), int(TARGET_MONTH), 1)\n",
                "    END_DATE = date(int(TARGET_YEAR), int(TARGET_MONTH), last_day)\n",
                "    print(f\"Targeting specific month: {START_DATE} to {END_DATE}\")\n",
                "else:\n",
                "    # Ëá™ÂãïÂà§ÂÆö„É≠„Ç∏„ÉÉ„ÇØ (Êó¢Â≠ò„Éá„Éº„Çø„ÅÆÁøåÊó•„Åã„Çâ)\n",
                "    CSV_FILE_PATH = os.path.join(PROJECT_PATH, \"database.csv\")\n",
                "    if os.path.exists(CSV_FILE_PATH):\n",
                "        try:\n",
                "             df_exist = pd.read_csv(CSV_FILE_PATH)\n",
                "             if 'Êó•‰ªò' in df_exist.columns and not df_exist.empty:\n",
                "                 df_exist['date_obj'] = pd.to_datetime(df_exist['Êó•‰ªò'], format='%YÂπ¥%mÊúà%dÊó•', errors='coerce')\n",
                "                 last_date = df_exist['date_obj'].max()\n",
                "                 if pd.notna(last_date):\n",
                "                     # START_DATE = last_date.date() # Êóß: Á∂ö„Åç„Åã„Çâ\n",
                "                     # Êñ∞: Ê¨†ËêΩË£úÂÆå„ÅÆ„Åü„ÇÅ„Å´„ÄÅÂº∑Âà∂ÁöÑ„Å´„Åù„ÅÆÂπ¥„ÅÆ1Êúà1Êó•„Åã„Çâ„Çπ„Ç≠„É£„É≥„Åô„Çã (existing_ids„Åß„Çπ„Ç≠„ÉÉ„Éó„Åï„Çå„Çã)\n",
                "                     START_DATE = date(int(TARGET_YEAR), 1, 1)\n",
                "                     print(f\"Êó¢Â≠ò„Éá„Éº„Çø„ÅÆÊúÄÁµÇÊó•ÊôÇ: {last_date.date()} (Ê¨†ËêΩÁ¢∫Ë™ç„ÅÆ„Åü„ÇÅ {START_DATE} „Åã„Çâ„Çπ„Ç≠„É£„É≥„Åó„Åæ„Åô)\")\n",
                "        except Exception as e:\n",
                "            print(f\"Êó¢Â≠ò„Éá„Éº„ÇøÁ¢∫Ë™ç„Ç®„É©„Éº: {e}\")\n",
                "\n",
                "print(f\"Scraping Target: {TARGET_YEAR}, Start: {START_DATE}, End: {END_DATE}\")\n",
                "jra_scrape_execution(TARGET_YEAR, start_date=START_DATE, end_date=END_DATE)\n"
            ],
            "metadata": {
                "id": "run_cell"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.1 „Éá„Éº„Çø„Éô„Éº„Çπ„Ç´„É©„É†Âêç„ÅÆ‰øÆÊ≠£ (ÈáçË§áÊîπÂñÑ)\n",
                "import os\n",
                "import pandas as pd\n",
                "\n",
                "def fix_database_columns_notebook():\n",
                "    # Google Drive„Éë„Çπ„Çí‰ΩøÁî®\n",
                "    target_file = os.path.join(PROJECT_PATH, 'database.csv')\n",
                "    \n",
                "    if not os.path.exists(target_file):\n",
                "        print('Database file not found.')\n",
                "        return\n",
                "\n",
                "    print(f'Checking columns in {target_file}...')\n",
                "    try:\n",
                "        df = pd.read_csv(target_file, low_memory=False)\n",
                "        \n",
                "        # Ë°®Ë®ò„ÇÜ„ÇåËæûÊõ∏\n",
                "        columns_to_fix = {\n",
                "            '„Ç≥„Éº„Éä„ÉºÈÄöÈÅéÈ†Ü': '„Ç≥„Éº„Éä„Éº ÈÄöÈÅéÈ†Ü',\n",
                "            'È¶¨‰ΩìÈáç(Â¢óÊ∏õ)': 'È¶¨‰ΩìÈáç (Â¢óÊ∏õ)'\n",
                "        }\n",
                "        \n",
                "        changed = False\n",
                "        for bad_col, good_col in columns_to_fix.items():\n",
                "            if bad_col in df.columns:\n",
                "                print(f'Merging {bad_col} -> {good_col}...')\n",
                "                if good_col not in df.columns:\n",
                "                    df[good_col] = df[bad_col]\n",
                "                else:\n",
                "                    df[good_col] = df[good_col].fillna(df[bad_col])\n",
                "                \n",
                "                df.drop(columns=[bad_col], inplace=True)\n",
                "                changed = True\n",
                "        \n",
                "        if changed:\n",
                "            df.to_csv(target_file, index=False, encoding='utf-8-sig')\n",
                "            print('‚úÖ Database columns fixed and saved.')\n",
                "        else:\n",
                "            print('No column fixes needed.')\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f'Error fixing columns: {e}')\n",
                "\n",
                "fix_database_columns_notebook()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.2 Ê¨†Êêç„Éá„Éº„Çø„ÅÆË£úÂÆå (HorseID & ÈÅéÂéªËµ∞ & Metadata & Pedigree)\n",
                "import pandas as pd\n",
                "import sys\n",
                "import os\n",
                "import re\n",
                "from datetime import datetime\n",
                "import time\n",
                "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
                "import threading\n",
                "\n",
                "# TQDM progress bar\n",
                "try:\n",
                "    from tqdm.auto import tqdm\n",
                "except ImportError:\n",
                "    !pip install tqdm\n",
                "    from tqdm.auto import tqdm\n",
                "\n",
                "# Ensure scraper path\n",
                "try:\n",
                "    from scraper.race_scraper import RaceScraper\n",
                "except ImportError:\n",
                "    sys.path.append(os.path.join(PROJECT_PATH, 'scraper'))\n",
                "    from scraper.race_scraper import RaceScraper\n",
                "\n",
                "# Global lock for dataframe update\n",
                "df_lock = threading.Lock()\n",
                "\n",
                "def fetch_race_horse_ids(rid):\n",
                "    scraper = RaceScraper()\n",
                "    try:\n",
                "        url = f'https://race.netkeiba.com/race/result.html?race_id={rid}'\n",
                "        soup = scraper._get_soup(url)\n",
                "        if not soup: return None\n",
                "        \n",
                "        table = soup.find('table', id='All_Result_Table')\n",
                "        if not table: return None\n",
                "        \n",
                "        horse_map = {}\n",
                "        rows = table.find_all('tr', class_='HorseList')\n",
                "        for row in rows:\n",
                "            name_tag = row.select_one('.Horse_Name a')\n",
                "            if name_tag:\n",
                "                h_name = name_tag.text.strip()\n",
                "                href = name_tag.get('href', '')\n",
                "                match = re.search(r'/horse/(\\d+)', href)\n",
                "                if match:\n",
                "                    horse_map[h_name] = match.group(1)\n",
                "        return (rid, horse_map)\n",
                "    except Exception as e:\n",
                "        return None\n",
                "\n",
                "def fetch_horse_history(horse_id):\n",
                "    scraper = RaceScraper()\n",
                "    try:\n",
                "        df = scraper.get_past_races(str(horse_id), n_samples=None)\n",
                "        return (horse_id, df)\n",
                "    except Exception as e:\n",
                "        return (horse_id, pd.DataFrame())\n",
                "\n",
                "def fill_missing_past_data_notebook():\n",
                "    csv_path = os.path.join(PROJECT_PATH, 'database.csv')\n",
                "    if not os.path.exists(csv_path):\n",
                "        print(f'Error: {csv_path} not found.')\n",
                "        return\n",
                "\n",
                "    print(f'Reading {csv_path}...')\n",
                "    df = pd.read_csv(csv_path, low_memory=False, dtype={'race_id': str, 'horse_id': str})\\n    def clean_id_str(x):\\n        if pd.isna(x) or x == '': return None\\n        s = str(x)\\n        if s.endswith('.0'): return s[:-2]\\n        return s\\n    if 'race_id' in df.columns:\\n        df['race_id'] = df['race_id'].apply(clean_id_str)\\n    if 'horse_id' in df.columns:\\n        df['horse_id'] = df['horse_id'].apply(clean_id_str)\n",
                "\n",
                "    if 'Êó•‰ªò' in df.columns:\n",
                "        df['date_dt'] = pd.to_datetime(df['Êó•‰ªò'], format='%YÂπ¥%mÊúà%dÊó•', errors='coerce')\n",
                "    else:\n",
                "        print('Error: Êó•‰ªò column not found.')\n",
                "        return\n",
                "\n",
                "    if 'horse_id' not in df.columns:\n",
                "        df['horse_id'] = None\n",
                "\n",
                "    # === 1. Fill Missing Horse IDs ===\n",
                "    if 'race_id' in df.columns:\n",
                "        missing_mask = df['horse_id'].isna() | (df['horse_id'] == '')\n",
                "        if missing_mask.any():\n",
                "            races_to_update = df.loc[missing_mask, 'race_id'].astype(str).unique()\n",
                "            print(f'Need to fetch IDs for {len(races_to_update)} races...')\n",
                "            \n",
                "            with ThreadPoolExecutor(max_workers=10) as executor:\n",
                "                futures = {executor.submit(fetch_race_horse_ids, rid): rid for rid in races_to_update}\n",
                "                \n",
                "                for future in tqdm(as_completed(futures), total=len(races_to_update), desc=\"Fetching IDs\"):\n",
                "                    result = future.result()\n",
                "                    if result:\n",
                "                        rid, horse_map = result\n",
                "                        if horse_map:\n",
                "                            mask = df['race_id'].astype(str) == str(rid)\n",
                "                            for h_name, h_id in horse_map.items():\n",
                "                                h_mask = mask & (df['È¶¨Âêç'] == h_name)\n",
                "                                if h_mask.any():\n",
                "                                    df.loc[h_mask, 'horse_id'] = h_id\n",
                "            \n",
                "            df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
                "            print('Saved updated IDs.')\n",
                "        else:\n",
                "            print('All Horse IDs present.')\n",
                "\n",
                "    # === 2. Fill Past History ===\n",
                "    fields_map = {\n",
                "        'date': 'date', 'rank': 'rank', 'time': 'time', 'run_style': 'run_style',\n",
                "        'race_name': 'race_name', 'last_3f': 'last_3f', 'horse_weight': 'horse_weight',\n",
                "        'jockey': 'jockey', 'condition': 'condition', 'odds': 'odds',\n",
                "        'weather': 'weather', 'distance': 'distance', 'course_type': 'course_type'\n",
                "    }\n",
                "\n",
                "    unique_horses = df['horse_id'].dropna().astype(str).unique()\n",
                "    \n",
                "    # Init columns\n",
                "    new_cols = []\n",
                "    for k in fields_map.keys():\n",
                "        for i in range(1, 6):\n",
                "            col = f'past_{i}_{k}'\n",
                "            if col not in df.columns:\n",
                "                new_cols.append(col)\n",
                "    \n",
                "    if new_cols:\n",
                "        df_new = pd.DataFrame(None, index=df.index, columns=new_cols, dtype='object')\n",
                "        df = pd.concat([df, df_new], axis=1)\n",
                "\n",
                "    print(f'Found {len(unique_horses)} unique horses. Fetching history...')\n",
                "    history_store = {}\n",
                "\n",
                "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
                "        futures = {executor.submit(fetch_horse_history, hid): hid for hid in unique_horses}\n",
                "        \n",
                "        for future in tqdm(as_completed(futures), total=len(unique_horses), desc=\"Fetching History\"):\n",
                "            hid, hist_df = future.result()\n",
                "            if not hist_df.empty:\n",
                "                history_store[str(hid)] = hist_df\n",
                "\n",
                "    print('Applying history data...')\n",
                "    for hid in history_store:\n",
                "        if 'date' in history_store[hid]:\n",
                "            history_store[hid]['date_obj'] = pd.to_datetime(history_store[hid]['date'], errors='coerce')\n",
                "\n",
                "    valid_rows = df.dropna(subset=['horse_id', 'date_dt'])\n",
                "    \n",
                "    for idx, row in tqdm(valid_rows.iterrows(), total=len(valid_rows), desc=\"Applying History\"):\n",
                "        hid = str(row['horse_id'])\n",
                "        current_date = row['date_dt']\n",
                "        \n",
                "        if hid in history_store:\n",
                "            hist_df = history_store[hid]\n",
                "            if hist_df.empty or 'date_obj' not in hist_df.columns: continue\n",
                "            \n",
                "            past_races = hist_df[hist_df['date_obj'] < current_date].sort_values('date_obj', ascending=False).head(5)\n",
                "            \n",
                "            for i, (p_idx, p_row) in enumerate(past_races.iterrows()):\n",
                "                n = i + 1\n",
                "                for k, v in fields_map.items():\n",
                "                    df.at[idx, f'past_{n}_{k}'] = p_row.get(v)\n",
                "\n",
                "    if 'date_dt' in df.columns: df.drop(columns=['date_dt'], inplace=True)\n",
                "    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
                "    print('Done filling past data.')\n",
                "\n",
                "    # === 3. Fill Blood/Pedigree Data ===\n",
                "    print('Checking Pedigree Data...')\n",
                "    ped_cols = ['father', 'mother', 'bms']\n",
                "    new_ped_cols = [c for c in ped_cols if c not in df.columns]\n",
                "    if new_ped_cols:\n",
                "         df_ped = pd.DataFrame(None, index=df.index, columns=new_ped_cols, dtype='object')\n",
                "         df = pd.concat([df, df_ped], axis=1)\n",
                "\n",
                "    unique_horses_df = df[['horse_id', 'father']].drop_duplicates('horse_id')\n",
                "    missing_horses = unique_horses_df[unique_horses_df['father'].isna() | (unique_horses_df['father'] == '')]['horse_id'].dropna().astype(str).unique()\n",
                "    \n",
                "    if len(missing_horses) > 0:\n",
                "        print(f'Found {len(missing_horses)} horses with missing pedigree. Fetching...')\n",
                "        ped_map = {}\n",
                "        \n",
                "        def fetch_ped_entry(hid):\n",
                "            scr = RaceScraper()\n",
                "            return (hid, scr.get_horse_profile(hid))\n",
                "            \n",
                "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
                "            futures = {executor.submit(fetch_ped_entry, hid): hid for hid in missing_horses}\n",
                "            for future in tqdm(as_completed(futures), total=len(missing_horses), desc=\"Fetching Pedigree\"):\n",
                "                try:\n",
                "                    hid, p_data = future.result()\n",
                "                    if p_data:\n",
                "                        ped_map[str(hid)] = p_data\n",
                "                except: pass\n",
                "        \n",
                "        print('Applying pedigree data...')\n",
                "        f_map = {h: d.get('father') for h, d in ped_map.items()}\n",
                "        m_map = {h: d.get('mother') for h, d in ped_map.items()}\n",
                "        b_map = {h: d.get('bms') for h, d in ped_map.items()}\n",
                "        \n",
                "        df['horse_id_str'] = df['horse_id'].astype(str)\n",
                "        mask = df['horse_id_str'].isin(ped_map.keys())\n",
                "        df.loc[mask, 'father'] = df.loc[mask, 'horse_id_str'].map(f_map).fillna(df.loc[mask, 'father'])\n",
                "        df.loc[mask, 'mother'] = df.loc[mask, 'horse_id_str'].map(m_map).fillna(df.loc[mask, 'mother'])\n",
                "        df.loc[mask, 'bms'] = df.loc[mask, 'horse_id_str'].map(b_map).fillna(df.loc[mask, 'bms'])\n",
                "        \n",
                "        df.drop(columns=['horse_id_str'], inplace=True, errors='ignore')\n",
                "        \n",
                "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
                "        print('Done filling pedigree.')\n",
                "\n",
                "    # === 4. Fill Metadata (Course, Dist, Weather) ===\n",
                "    print('Checking Metadata (Course, Distance, Weather)...')\n",
                "    meta_cols = ['„Ç≥„Éº„Çπ„Çø„Ç§„Éó', 'Ë∑ùÈõ¢', 'Â§©ÂÄô', 'È¶¨Â†¥Áä∂ÊÖã']\n",
                "    for c in meta_cols:\n",
                "        if c not in df.columns: df[c] = None\n",
                "\n",
                "    missing_meta_mask = (df['„Ç≥„Éº„Çπ„Çø„Ç§„Éó'].isna()) | (df['„Ç≥„Éº„Çπ„Çø„Ç§„Éó'] == '')\n",
                "    if 'race_id' in df.columns:\n",
                "         rids_missing = df.loc[missing_meta_mask, 'race_id'].astype(str).unique()\n",
                "    else:\n",
                "         rids_missing = []\n",
                "\n",
                "    if len(rids_missing) > 0:\n",
                "        print(f'Found {len(rids_missing)} races with missing metadata. Fetching...')\n",
                "        meta_results = {}\n",
                "        \n",
                "        def fetch_meta_entry(rid):\n",
                "            scr = RaceScraper()\n",
                "            return scr.get_race_metadata(rid)\n",
                "            \n",
                "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
                "            futures = {executor.submit(fetch_meta_entry, rid): rid for rid in rids_missing}\n",
                "            \n",
                "            for future in tqdm(as_completed(futures), total=len(rids_missing), desc=\"Fetching Metadata\"):\n",
                "                try:\n",
                "                    data = future.result()\n",
                "                    if data and data.get('course_type'):\n",
                "                        meta_results[str(data['race_id'])] = data\n",
                "                except: pass\n",
                "        \n",
                "        print('Applying metadata...')\n",
                "        c_map = {rid: d['course_type'] for rid, d in meta_results.items()}\n",
                "        d_map = {rid: d['distance'] for rid, d in meta_results.items()}\n",
                "        w_map = {rid: d['weather'] for rid, d in meta_results.items()}\n",
                "        cond_map = {rid: d['condition'] for rid, d in meta_results.items()}\n",
                "        \n",
                "        df['race_id_str'] = df['race_id'].astype(str)\n",
                "        mask = df['race_id_str'].isin(meta_results.keys())\n",
                "        \n",
                "        df.loc[mask, '„Ç≥„Éº„Çπ„Çø„Ç§„Éó'] = df.loc[mask, 'race_id_str'].map(c_map).fillna(df.loc[mask, '„Ç≥„Éº„Çπ„Çø„Ç§„Éó'])\n",
                "        df.loc[mask, 'Ë∑ùÈõ¢'] = df.loc[mask, 'race_id_str'].map(d_map).fillna(df.loc[mask, 'Ë∑ùÈõ¢'])\n",
                "        df.loc[mask, 'Â§©ÂÄô'] = df.loc[mask, 'race_id_str'].map(w_map).fillna(df.loc[mask, 'Â§©ÂÄô'])\n",
                "        df.loc[mask, 'È¶¨Â†¥Áä∂ÊÖã'] = df.loc[mask, 'race_id_str'].map(cond_map).fillna(df.loc[mask, 'È¶¨Â†¥Áä∂ÊÖã'])\n",
                "        \n",
                "        df.drop(columns=['race_id_str'], inplace=True, errors='ignore')\n",
                "\n",
                "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
                "        print('Done filling metadata.')\n",
                "    else:\n",
                "        print('All metadata present.')\n",
                "\n",
                "fill_missing_past_data_notebook()\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# 5. „Éá„Éº„ÇøÂä†Â∑• (Feature Engineering) „ÅÆÂÆüË°å\n",
                "# ----------------------------------------\n",
                "# „Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„Åó„Åü database.csv „Åã„ÇâÂ≠¶ÁøíÁî®„Éá„Éº„Çø„ÇíÁîüÊàê„Åó„Åæ„Åô\n",
                "from ml.feature_engineering import calculate_features\n",
                "\n",
                "INPUT_CSV = os.path.join(PROJECT_PATH, \"database.csv\")\n",
                "OUTPUT_CSV = os.path.join(PROJECT_PATH, \"processed_data.csv\")\n",
                "\n",
                "if os.path.exists(INPUT_CSV):\n",
                "    print(\"Starting Feature Engineering...\")\n",
                "    calculate_features(INPUT_CSV, OUTPUT_CSV)\n",
                "    print(\"Done!\")\n",
                "else:\n",
                "    print(\"Error: database.csv not found.\")\n"
            ],
            "metadata": {
                "id": "feature_eng"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}