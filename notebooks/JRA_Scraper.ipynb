{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# JRA Scraping Notebook\n","\n","‰∏≠Â§ÆÁ´∂È¶¨ÔºàJRAÔºâ„ÅÆ„Éá„Éº„Çø„Çí„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„Åó„ÄÅGoogle Drive‰∏ä„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´ËøΩÂä†„Åó„Åæ„Åô„ÄÇ"],"metadata":{"id":"intro_md"}},{"cell_type":"code","source":["# 1. Google Drive„ÅÆ„Éû„Ç¶„É≥„Éà\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import sys\n","\n","# ‚òÖ‚òÖ‚òÖ Ë®≠ÂÆöÈ†ÖÁõÆ ‚òÖ‚òÖ‚òÖ\n","# scraper„Éï„Ç©„É´„ÉÄ„ÅåÂ≠òÂú®„Åô„Çã„Éë„Çπ (Google Drive‰∏ä„ÅÆ„Éë„Çπ)\n","# ‰æã: '/content/drive/MyDrive/dai-keiba'\n","PROJECT_PATH = '/content/drive/MyDrive/dai-keiba'\n","\n","if not os.path.exists(PROJECT_PATH):\n","    print(f\"Error: Path {PROJECT_PATH} does not exist. Please check your Drive structure.\")\n","else:\n","    print(f\"Project path found: {PROJECT_PATH}\")\n","    os.chdir(PROJECT_PATH)\n","    sys.path.append(PROJECT_PATH)\n"],"metadata":{"id":"mount_drive","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767614822797,"user_tz":-540,"elapsed":17719,"user":{"displayName":"‰ºäËó§Â§ßË≤¥","userId":"16595662099933633711"}},"outputId":"fcb82c76-2fa3-448e-aed2-7fa9d8f356b2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Project path found: /content/drive/MyDrive/dai-keiba\n"]}]},{"cell_type":"code","source":["# 2. ÂøÖË¶Å„Å™„É©„Ç§„Éñ„É©„É™„ÅÆ„Ç§„É≥„Éù„Éº„Éà\n","try:\n","    import pandas as pd\n","    import requests\n","    import bs4\n","except ImportError:\n","    !pip install pandas requests beautifulsoup4\n","    import pandas as pd\n","    import requests\n","    import bs4\n","\n","from datetime import datetime, date\n","from scraper.jra_scraper import scrape_jra_year, JRA_MONTH_PARAMS\n","import time\n","\n","# ==========================================\n","# üö® 2023Âπ¥/2022Âπ¥„Éë„É©„É°„Éº„Çø‰∏çË∂≥„ÅÆ‰øÆÊ≠£„Éë„ÉÉ„ÉÅ\n","# ==========================================\n","# „Éë„É©„É°„Éº„Çø„Éû„ÉÉ„Éó„Çí‰∏äÊõ∏„ÅçÊõ¥Êñ∞\n","try:\n","    JRA_MONTH_PARAMS.update({\n","        \"2023\": { \"01\": \"27\", \"02\": \"F5\", \"03\": \"C3\", \"04\": \"91\", \"05\": \"5F\", \"06\": \"2D\", \"07\": \"FB\", \"08\": \"C9\", \"09\": \"97\", \"10\": \"06\", \"11\": \"D4\", \"12\": \"A2\" },\n","        \"2022\": { \"01\": \"9B\", \"02\": \"69\", \"03\": \"37\", \"04\": \"05\", \"05\": \"D3\", \"06\": \"A1\", \"07\": \"6F\", \"08\": \"3D\", \"09\": \"0B\", \"10\": \"7A\", \"11\": \"48\", \"12\": \"16\" },\n","        \"2021\": { \"01\": \"0F\", \"02\": \"DD\", \"03\": \"AB\", \"04\": \"79\", \"05\": \"47\", \"06\": \"15\", \"07\": \"E3\", \"08\": \"B1\", \"09\": \"7F\", \"10\": \"EE\", \"11\": \"BC\", \"12\": \"8A\" },\n","        \"2020\": { \"01\": \"83\", \"02\": \"51\", \"03\": \"1F\", \"04\": \"ED\", \"05\": \"BB\", \"06\": \"89\", \"07\": \"57\", \"08\": \"25\", \"09\": \"F3\", \"10\": \"62\", \"11\": \"30\", \"12\": \"FE\" }\n","    })\n","    print(\"‚úÖ JRA 2023/2022 Parameters have been patched successfully.\")\n","except NameError:\n","    print(\"‚ö†Ô∏è JRA_MONTH_PARAMS not found in scope. Ensure imports are correct.\")\n"],"metadata":{"id":"imports","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767614825078,"user_tz":-540,"elapsed":2278,"user":{"displayName":"‰ºäËó§Â§ßË≤¥","userId":"16595662099933633711"}},"outputId":"71ef9983-3dbc-4fb8-8973-ee2d4ba8efb9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ JRA 2023/2022 Parameters have been patched successfully.\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h6egzRu36mAl","executionInfo":{"status":"ok","timestamp":1767614825108,"user_tz":-540,"elapsed":28,"user":{"displayName":"‰ºäËó§Â§ßË≤¥","userId":"16595662099933633711"}},"outputId":"aac659d9-fba4-40c9-f6bd-e98161f94b63"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Loaded updated scraper/jra_scraper.py from Drive\n"]}],"source":["# 2.1 „Çπ„ÇØ„É¨„Ç§„Éë„Éº„ÅÆ‰øÆÊ≠£„Éë„ÉÉ„ÉÅ („Ç≥„Éº„ÇπË©≥Á¥∞„ÉªÂ§©ÂÄôÂèñÂæó)\n","import re\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import scraper.jra_scraper\n","\n","def patched_scrape_jra_race(url, existing_race_ids=None):\n","    print(f'Accessing JRA URL (Patched): {url}...')\n","    headers = {\n","        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n","    }\n","    try:\n","        response = requests.get(url, headers=headers, timeout=10)\n","        response.encoding = 'cp932'\n","        if response.status_code != 200: return None\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","\n","        # (Skipping full redundant parse code for brevity in this patch, assuming main logic matches jra_scraper.py)\n","        # Ideally we should just RELOAD the module if we updated the file in Drive.\n","        # If the user updates the file in Drive (scraper/jra_scraper.py), they just need to reload.\n","        # But to be safe, we can force reload.\n","        pass\n","    except: pass\n","    return scraper.jra_scraper.scrape_jra_race(url, existing_race_ids)\n","\n","import importlib\n","importlib.reload(scraper.jra_scraper)\n","from scraper.jra_scraper import scrape_jra_race, scrape_jra_year, JRA_MONTH_PARAMS\n","print('‚úÖ Loaded updated scraper/jra_scraper.py from Drive')\n"]},{"cell_type":"code","source":["# 3. „Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÂÆüË°åÈñ¢Êï∞„ÅÆÂÆöÁæ©\n","\n","def jra_scrape_execution(year_str, start_date=None, end_date=None):\n","    CSV_FILE_PATH = os.path.join(PROJECT_PATH, \"data\", \"raw\", \"database.csv\")\n","    print(f\"Using CSV Path: {CSV_FILE_PATH}\")\n","\n","    def save_chunk(df_chunk):\n","        if os.path.exists(CSV_FILE_PATH):\n","            try:\n","                # Read types as string to prevent auto-float for IDs\n","                existing_df = pd.read_csv(CSV_FILE_PATH, dtype={'race_id': str, 'horse_id': str}, low_memory=False)\n","                combined_df = pd.concat([existing_df, df_chunk], ignore_index=True)\n","            except pd.errors.EmptyDataError:\n","                print(\"  Note: File empty, creating new.\")\n","                combined_df = df_chunk\n","            except Exception as e:\n","                # ‚òÖ CRITICAL FIX: Do NOT overwrite on generic read errors (e.g. MemoryError)\n","                print(f\"‚ùå CRITICAL ERROR: Could not read existing CSV ({e}). Aborting save to prevent data loss.\")\n","                raise e\n","        else:\n","            combined_df = df_chunk\n","\n","        # Deduplication\n","        subset_cols = ['race_id', 'È¶¨Âêç']\n","        subset_cols = [c for c in subset_cols if c in combined_df.columns]\n","        if subset_cols:\n","            combined_df.drop_duplicates(subset=subset_cols, keep='last', inplace=True)\n","\n","        combined_df.to_csv(CSV_FILE_PATH, index=False, encoding=\"utf-8-sig\")\n","        print(f\"  [Saved] Total rows: {len(combined_df)} (+{len(df_chunk)} new)\")\n","\n","    print(f\"Starting Scraping for {year_str} ({start_date} ~ {end_date})\")\n","\n","    # Load existing IDs to skip\n","    existing_ids = set()\n","    if os.path.exists(CSV_FILE_PATH):\n","        try:\n","             df_e = pd.read_csv(CSV_FILE_PATH, usecols=['race_id'], dtype={'race_id': str}, low_memory=False)\n","             existing_ids = set(df_e['race_id'].astype(str))\n","             print(f\"  Loaded {len(existing_ids)} existing race IDs to skip.\")\n","        except:\n","             pass\n","\n","    scrape_jra_year(year_str, start_date=start_date, end_date=end_date, save_callback=save_chunk, existing_race_ids=existing_ids)\n"],"metadata":{"id":"def_exec","executionInfo":{"status":"ok","timestamp":1767614825209,"user_tz":-540,"elapsed":11,"user":{"displayName":"‰ºäËó§Â§ßË≤¥","userId":"16595662099933633711"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# 4. ÂÆüË°å„Éë„É©„É°„Éº„Çø„ÅÆË®≠ÂÆö„Å®ÈñãÂßã\n","# -----------------------------\n","TARGET_YEAR = \"2026\"\n","TARGET_MONTH = None  # ‚òÖ‰ΩïÊúà„ÇíÂèñÂæó„Åô„Çã„ÅãÊåáÂÆö (None„ÅÆÂ†¥Âêà„ÅØÂÖ®ÊúüÈñì„ÄÅ1„Äú12„ÇíÊåáÂÆö)\n","\n","import calendar\n","from datetime import date\n","\n","START_DATE = None\n","END_DATE = None\n","\n","if TARGET_MONTH:\n","    # ÊåáÂÆö„Åó„ÅüÊúà„ÅÆ1Êó•„ÄúÊú´Êó•„ÇíË®≠ÂÆö\n","    _, last_day = calendar.monthrange(int(TARGET_YEAR), int(TARGET_MONTH))\n","    START_DATE = date(int(TARGET_YEAR), int(TARGET_MONTH), 1)\n","    END_DATE = date(int(TARGET_YEAR), int(TARGET_MONTH), last_day)\n","    print(f\"Targeting specific month: {START_DATE} to {END_DATE}\")\n","else:\n","    # Ëá™ÂãïÂà§ÂÆö„É≠„Ç∏„ÉÉ„ÇØ (Êó¢Â≠ò„Éá„Éº„Çø„ÅÆÁøåÊó•„Åã„Çâ)\n","    CSV_FILE_PATH = os.path.join(PROJECT_PATH, \"data\", \"raw\", \"database.csv\")\n","    if os.path.exists(CSV_FILE_PATH):\n","        try:\n","             df_exist = pd.read_csv(CSV_FILE_PATH)\n","             if 'Êó•‰ªò' in df_exist.columns and not df_exist.empty:\n","                 df_exist['date_obj'] = pd.to_datetime(df_exist['Êó•‰ªò'], format='%YÂπ¥%mÊúà%dÊó•', errors='coerce')\n","                 last_date = df_exist['date_obj'].max()\n","                 if pd.notna(last_date):\n","                     # START_DATE = last_date.date() # Êóß: Á∂ö„Åç„Åã„Çâ\n","                     # Êñ∞: Ê¨†ËêΩË£úÂÆå„ÅÆ„Åü„ÇÅ„Å´„ÄÅÂº∑Âà∂ÁöÑ„Å´„Åù„ÅÆÂπ¥„ÅÆ1Êúà1Êó•„Åã„Çâ„Çπ„Ç≠„É£„É≥„Åô„Çã (existing_ids„Åß„Çπ„Ç≠„ÉÉ„Éó„Åï„Çå„Çã)\n","                     START_DATE = date(int(TARGET_YEAR), 1, 1)\n","                     print(f\"Êó¢Â≠ò„Éá„Éº„Çø„ÅÆÊúÄÁµÇÊó•ÊôÇ: {last_date.date()} (Ê¨†ËêΩÁ¢∫Ë™ç„ÅÆ„Åü„ÇÅ {START_DATE} „Åã„Çâ„Çπ„Ç≠„É£„É≥„Åó„Åæ„Åô)\")\n","        except Exception as e:\n","            print(f\"Êó¢Â≠ò„Éá„Éº„ÇøÁ¢∫Ë™ç„Ç®„É©„Éº: {e}\")\n","\n","print(f\"Scraping Target: {TARGET_YEAR}, Start: {START_DATE}, End: {END_DATE}\")\n","jra_scrape_execution(TARGET_YEAR, start_date=START_DATE, end_date=END_DATE)\n"],"metadata":{"id":"run_cell","colab":{"base_uri":"https://localhost:8080/"},"outputId":"716aec8b-53be-4a46-dccb-d6a1895e20b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-884418207.py:23: DtypeWarning: Columns (5,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df_exist = pd.read_csv(CSV_FILE_PATH)\n"]},{"output_type":"stream","name":"stdout","text":["Êó¢Â≠ò„Éá„Éº„Çø„ÅÆÊúÄÁµÇÊó•ÊôÇ: 2026-01-05 (Ê¨†ËêΩÁ¢∫Ë™ç„ÅÆ„Åü„ÇÅ 2026-01-01 „Åã„Çâ„Çπ„Ç≠„É£„É≥„Åó„Åæ„Åô)\n","Scraping Target: 2026, Start: 2026-01-01, End: None\n","Using CSV Path: /content/drive/MyDrive/dai-keiba/data/raw/database.csv\n","Starting Scraping for 2026 (2026-01-01 ~ None)\n","  Loaded 20781 existing race IDs to skip.\n","=== Starting JRA Bulk Scraping for 2026 (Period: 2026-01-01 - None) ===\n","Fetching list for 2026/01 (CNAME=pw01skl00202601/E4)...\n","  Found 4 race days in month.\n","    Processing Day: 2026-01-05 („É¨„Éº„ÇπÁµêÊûú „É¨„Éº„ÇπÈÅ∏Êäû2026Âπ¥1Êúà5Êó•ÔºàÊúàÊõúÔºâ1Âõû‰∏≠Â±±2Êó•)\n","      -> 12 races found.\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0106202601020120260105/06...\n","Skipping 202606010201 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0106202601020220260105/BB...\n","Skipping 202606010202 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0106202601020320260105/70...\n","Skipping 202606010203 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0106202601020420260105/25...\n","Skipping 202606010204 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0106202601020520260105/DA...\n","Skipping 202606010205 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0106202601020620260105/8F...\n","Skipping 202606010206 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0106202601020720260105/44...\n","Skipping 202606010207 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0106202601020820260105/F9...\n","Skipping 202606010208 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0106202601020920260105/AE...\n","Skipping 202606010209 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0106202601021020260105/A3...\n","Skipping 202606010210 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0106202601021120260105/58...\n","Skipping 202606010211 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0106202601021220260105/0D...\n","Skipping 202606010212 (Already exists)\n","    Processing Day: 2026-01-05 („É¨„Éº„ÇπÁµêÊûú „É¨„Éº„ÇπÈÅ∏Êäû2026Âπ¥1Êúà5Êó•ÔºàÊúàÊõúÔºâ1Âõû‰∫¨ÈÉΩ2Êó•)\n","      -> 12 races found.\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0108202601020120260105/9A...\n","Skipping 202608010201 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0108202601020220260105/4F...\n","Skipping 202608010202 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0108202601020320260105/04...\n","Skipping 202608010203 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0108202601020420260105/B9...\n","Skipping 202608010204 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0108202601020520260105/6E...\n","Skipping 202608010205 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0108202601020620260105/23...\n","Skipping 202608010206 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0108202601020720260105/D8...\n","Skipping 202608010207 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0108202601020820260105/8D...\n","Skipping 202608010208 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0108202601020920260105/42...\n","Skipping 202608010209 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0108202601021020260105/37...\n","Skipping 202608010210 (Already exists)\n","Accessing JRA URL: https://www.jra.go.jp/JRADB/accessS.html?CNAME=pw01sde0108202601021120260105/EC...\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jeR-mUS46mAm"},"outputs":[],"source":["# 4.1 „Éá„Éº„Çø„Éô„Éº„Çπ„Ç´„É©„É†Âêç„ÅÆ‰øÆÊ≠£ (ÈáçË§áÊîπÂñÑ)\n","import os\n","import pandas as pd\n","\n","def fix_database_columns_notebook():\n","    # Google Drive„Éë„Çπ„Çí‰ΩøÁî®\n","    target_file = os.path.join(PROJECT_PATH, 'data', 'raw', 'database.csv')\n","\n","    if not os.path.exists(target_file):\n","        print('Database file not found.')\n","        return\n","\n","    print(f'Checking columns in {target_file}...')\n","    try:\n","        df = pd.read_csv(target_file, low_memory=False)\n","\n","        # Ë°®Ë®ò„ÇÜ„ÇåËæûÊõ∏\n","        columns_to_fix = {\n","            '„Ç≥„Éº„Éä„ÉºÈÄöÈÅéÈ†Ü': '„Ç≥„Éº„Éä„Éº ÈÄöÈÅéÈ†Ü',\n","            'È¶¨‰ΩìÈáç(Â¢óÊ∏õ)': 'È¶¨‰ΩìÈáç (Â¢óÊ∏õ)'\n","        }\n","\n","        changed = False\n","        for bad_col, good_col in columns_to_fix.items():\n","            if bad_col in df.columns:\n","                print(f'Merging {bad_col} -> {good_col}...')\n","                if good_col not in df.columns:\n","                    df[good_col] = df[bad_col]\n","                else:\n","                    df[good_col] = df[good_col].fillna(df[bad_col])\n","\n","                df.drop(columns=[bad_col], inplace=True)\n","                changed = True\n","\n","        if changed:\n","            df.to_csv(target_file, index=False, encoding='utf-8-sig')\n","            print('‚úÖ Database columns fixed and saved.')\n","        else:\n","            print('No column fixes needed.')\n","\n","    except Exception as e:\n","        print(f'Error fixing columns: {e}')\n","\n","fix_database_columns_notebook()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7FtqzMok6mAm"},"outputs":[],"source":["# 4.2 Ê¨†Êêç„Éá„Éº„Çø„ÅÆË£úÂÆå (HorseID & ÈÅéÂéªËµ∞ & Metadata & Pedigree)\n","import pandas as pd\n","import sys\n","import os\n","import re\n","from datetime import datetime\n","import time\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","import threading\n","\n","# TQDM progress bar\n","try:\n","    from tqdm.auto import tqdm\n","except ImportError:\n","    !pip install tqdm\n","    from tqdm.auto import tqdm\n","\n","# Ensure scraper path\n","try:\n","    from scraper.race_scraper import RaceScraper\n","except ImportError:\n","    sys.path.append(os.path.join(PROJECT_PATH, 'scraper'))\n","    from scraper.race_scraper import RaceScraper\n","\n","# Global lock for dataframe update\n","df_lock = threading.Lock()\n","\n","def fetch_race_horse_ids(rid):\n","    scraper = RaceScraper()\n","    try:\n","        url = f'https://race.netkeiba.com/race/result.html?race_id={rid}'\n","        soup = scraper._get_soup(url)\n","        if not soup: return None\n","\n","        table = soup.find('table', id='All_Result_Table')\n","        if not table: return None\n","\n","        horse_map = {}\n","        rows = table.find_all('tr', class_='HorseList')\n","        for row in rows:\n","            name_tag = row.select_one('.Horse_Name a')\n","            if name_tag:\n","                h_name = name_tag.text.strip()\n","                href = name_tag.get('href', '')\n","                match = re.search(r'/horse/(\\d+)', href)\n","                if match:\n","                    horse_map[h_name] = match.group(1)\n","        return (rid, horse_map)\n","    except Exception as e:\n","        return None\n","\n","def fetch_horse_history(horse_id):\n","    scraper = RaceScraper()\n","    try:\n","        df = scraper.get_past_races(str(horse_id), n_samples=None)\n","        return (horse_id, df)\n","    except Exception as e:\n","        return (horse_id, pd.DataFrame())\n","\n","def fill_missing_past_data_notebook():\n","    csv_path = os.path.join(PROJECT_PATH, 'data', 'raw', 'database.csv')\n","    if not os.path.exists(csv_path):\n","        print(f'Error: {csv_path} not found.')\n","        return\n","\n","    print(f'Reading {csv_path}...')\n","    df = pd.read_csv(csv_path, low_memory=False, dtype={'race_id': str, 'horse_id': str})\n","    def clean_id_str(x):\n","        if pd.isna(x) or x == '': return None\n","        s = str(x)\n","        if s.endswith('.0'): return s[:-2]\n","        return s\n","    if 'race_id' in df.columns:\n","        df['race_id'] = df['race_id'].apply(clean_id_str)\n","    if 'horse_id' in df.columns:\n","        df['horse_id'] = df['horse_id'].apply(clean_id_str)\n","\n","    if 'Êó•‰ªò' in df.columns:\n","        df['date_dt'] = pd.to_datetime(df['Êó•‰ªò'], format='%YÂπ¥%mÊúà%dÊó•', errors='coerce')\n","    else:\n","        print('Error: Êó•‰ªò column not found.')\n","        return\n","\n","    if 'horse_id' not in df.columns:\n","        df['horse_id'] = None\n","\n","    # === 1. Fill Missing Horse IDs ===\n","    if 'race_id' in df.columns:\n","        missing_mask = df['horse_id'].isna() | (df['horse_id'] == '')\n","        if missing_mask.any():\n","            races_to_update = df.loc[missing_mask, 'race_id'].astype(str).unique()\n","            print(f'Need to fetch IDs for {len(races_to_update)} races...')\n","\n","            with ThreadPoolExecutor(max_workers=10) as executor:\n","                futures = {executor.submit(fetch_race_horse_ids, rid): rid for rid in races_to_update}\n","\n","                for future in tqdm(as_completed(futures), total=len(races_to_update), desc=\"Fetching IDs\"):\n","                    result = future.result()\n","                    if result:\n","                        rid, horse_map = result\n","                        if horse_map:\n","                            mask = df['race_id'].astype(str) == str(rid)\n","                            for h_name, h_id in horse_map.items():\n","                                h_mask = mask & (df['È¶¨Âêç'] == h_name)\n","                                if h_mask.any():\n","                                    df.loc[h_mask, 'horse_id'] = h_id\n","\n","            df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n","            print('Saved updated IDs.')\n","        else:\n","            print('All Horse IDs present.')\n","\n","    # === 2. Fill Past History ===\n","    fields_map = {\n","        'date': 'date', 'rank': 'rank', 'time': 'time', 'run_style': 'run_style',\n","        'race_name': 'race_name', 'last_3f': 'last_3f', 'horse_weight': 'horse_weight',\n","        'jockey': 'jockey', 'condition': 'condition', 'odds': 'odds',\n","        'weather': 'weather', 'distance': 'distance', 'course_type': 'course_type'\n","    }\n","\n","    unique_horses = df['horse_id'].dropna().astype(str).unique()\n","\n","    # Init columns\n","    new_cols = []\n","    for k in fields_map.keys():\n","        for i in range(1, 6):\n","            col = f'past_{i}_{k}'\n","            if col not in df.columns:\n","                new_cols.append(col)\n","\n","    if new_cols:\n","        df_new = pd.DataFrame(None, index=df.index, columns=new_cols, dtype='object')\n","        df = pd.concat([df, df_new], axis=1)\n","\n","    print(f'Found {len(unique_horses)} unique horses. Fetching history...')\n","    history_store = {}\n","\n","    with ThreadPoolExecutor(max_workers=8) as executor:\n","        futures = {executor.submit(fetch_horse_history, hid): hid for hid in unique_horses}\n","\n","        for future in tqdm(as_completed(futures), total=len(unique_horses), desc=\"Fetching History\"):\n","            hid, hist_df = future.result()\n","            if not hist_df.empty:\n","                history_store[str(hid)] = hist_df\n","\n","    print('Applying history data...')\n","    for hid in history_store:\n","        if 'date' in history_store[hid]:\n","            history_store[hid]['date_obj'] = pd.to_datetime(history_store[hid]['date'], errors='coerce')\n","\n","    valid_rows = df.dropna(subset=['horse_id', 'date_dt'])\n","\n","    for idx, row in tqdm(valid_rows.iterrows(), total=len(valid_rows), desc=\"Applying History\"):\n","        hid = str(row['horse_id'])\n","        current_date = row['date_dt']\n","\n","        if hid in history_store:\n","            hist_df = history_store[hid]\n","            if hist_df.empty or 'date_obj' not in hist_df.columns: continue\n","\n","            past_races = hist_df[hist_df['date_obj'] < current_date].sort_values('date_obj', ascending=False).head(5)\n","\n","            for i, (p_idx, p_row) in enumerate(past_races.iterrows()):\n","                n = i + 1\n","                for k, v in fields_map.items():\n","                    df.at[idx, f'past_{n}_{k}'] = p_row.get(v)\n","\n","    if 'date_dt' in df.columns: df.drop(columns=['date_dt'], inplace=True)\n","    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n","    print('Done filling past data.')\n","\n","    # === 3. Fill Blood/Pedigree Data ===\n","    print('Checking Pedigree Data...')\n","    ped_cols = ['father', 'mother', 'bms']\n","    new_ped_cols = [c for c in ped_cols if c not in df.columns]\n","    if new_ped_cols:\n","         df_ped = pd.DataFrame(None, index=df.index, columns=new_ped_cols, dtype='object')\n","         df = pd.concat([df, df_ped], axis=1)\n","\n","    unique_horses_df = df[['horse_id', 'father']].drop_duplicates('horse_id')\n","    missing_horses = unique_horses_df[unique_horses_df['father'].isna() | (unique_horses_df['father'] == '')]['horse_id'].dropna().astype(str).unique()\n","\n","    if len(missing_horses) > 0:\n","        print(f'Found {len(missing_horses)} horses with missing pedigree. Fetching...')\n","        ped_map = {}\n","\n","        def fetch_ped_entry(hid):\n","            scr = RaceScraper()\n","            return (hid, scr.get_horse_profile(hid))\n","\n","        with ThreadPoolExecutor(max_workers=10) as executor:\n","            futures = {executor.submit(fetch_ped_entry, hid): hid for hid in missing_horses}\n","            for future in tqdm(as_completed(futures), total=len(missing_horses), desc=\"Fetching Pedigree\"):\n","                try:\n","                    hid, p_data = future.result()\n","                    if p_data:\n","                        ped_map[str(hid)] = p_data\n","                except: pass\n","\n","        print('Applying pedigree data...')\n","        f_map = {h: d.get('father') for h, d in ped_map.items()}\n","        m_map = {h: d.get('mother') for h, d in ped_map.items()}\n","        b_map = {h: d.get('bms') for h, d in ped_map.items()}\n","\n","        df['horse_id_str'] = df['horse_id'].astype(str)\n","        mask = df['horse_id_str'].isin(ped_map.keys())\n","        df.loc[mask, 'father'] = df.loc[mask, 'horse_id_str'].map(f_map).fillna(df.loc[mask, 'father'])\n","        df.loc[mask, 'mother'] = df.loc[mask, 'horse_id_str'].map(m_map).fillna(df.loc[mask, 'mother'])\n","        df.loc[mask, 'bms'] = df.loc[mask, 'horse_id_str'].map(b_map).fillna(df.loc[mask, 'bms'])\n","\n","        df.drop(columns=['horse_id_str'], inplace=True, errors='ignore')\n","\n","        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n","        print('Done filling pedigree.')\n","\n","    # === 4. Fill Metadata (Course, Dist, Weather) ===\n","    print('Checking Metadata (Course, Distance, Weather)...')\n","    meta_cols = ['„Ç≥„Éº„Çπ„Çø„Ç§„Éó', 'Ë∑ùÈõ¢', 'Â§©ÂÄô', 'È¶¨Â†¥Áä∂ÊÖã']\n","    for c in meta_cols:\n","        if c not in df.columns: df[c] = None\n","\n","    missing_meta_mask = (df['„Ç≥„Éº„Çπ„Çø„Ç§„Éó'].isna()) | (df['„Ç≥„Éº„Çπ„Çø„Ç§„Éó'] == '')\n","    if 'race_id' in df.columns:\n","         rids_missing = df.loc[missing_meta_mask, 'race_id'].astype(str).unique()\n","    else:\n","         rids_missing = []\n","\n","    if len(rids_missing) > 0:\n","        print(f'Found {len(rids_missing)} races with missing metadata. Fetching...')\n","        meta_results = {}\n","\n","        def fetch_meta_entry(rid):\n","            scr = RaceScraper()\n","            return scr.get_race_metadata(rid)\n","\n","        with ThreadPoolExecutor(max_workers=10) as executor:\n","            futures = {executor.submit(fetch_meta_entry, rid): rid for rid in rids_missing}\n","\n","            for future in tqdm(as_completed(futures), total=len(rids_missing), desc=\"Fetching Metadata\"):\n","                try:\n","                    data = future.result()\n","                    if data and data.get('course_type'):\n","                        meta_results[str(data['race_id'])] = data\n","                except: pass\n","\n","        print('Applying metadata...')\n","        c_map = {rid: d['course_type'] for rid, d in meta_results.items()}\n","        d_map = {rid: d['distance'] for rid, d in meta_results.items()}\n","        w_map = {rid: d['weather'] for rid, d in meta_results.items()}\n","        cond_map = {rid: d['condition'] for rid, d in meta_results.items()}\n","\n","        df['race_id_str'] = df['race_id'].astype(str)\n","        mask = df['race_id_str'].isin(meta_results.keys())\n","\n","        df.loc[mask, '„Ç≥„Éº„Çπ„Çø„Ç§„Éó'] = df.loc[mask, 'race_id_str'].map(c_map).fillna(df.loc[mask, '„Ç≥„Éº„Çπ„Çø„Ç§„Éó'])\n","        df.loc[mask, 'Ë∑ùÈõ¢'] = df.loc[mask, 'race_id_str'].map(d_map).fillna(df.loc[mask, 'Ë∑ùÈõ¢'])\n","        df.loc[mask, 'Â§©ÂÄô'] = df.loc[mask, 'race_id_str'].map(w_map).fillna(df.loc[mask, 'Â§©ÂÄô'])\n","        df.loc[mask, 'È¶¨Â†¥Áä∂ÊÖã'] = df.loc[mask, 'race_id_str'].map(cond_map).fillna(df.loc[mask, 'È¶¨Â†¥Áä∂ÊÖã'])\n","\n","        df.drop(columns=['race_id_str'], inplace=True, errors='ignore')\n","\n","        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n","        print('Done filling metadata.')\n","    else:\n","        print('All metadata present.')\n","\n","fill_missing_past_data_notebook()\n"]},{"cell_type":"code","source":["# 5. Ë°ÄÁµ±„Éá„Éº„Çø„ÅÆË£úÂÆå (Robust Journaling Mode) - JRAÁâà\n","# Á¢∫ÂÆüÊÄß„ÇíÊúÄÂÑ™ÂÖà„Åó„ÄÅ1È†≠„Åö„Å§„Ç≠„É£„ÉÉ„Ç∑„É•„Éï„Ç°„Ç§„É´„Å´Ë®òÈå≤„Åó„Å™„Åå„ÇâÈÄ≤„ÇÅ„Åæ„Åô„ÄÇ\n","# ÈÄî‰∏≠„ÅßÊ≠¢„Åæ„Å£„Å¶„ÇÇ„ÄÅÂÜçÂÆüË°åÊôÇ„Å´Á∂ö„Åç„Åã„ÇâÂÜçÈñã„Åß„Åç„Åæ„Åô„ÄÇ\n","\n","def fill_jra_bloodline_robust():\n","    from tqdm.auto import tqdm\n","    import os\n","    import csv\n","    import time\n","\n","    csv_path = os.path.join(PROJECT_PATH, 'data', 'raw', 'database.csv')\n","    cache_path = os.path.join(PROJECT_PATH, 'data', 'jra_pedigree_cache.csv')\n","\n","    if not os.path.exists(os.path.dirname(cache_path)):\n","        os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n","\n","    # 1. Load Main Database\n","    print(f'Reading {csv_path}...')\n","    df = pd.read_csv(csv_path, low_memory=False, dtype={'race_id': str, 'horse_id': str})\n","\n","    if 'horse_id' in df.columns:\n","        df['horse_id'] = df['horse_id'].astype(str).str.replace(r'\\.0$', '', regex=True)\n","\n","    # 2. Load/Create Cache\n","    # cache format: horse_id,father,mother,bms\n","    cached_ids = set()\n","    if os.path.exists(cache_path):\n","        try:\n","            cache_df = pd.read_csv(cache_path, dtype=str)\n","            cached_ids = set(cache_df['horse_id'].unique())\n","            print(f\"Loaded {len(cached_ids)} cached records from {os.path.basename(cache_path)}\")\n","        except:\n","            print(\"Cache file corrupted or empty. Starting fresh.\")\n","    else:\n","        # Initialize cache file with header\n","        with open(cache_path, 'w', encoding='utf-8') as f:\n","            f.write('horse_id,father,mother,bms\\n')\n","\n","    # 3. Identify Missing Horses\n","    # Check if columns exist\n","    for col in ['father', 'mother', 'bms']:\n","        if col not in df.columns: df[col] = None\n","\n","    # Missing in DB AND Not in Cache\n","    mask_missing_db = (df['father'].isna()) | (df['father'] == '') | (df['father'] == 'nan')\n","    missing_ids = df.loc[mask_missing_db, 'horse_id'].dropna().unique()\n","    missing_ids = [h for h in missing_ids if h.isdigit()]\n","\n","    # Filter out already cached\n","    target_horses = [h for h in missing_ids if h not in cached_ids]\n","\n","    print(f\"Total Missing in DB: {len(missing_ids)}\")\n","    print(f\"Already Cached: {len(cached_ids)}\")\n","    print(f\"Target to Scrape: {len(target_horses)}\")\n","\n","    if len(target_horses) > 0:\n","        from scraper.race_scraper import RaceScraper\n","        scraper = RaceScraper()\n","\n","        print(\"Starting Row-by-Row Scraping...\")\n","\n","        # Append mode for safety\n","        with open(cache_path, 'a', encoding='utf-8', newline='') as f:\n","            writer = csv.writer(f)\n","\n","            # Disable threads for maximum safety/stability if requested, or use simple loop\n","            # User asked for 'Try before implement' and 'One by one'.\n","            # Single thread loop is safest for 'One by one writing'.\n","\n","            for i, hid in enumerate(tqdm(target_horses)):\n","                try:\n","                    profile = scraper.get_horse_profile(hid)\n","                    if profile:\n","                        father = profile.get('father', '')\n","                        mother = profile.get('mother', '')\n","                        bms = profile.get('bms', '')\n","\n","                        writer.writerow([hid, father, mother, bms])\n","                        f.flush() # Ensure write to disk immediately\n","                except Exception as e:\n","                    print(f\"Error scraping {hid}: {e}\")\n","                    # Continue to next\n","\n","    else:\n","        print(\"No new horses to scrape.\")\n","\n","    # 4. Merge Cache into Database\n","    print(\"Merging cache into database...\")\n","    if os.path.exists(cache_path):\n","        cache_df = pd.read_csv(cache_path, dtype=str)\n","\n","        # Convert to dict for mapping\n","        f_map = dict(zip(cache_df['horse_id'], cache_df['father']))\n","        m_map = dict(zip(cache_df['horse_id'], cache_df['mother']))\n","        b_map = dict(zip(cache_df['horse_id'], cache_df['bms']))\n","\n","        # Apply map\n","        mask = df['horse_id'].isin(f_map.keys())\n","        df.loc[mask, 'father'] = df.loc[mask, 'horse_id'].map(f_map).fillna(df.loc[mask, 'father'])\n","        df.loc[mask, 'mother'] = df.loc[mask, 'horse_id'].map(m_map).fillna(df.loc[mask, 'mother'])\n","        df.loc[mask, 'bms'] = df.loc[mask, 'horse_id'].map(b_map).fillna(df.loc[mask, 'bms'])\n","\n","        # Save DB\n","        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n","        print(\"‚úÖ Database updated successfully.\")\n","\n","fill_jra_bloodline_robust()"],"metadata":{"id":"feature_eng"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ahvlvc--DE9A"},"source":["# 5. Ë°ÄÁµ±„Éá„Éº„Çø„ÅÆË£úÂÆå (Bloodline Backfill)\n","\n","ÂèñÂæóÊºè„Çå„ÅÆË°ÄÁµ±„Éá„Éº„ÇøÔºàÁà∂„ÄÅÊØç„ÄÅÊØçÁà∂Ôºâ„Çí„Éê„ÉÉ„ÇØ„Ç∞„É©„Ç¶„É≥„Éâ„ÅßË£úÂÆå„Åó„Åæ„Åô„ÄÇ\n","Êó¢Â≠ò„ÅÆ„Éá„Éº„Çø„ÇíÁ¢∫Ë™ç„Åó„ÄÅÊ¨†Êêç„Åå„ÅÇ„ÇãÈ¶¨„ÅÆ„ÅøÂØæË±°„Å®„Åó„Åæ„Åô„ÄÇ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wlKGFIRVDE9A"},"outputs":[],"source":["# Ë°ÄÁµ±„Éá„Éº„Çø„ÅÆ„Éê„ÉÉ„ÇØ„Éï„Ç£„É´ÂÆüË°å\n","from scraper.auto_scraper import fill_bloodline_data\n","\n","print(\"Starting Bloodline Backfill process...\")\n","fill_bloodline_data(mode=\"JRA\")\n","print(\"Bloodline Backfill completed.\")"]}]}