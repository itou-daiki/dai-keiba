{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# JRA Scraping Notebook\n",
    "\n",
    "‰∏≠Â§ÆÁ´∂È¶¨ÔºàJRAÔºâ„ÅÆ„Éá„Éº„Çø„Çí„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„Åó„ÄÅGoogle Drive‰∏ä„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´ËøΩÂä†„Åó„Åæ„Åô„ÄÇ"
   ],
   "metadata": {
    "id": "intro_md"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. Google Drive„ÅÆ„Éû„Ç¶„É≥„Éà\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# ‚òÖ‚òÖ‚òÖ Ë®≠ÂÆöÈ†ÖÁõÆ ‚òÖ‚òÖ‚òÖ\n",
    "# scraper„Éï„Ç©„É´„ÉÄ„ÅåÂ≠òÂú®„Åô„Çã„Éë„Çπ (Google Drive‰∏ä„ÅÆ„Éë„Çπ)\n",
    "# ‰æã: '/content/drive/MyDrive/dai-keiba'\n",
    "PROJECT_PATH = '/content/drive/MyDrive/dai-keiba'\n",
    "\n",
    "if not os.path.exists(PROJECT_PATH):\n",
    "    print(f\"Error: Path {PROJECT_PATH} does not exist. Please check your Drive structure.\")\n",
    "else:\n",
    "    print(f\"Project path found: {PROJECT_PATH}\")\n",
    "    os.chdir(PROJECT_PATH)\n",
    "    sys.path.append(PROJECT_PATH)\n"
   ],
   "metadata": {
    "id": "mount_drive"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 2. ÂøÖË¶Å„Å™„É©„Ç§„Éñ„É©„É™„ÅÆ„Ç§„É≥„Éù„Éº„Éà\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    import bs4\n",
    "except ImportError:\n",
    "    !pip install pandas requests beautifulsoup4\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    import bs4\n",
    "\n",
    "from datetime import datetime, date\n",
    "from scraper.jra_scraper import scrape_jra_year, JRA_MONTH_PARAMS\n",
    "import time\n",
    "\n",
    "# ==========================================\n",
    "# üö® 2023Âπ¥/2022Âπ¥„Éë„É©„É°„Éº„Çø‰∏çË∂≥„ÅÆ‰øÆÊ≠£„Éë„ÉÉ„ÉÅ\n",
    "# ==========================================\n",
    "# „Éë„É©„É°„Éº„Çø„Éû„ÉÉ„Éó„Çí‰∏äÊõ∏„ÅçÊõ¥Êñ∞\n",
    "try:\n",
    "    JRA_MONTH_PARAMS.update({\n",
    "        \"2023\": { \"01\": \"27\", \"02\": \"F5\", \"03\": \"C3\", \"04\": \"91\", \"05\": \"5F\", \"06\": \"2D\", \"07\": \"FB\", \"08\": \"C9\", \"09\": \"97\", \"10\": \"06\", \"11\": \"D4\", \"12\": \"A2\" },\n",
    "        \"2022\": { \"01\": \"9B\", \"02\": \"69\", \"03\": \"37\", \"04\": \"05\", \"05\": \"D3\", \"06\": \"A1\", \"07\": \"6F\", \"08\": \"3D\", \"09\": \"0B\", \"10\": \"7A\", \"11\": \"48\", \"12\": \"16\" }\n",
    "    })\n",
    "    print(\"‚úÖ JRA 2023/2022 Parameters have been patched successfully.\")\n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è JRA_MONTH_PARAMS not found in scope. Ensure imports are correct.\")\n"
   ],
   "metadata": {
    "id": "imports"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 „Çπ„ÇØ„É¨„Ç§„Éë„Éº„ÅÆ‰øÆÊ≠£„Éë„ÉÉ„ÉÅ („Ç≥„Éº„ÇπË©≥Á¥∞„ÉªÂ§©ÂÄôÂèñÂæó)\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import scraper.jra_scraper\n",
    "\n",
    "def patched_scrape_jra_race(url, existing_race_ids=None):\n",
    "    print(f'Accessing JRA URL (Patched): {url}...')\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.encoding = 'cp932'\n",
    "        if response.status_code != 200: return None\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # (Skipping full redundant parse code for brevity in this patch, assuming main logic matches jra_scraper.py)\n",
    "        # Ideally we should just RELOAD the module if we updated the file in Drive.\n",
    "        # If the user updates the file in Drive (scraper/jra_scraper.py), they just need to reload.\n",
    "        # But to be safe, we can force reload.\n",
    "        pass\n",
    "    except: pass\n",
    "    return scraper.jra_scraper.scrape_jra_race(url, existing_race_ids)\n",
    "\n",
    "import importlib\n",
    "importlib.reload(scraper.jra_scraper)\n",
    "from scraper.jra_scraper import scrape_jra_race, scrape_jra_year, JRA_MONTH_PARAMS\n",
    "print('‚úÖ Loaded updated scraper/jra_scraper.py from Drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 3. „Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÂÆüË°åÈñ¢Êï∞„ÅÆÂÆöÁæ©\n",
    "\n",
    "def jra_scrape_execution(year_str, start_date=None, end_date=None):\n",
    "    CSV_FILE_PATH = os.path.join(PROJECT_PATH, \"database.csv\")\n",
    "    print(f\"Using CSV Path: {CSV_FILE_PATH}\")\n",
    "\n",
    "    def save_chunk(df_chunk):\n",
    "        if os.path.exists(CSV_FILE_PATH):\n",
    "            try:\n",
    "                # Read types as string to prevent auto-float for IDs\n",
    "                existing_df = pd.read_csv(CSV_FILE_PATH, dtype={'race_id': str, 'horse_id': str})\n",
    "                combined_df = pd.concat([existing_df, df_chunk], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Read Error: {e}, creating new.\")\n",
    "                combined_df = df_chunk\n",
    "        else:\n",
    "            combined_df = df_chunk\n",
    "\n",
    "        # Deduplicate\n",
    "        subset_cols = ['race_id', 'È¶¨Âêç']\n",
    "        subset_cols = [c for c in subset_cols if c in combined_df.columns]\n",
    "        if subset_cols:\n",
    "            combined_df.drop_duplicates(subset=subset_cols, keep='last', inplace=True)\n",
    "\n",
    "        combined_df.to_csv(CSV_FILE_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"  [Saved] Total rows: {len(combined_df)} (+{len(df_chunk)} new)\")\n",
    "\n",
    "    print(f\"Starting Scraping for {year_str} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    # Load existing IDs to skip\n",
    "    existing_ids = set()\n",
    "    if os.path.exists(CSV_FILE_PATH):\n",
    "        try:\n",
    "             df_e = pd.read_csv(CSV_FILE_PATH, usecols=['race_id'], dtype={'race_id': str})\n",
    "             existing_ids = set(df_e['race_id'].astype(str))\n",
    "             print(f\"  Loaded {len(existing_ids)} existing race IDs to skip.\")\n",
    "        except:\n",
    "             pass\n",
    "\n",
    "    scrape_jra_year(year_str, start_date=start_date, end_date=end_date, save_callback=save_chunk, existing_race_ids=existing_ids)\n"
   ],
   "metadata": {
    "id": "def_exec"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 4. ÂÆüË°å„Éë„É©„É°„Éº„Çø„ÅÆË®≠ÂÆö„Å®ÈñãÂßã\n",
    "# -----------------------------\n",
    "TARGET_YEAR = \"2024\"\n",
    "TARGET_MONTH = 1  # ‚òÖ‰ΩïÊúà„ÇíÂèñÂæó„Åô„Çã„ÅãÊåáÂÆö (None„ÅÆÂ†¥Âêà„ÅØÂÖ®ÊúüÈñì„ÄÅ1„Äú12„ÇíÊåáÂÆö)\n",
    "\n",
    "import calendar\n",
    "from datetime import date\n",
    "\n",
    "START_DATE = None\n",
    "END_DATE = None\n",
    "\n",
    "if TARGET_MONTH:\n",
    "    # ÊåáÂÆö„Åó„ÅüÊúà„ÅÆ1Êó•„ÄúÊú´Êó•„ÇíË®≠ÂÆö\n",
    "    _, last_day = calendar.monthrange(int(TARGET_YEAR), int(TARGET_MONTH))\n",
    "    START_DATE = date(int(TARGET_YEAR), int(TARGET_MONTH), 1)\n",
    "    END_DATE = date(int(TARGET_YEAR), int(TARGET_MONTH), last_day)\n",
    "    print(f\"Targeting specific month: {START_DATE} to {END_DATE}\")\n",
    "else:\n",
    "    # Ëá™ÂãïÂà§ÂÆö„É≠„Ç∏„ÉÉ„ÇØ (Êó¢Â≠ò„Éá„Éº„Çø„ÅÆÁøåÊó•„Åã„Çâ)\n",
    "    CSV_FILE_PATH = os.path.join(PROJECT_PATH, \"database.csv\")\n",
    "    if os.path.exists(CSV_FILE_PATH):\n",
    "        try:\n",
    "             df_exist = pd.read_csv(CSV_FILE_PATH)\n",
    "             if 'Êó•‰ªò' in df_exist.columns and not df_exist.empty:\n",
    "                 df_exist['date_obj'] = pd.to_datetime(df_exist['Êó•‰ªò'], format='%YÂπ¥%mÊúà%dÊó•', errors='coerce')\n",
    "                 last_date = df_exist['date_obj'].max()\n",
    "                 if pd.notna(last_date):\n",
    "                     # START_DATE = last_date.date() # Êóß: Á∂ö„Åç„Åã„Çâ\n",
    "                     # Êñ∞: Ê¨†ËêΩË£úÂÆå„ÅÆ„Åü„ÇÅ„Å´„ÄÅÂº∑Âà∂ÁöÑ„Å´„Åù„ÅÆÂπ¥„ÅÆ1Êúà1Êó•„Åã„Çâ„Çπ„Ç≠„É£„É≥„Åô„Çã (existing_ids„Åß„Çπ„Ç≠„ÉÉ„Éó„Åï„Çå„Çã)\n",
    "                     START_DATE = date(int(TARGET_YEAR), 1, 1)\n",
    "                     print(f\"Êó¢Â≠ò„Éá„Éº„Çø„ÅÆÊúÄÁµÇÊó•ÊôÇ: {last_date.date()} (Ê¨†ËêΩÁ¢∫Ë™ç„ÅÆ„Åü„ÇÅ {START_DATE} „Åã„Çâ„Çπ„Ç≠„É£„É≥„Åó„Åæ„Åô)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Êó¢Â≠ò„Éá„Éº„ÇøÁ¢∫Ë™ç„Ç®„É©„Éº: {e}\")\n",
    "\n",
    "print(f\"Scraping Target: {TARGET_YEAR}, Start: {START_DATE}, End: {END_DATE}\")\n",
    "jra_scrape_execution(TARGET_YEAR, start_date=START_DATE, end_date=END_DATE)\n"
   ],
   "metadata": {
    "id": "run_cell"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 „Éá„Éº„Çø„Éô„Éº„Çπ„Ç´„É©„É†Âêç„ÅÆ‰øÆÊ≠£ (ÈáçË§áÊîπÂñÑ)\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def fix_database_columns_notebook():\n",
    "    # Google Drive„Éë„Çπ„Çí‰ΩøÁî®\n",
    "    target_file = os.path.join(PROJECT_PATH, 'database.csv')\n",
    "    \n",
    "    if not os.path.exists(target_file):\n",
    "        print('Database file not found.')\n",
    "        return\n",
    "\n",
    "    print(f'Checking columns in {target_file}...')\n",
    "    try:\n",
    "        df = pd.read_csv(target_file, low_memory=False)\n",
    "        \n",
    "        # Ë°®Ë®ò„ÇÜ„ÇåËæûÊõ∏\n",
    "        columns_to_fix = {\n",
    "            '„Ç≥„Éº„Éä„ÉºÈÄöÈÅéÈ†Ü': '„Ç≥„Éº„Éä„Éº ÈÄöÈÅéÈ†Ü',\n",
    "            'È¶¨‰ΩìÈáç(Â¢óÊ∏õ)': 'È¶¨‰ΩìÈáç (Â¢óÊ∏õ)'\n",
    "        }\n",
    "        \n",
    "        changed = False\n",
    "        for bad_col, good_col in columns_to_fix.items():\n",
    "            if bad_col in df.columns:\n",
    "                print(f'Merging {bad_col} -> {good_col}...')\n",
    "                if good_col not in df.columns:\n",
    "                    df[good_col] = df[bad_col]\n",
    "                else:\n",
    "                    df[good_col] = df[good_col].fillna(df[bad_col])\n",
    "                \n",
    "                df.drop(columns=[bad_col], inplace=True)\n",
    "                changed = True\n",
    "        \n",
    "        if changed:\n",
    "            df.to_csv(target_file, index=False, encoding='utf-8-sig')\n",
    "            print('‚úÖ Database columns fixed and saved.')\n",
    "        else:\n",
    "            print('No column fixes needed.')\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'Error fixing columns: {e}')\n",
    "\n",
    "fix_database_columns_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Ê¨†Êêç„Éá„Éº„Çø„ÅÆË£úÂÆå (HorseID & ÈÅéÂéªËµ∞)\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "\n",
    "# Ensure scraper path\n",
    "sys.path.append(os.path.join(PROJECT_PATH, 'scraper'))\n",
    "from scraper.race_scraper import RaceScraper\n",
    "\n",
    "# Global lock for dataframe update\n",
    "df_lock = threading.Lock()\n",
    "\n",
    "def fetch_race_horse_ids(rid):\n",
    "    scraper = RaceScraper()\n",
    "    try:\n",
    "        url = f'https://race.netkeiba.com/race/result.html?race_id={rid}'\n",
    "        soup = scraper._get_soup(url)\n",
    "        if not soup: return None\n",
    "        \n",
    "        table = soup.find('table', id='All_Result_Table')\n",
    "        if not table: return None\n",
    "        \n",
    "        horse_map = {}\n",
    "        rows = table.find_all('tr', class_='HorseList')\n",
    "        for row in rows:\n",
    "            name_tag = row.select_one('.Horse_Name a')\n",
    "            if name_tag:\n",
    "                h_name = name_tag.text.strip()\n",
    "                href = name_tag.get('href', '')\n",
    "                match = re.search(r'/horse/(\\d+)', href)\n",
    "                if match:\n",
    "                    horse_map[h_name] = match.group(1)\n",
    "        return (rid, horse_map)\n",
    "    except Exception as e:\n",
    "        print(f'Error fetching race {rid}: {e}')\n",
    "        return None\n",
    "\n",
    "def fetch_horse_history(horse_id):\n",
    "    scraper = RaceScraper()\n",
    "    try:\n",
    "        df = scraper.get_past_races(str(horse_id), n_samples=None)\n",
    "        return (horse_id, df)\n",
    "    except Exception as e:\n",
    "        print(f'Error fetching horse {horse_id}: {e}')\n",
    "        return (horse_id, pd.DataFrame())\n",
    "\n",
    "def fill_missing_past_data_notebook():\n",
    "    csv_path = os.path.join(PROJECT_PATH, 'database.csv')\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f'Error: {csv_path} not found.')\n",
    "        return\n",
    "\n",
    "    print(f'Reading {csv_path}...')\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if 'Êó•‰ªò' in df.columns:\n",
    "        df['date_dt'] = pd.to_datetime(df['Êó•‰ªò'], format='%YÂπ¥%mÊúà%dÊó•', errors='coerce')\n",
    "    else:\n",
    "        print('Error: Êó•‰ªò column not found.')\n",
    "        return\n",
    "\n",
    "    if 'horse_id' not in df.columns:\n",
    "        df['horse_id'] = None\n",
    "\n",
    "    # 1. Fill Missing Horse IDs\n",
    "    if 'race_id' in df.columns:\n",
    "        missing_mask = df['horse_id'].isna() | (df['horse_id'] == '')\n",
    "        if missing_mask.any():\n",
    "            races_to_update = df.loc[missing_mask, 'race_id'].unique()\n",
    "            print(f'Need to fetch IDs for {len(races_to_update)} races...')\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "                futures = {executor.submit(fetch_race_horse_ids, rid): rid for rid in races_to_update}\n",
    "                completed = 0\n",
    "                for future in as_completed(futures):\n",
    "                    completed += 1\n",
    "                    if completed % 10 == 0: print(f'  [IDs] {completed}/{len(races_to_update)}')\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        rid, horse_map = result\n",
    "                        if horse_map:\n",
    "                            indices = df[df['race_id'] == rid].index\n",
    "                            for idx in indices:\n",
    "                                h_name = df.at[idx, 'È¶¨Âêç']\n",
    "                                if h_name in horse_map:\n",
    "                                    df.at[idx, 'horse_id'] = horse_map[h_name]\n",
    "            \n",
    "            df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "            print('Saved updated IDs.')\n",
    "        else:\n",
    "            print('All Horse IDs present.')\n",
    "\n",
    "    # 2. Fill Past History\n",
    "    fields_map = {\n",
    "        'date': 'date', 'rank': 'rank', 'time': 'time', 'run_style': 'run_style',\n",
    "        'race_name': 'race_name', 'last_3f': 'last_3f', 'horse_weight': 'horse_weight',\n",
    "        'jockey': 'jockey', 'condition': 'condition', 'odds': 'odds',\n",
    "        'weather': 'weather', 'distance': 'distance', 'course_type': 'course_type'\n",
    "    }\n",
    "\n",
    "    unique_horses = df['horse_id'].dropna().unique()\n",
    "    # Simplification: Only fetch for horses missing past data to save time?\n",
    "    # For now, let's keep full logic but maybe we should check if columns exist\n",
    "    # If columns don't exist, create them\n",
    "    for k in fields_map.keys():\n",
    "        for i in range(1, 6):\n",
    "            col = f'past_{i}_{k}'\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "\n",
    "    print(f'Found {len(unique_horses)} unique horses. Fetching history...')\n",
    "    history_store = {}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = {executor.submit(fetch_horse_history, hid): hid for hid in unique_horses}\n",
    "        completed = 0\n",
    "        for future in as_completed(futures):\n",
    "            completed += 1\n",
    "            if completed % 50 == 0: print(f'  [History] {completed}/{len(unique_horses)}')\n",
    "            hid, hist_df = future.result()\n",
    "            history_store[hid] = hist_df\n",
    "\n",
    "    print('Applying history data...')\n",
    "    for idx, row in df.iterrows():\n",
    "        hid = row.get('horse_id')\n",
    "        current_date = row.get('date_dt')\n",
    "        if pd.notna(hid) and hid in history_store:\n",
    "            hist_df = history_store[hid]\n",
    "            if hist_df.empty: continue\n",
    "            \n",
    "            if 'date' in hist_df.columns:\n",
    "                # Fix Date format in hist_df if needed (YYYY/MM/DD)\n",
    "                hist_df['date_obj'] = pd.to_datetime(hist_df['date'], errors='coerce')\n",
    "            \n",
    "            if 'date_obj' not in hist_df.columns: continue\n",
    "            if pd.isna(current_date): continue\n",
    "            \n",
    "            past_races = hist_df[hist_df['date_obj'] < current_date].sort_values('date_obj', ascending=False).head(5)\n",
    "            \n",
    "            for i, (p_idx, p_row) in enumerate(past_races.iterrows()):\n",
    "                n = i + 1\n",
    "                if n > 5: break\n",
    "                for k, v in fields_map.items():\n",
    "                    df.at[idx, f'past_{n}_{k}'] = p_row.get(v)\n",
    "\n",
    "    if 'date_dt' in df.columns: df.drop(columns=['date_dt'], inplace=True)\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    print('Done filling past data.')\n",
    "\n",
    "fill_missing_past_data_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 5. „Éá„Éº„ÇøÂä†Â∑• (Feature Engineering) „ÅÆÂÆüË°å\n",
    "# ----------------------------------------\n",
    "# „Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„Åó„Åü database.csv „Åã„ÇâÂ≠¶ÁøíÁî®„Éá„Éº„Çø„ÇíÁîüÊàê„Åó„Åæ„Åô\n",
    "from ml.feature_engineering import calculate_features\n",
    "\n",
    "INPUT_CSV = os.path.join(PROJECT_PATH, \"database.csv\")\n",
    "OUTPUT_CSV = os.path.join(PROJECT_PATH, \"processed_data.csv\")\n",
    "\n",
    "if os.path.exists(INPUT_CSV):\n",
    "    print(\"Starting Feature Engineering...\")\n",
    "    calculate_features(INPUT_CSV, OUTPUT_CSV)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"Error: database.csv not found.\")\n"
   ],
   "metadata": {
    "id": "feature_eng"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}