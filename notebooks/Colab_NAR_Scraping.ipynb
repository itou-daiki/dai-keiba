{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ‡ NAR å…¨ãƒ¬ãƒ¼ã‚¹å–å¾—\n",
    "ä»¥ä¸‹ã®è¨­å®šå¤‰æ•°ã‚’å¤‰æ›´ã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚NARï¼ˆåœ°æ–¹ç«¶é¦¬ï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ—¥ä»˜é †ã«å–å¾—ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆã™ã‚‹å ´åˆã®ã¿å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class RaceScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "    def _get_soup(self, url):\n",
    "        try:\n",
    "            time.sleep(1) # Be polite\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.encoding = response.apparent_encoding\n",
    "            if response.status_code == 200:\n",
    "                return BeautifulSoup(response.text, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def get_past_races(self, horse_id, n_samples=5):\n",
    "        \"\"\"\n",
    "        Fetches past n_samples race results for a given horse_id from netkeiba db.\n",
    "        Returns a DataFrame of past races.\n",
    "        \"\"\"\n",
    "        url = f\"https://db.netkeiba.com/horse/result/{horse_id}/\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # The results are usually in a table with class \"db_h_race_results\"\n",
    "        table = soup.select_one(\"table.db_h_race_results\")\n",
    "        if not table:\n",
    "            # Try to find any table with \"ç€é †\"\n",
    "            tables = soup.find_all(\"table\")\n",
    "            for t in tables:\n",
    "                if \"ç€é †\" in t.text:\n",
    "                    table = t\n",
    "                    break\n",
    "        \n",
    "        if not table:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Parse table\n",
    "        # We need to manually parse to get clean data and handle links if needed (though for past data, text is mostly fine)\n",
    "        # pd.read_html is easier for the table\n",
    "        try:\n",
    "            df = pd.read_html(io.StringIO(str(table)))[0]\n",
    "            \n",
    "            # Basic cleaning\n",
    "            df = df.dropna(how='all')\n",
    "            \n",
    "            # The columns in db.netkeiba are roughly:\n",
    "            # æ—¥ä»˜, é–‹å‚¬, å¤©æ°—, R, ãƒ¬ãƒ¼ã‚¹å, æ˜ åƒ, é ­æ•°, æ ç•ª, ... ç€é †, ... é€šé, ...\n",
    "            \n",
    "            # We want to keep: Date, Race Name, Course info, Rank, Time, Passing (Style)\n",
    "            \n",
    "            # Normalize column names (remove spaces/newlines)\n",
    "            df.columns = df.columns.astype(str).str.replace(r'\\s+', '', regex=True)\n",
    "\n",
    "            # Filter rows that look like actual races (Date column exists)\n",
    "            if 'æ—¥ä»˜' in df.columns:\n",
    "                df['date_obj'] = pd.to_datetime(df['æ—¥ä»˜'], format='%Y/%m/%d', errors='coerce')\n",
    "                df = df.dropna(subset=['date_obj'])\n",
    "                df = df.sort_values('date_obj', ascending=False)\n",
    "                \n",
    "            # Take top N\n",
    "            if n_samples:\n",
    "                df = df.head(n_samples)\n",
    "            \n",
    "            # Process Run Style (Leg Type)\n",
    "            if 'é€šé' in df.columns:\n",
    "                df['run_style_val'] = df['é€šé'].apply(self.extract_run_style)\n",
    "            else:\n",
    "                df['run_style_val'] = 3 # Unknown\n",
    "\n",
    "            # Extract/Rename Columns\n",
    "            # We want: æ—¥ä»˜, é–‹å‚¬, å¤©æ°—, R, ãƒ¬ãƒ¼ã‚¹å, æ˜ åƒ, é ­æ•°, æ ç•ª, ... ç€é †, ... é€šé, ...\n",
    "            # Important: 'ä¸Šã‚Š' (3F), 'é¦¬ä½“é‡', 'é¨æ‰‹'\n",
    "            \n",
    "            # Map standard columns if they exist\n",
    "            column_map = {\n",
    "                'æ—¥ä»˜': 'date',\n",
    "                'é–‹å‚¬': 'venue',\n",
    "                'å¤©æ°—': 'weather',\n",
    "                'ãƒ¬ãƒ¼ã‚¹å': 'race_name',\n",
    "                'ç€é †': 'rank',\n",
    "                'æ ç•ª': 'waku',\n",
    "                'é¦¬ç•ª': 'umaban',\n",
    "                'é¨æ‰‹': 'jockey',\n",
    "                'æ–¤é‡': 'weight_carried',\n",
    "                'é¦¬å ´': 'condition', # è‰¯/é‡/ç¨é‡ etc.\n",
    "                'ã‚¿ã‚¤ãƒ ': 'time',\n",
    "                'ç€å·®': 'margin',\n",
    "                'ä¸Šã‚Š': 'last_3f',\n",
    "                'é€šé': 'passing',\n",
    "                'é¦¬ä½“é‡': 'horse_weight',\n",
    "                'run_style_val': 'run_style',\n",
    "                'å˜å‹': 'odds',\n",
    "                'ã‚ªãƒƒã‚º': 'odds',\n",
    "                'è·é›¢': 'raw_distance' # e.g. \"èŠ1600\"\n",
    "            }\n",
    "            \n",
    "            # Rename available columns\n",
    "            df.rename(columns=column_map, inplace=True)\n",
    "            \n",
    "            # Extract Surface and Distance from 'raw_distance'\n",
    "            if 'raw_distance' in df.columns:\n",
    "                def parse_dist(x):\n",
    "                    if not isinstance(x, str): return None, None\n",
    "                    # \"èŠ1600\", \"ãƒ€1200\", \"éšœ3000\"\n",
    "                    # Sometimes \"èŠ1600\" or just \"1600\"\n",
    "                    surf = None\n",
    "                    dist = None\n",
    "                    if 'èŠ' in x: surf = 'èŠ'\n",
    "                    elif 'ãƒ€' in x: surf = 'ãƒ€'\n",
    "                    elif 'éšœ' in x: surf = 'éšœ'\n",
    "                    \n",
    "                    # Extract number\n",
    "                    match = re.search(r'(\\d+)', x)\n",
    "                    if match:\n",
    "                        dist = int(match.group(1))\n",
    "                    return surf, dist\n",
    "\n",
    "                parsed = df['raw_distance'].apply(parse_dist)\n",
    "                df['course_type'] = parsed.apply(lambda x: x[0])\n",
    "                df['distance'] = parsed.apply(lambda x: x[1])\n",
    "            else:\n",
    "                df['course_type'] = None\n",
    "                df['distance'] = None\n",
    "\n",
    "            # Coerce numeric\n",
    "            if 'rank' in df.columns:\n",
    "                df['rank'] = pd.to_numeric(df['rank'], errors='coerce')\n",
    "            \n",
    "            if 'odds' in df.columns:\n",
    "                 df['odds'] = pd.to_numeric(df['odds'], errors='coerce')\n",
    "            \n",
    "            # Fill missing\n",
    "            for target_col in list(column_map.values()) + ['course_type', 'distance']:\n",
    "                if target_col not in df.columns:\n",
    "                    df[target_col] = None\n",
    "                \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing past races for {horse_id}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def extract_run_style(self, passing_str):\n",
    "        \"\"\"\n",
    "        Converts passing order string (e.g., \"1-1-1\", \"10-10-12\") to run style (1,2,3,4).\n",
    "        1: Nige (Escape) - Lead at 1st corner\n",
    "        2: Senkou (Leader) - Within first ~4 or so\n",
    "        3: Sashi (Mid) - Midpack\n",
    "        4: Oikomi (Chaser) - Back\n",
    "        Returns integer code.\n",
    "        \"\"\"\n",
    "        if not isinstance(passing_str, str):\n",
    "            return 3 # Default to Mid\n",
    "            \n",
    "        # Clean string \"1-1-1\" -> [1, 1, 1]\n",
    "        try:\n",
    "            cleaned = re.sub(r'[^0-9-]', '', passing_str)\n",
    "            parts = [int(p) for p in cleaned.split('-') if p]\n",
    "            \n",
    "            if not parts:\n",
    "                return 3\n",
    "                \n",
    "            first_corner = parts[0]\n",
    "            \n",
    "            # Heuristics\n",
    "            if first_corner == 1:\n",
    "                return 1 # Nige\n",
    "            elif first_corner <= 4:\n",
    "                return 2 # Senkou\n",
    "            elif first_corner <= 9: # Assuming standard field size of 10-16, 9 is mid-ish limit? \n",
    "                # Actually \"Sashi\" is usually mid-rear. \n",
    "                # Let's say: 1=Lead, 2-4=Front, 5-10=Mid, >10=Back\n",
    "                return 3 # Sashi\n",
    "            else:\n",
    "                return 4 # Oikomi\n",
    "                \n",
    "        except:\n",
    "            return 3\n",
    "\n",
    "    def scrape_race_with_history(self, race_id):\n",
    "        \"\"\"\n",
    "        Detailed scraper that enters a race_result page, finding horse IDs, \n",
    "        then fetches history for each horse.\n",
    "        Returns a dictionary or structured object with the race result + history.\n",
    "        \"\"\"\n",
    "        url = f\"https://race.netkeiba.com/race/result.html?race_id={race_id}\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "            \n",
    "        # 1. Parse Main Result Table to get Horse IDs and basic result\n",
    "        # Note: auto_scraper already does some of this, but we need Horse IDs specifically.\n",
    "        # \"All_Result_Table\"\n",
    "        \n",
    "        result_data = []\n",
    "        \n",
    "        table = soup.find(\"table\", id=\"All_Result_Table\")\n",
    "        if not table:\n",
    "            return None\n",
    "            \n",
    "        rows = table.find_all(\"tr\", class_=\"HorseList\")\n",
    "        \n",
    "        print(f\"Found {len(rows)} horses in race {race_id}. Fetching histories...\")\n",
    "        \n",
    "        for row in rows:\n",
    "            # Extract basic info\n",
    "            rank_elem = row.select_one(\".Rank\")\n",
    "            rank = rank_elem.text.strip() if rank_elem else \"\"\n",
    "            \n",
    "            horse_name_elem = row.select_one(\".Horse_Name a\")\n",
    "            horse_name = horse_name_elem.text.strip() if horse_name_elem else \"\"\n",
    "            horse_url = horse_name_elem.get(\"href\") if horse_name_elem else \"\"\n",
    "            \n",
    "            # Extract ID from URL\n",
    "            # https://db.netkeiba.com/horse/2018105027\n",
    "            horse_id = None\n",
    "            if horse_url:\n",
    "                match = re.search(r'/horse/(\\d+)', horse_url)\n",
    "                if match:\n",
    "                    horse_id = match.group(1)\n",
    "            \n",
    "            if not horse_id:\n",
    "                print(f\"  Skipping {horse_name} (No ID)\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  Fetching history for {horse_name} ({horse_id})...\")\n",
    "            \n",
    "            # 2. Get Past History\n",
    "            df_past = self.get_past_races(horse_id, n_samples=5)\n",
    "            \n",
    "            # 3. Structure Data\n",
    "            # converting df_past to a list of dicts or flattened fields\n",
    "            history = []\n",
    "            if not df_past.empty:\n",
    "                for idx, r in df_past.iterrows():\n",
    "                    # Extract relevant columns\n",
    "                    # We need at least: Rank, RunStyle, Time(Seconds?), Pace?\n",
    "                    # For now just dump raw-ish data\n",
    "                    hist_item = {\n",
    "                        \"date\": r.get('æ—¥ä»˜'),\n",
    "                        \"race_name\": r.get('ãƒ¬ãƒ¼ã‚¹å'),\n",
    "                        \"rank\": r.get('ç€é †'),\n",
    "                        \"passing\": r.get('é€šé'),\n",
    "                        \"run_style\": r.get('run_style_val'),\n",
    "                        \"time\": r.get('ã‚¿ã‚¤ãƒ '),\n",
    "                        # Add more as needed for Feature Engineering\n",
    "                    }\n",
    "                    history.append(hist_item)\n",
    "            \n",
    "            entry = {\n",
    "                \"race_id\": race_id,\n",
    "                \"horse_id\": horse_id,\n",
    "                \"horse_name\": horse_name,\n",
    "                \"rank\": rank,\n",
    "                \"history\": history\n",
    "            }\n",
    "            result_data.append(entry)\n",
    "            \n",
    "        return result_data\n",
    "\n",
    "    def get_horse_profile(self, horse_id):\n",
    "        \"\"\"\n",
    "        Fetches horse profile to get pedigree (Father, Mother, Grandfather(BMS)).\n",
    "        Returns a dictionary or None.\n",
    "        \"\"\"\n",
    "        # Use pedigree page for reliable bloodline data\n",
    "        url = f\"https://db.netkeiba.com/horse/ped/{horse_id}/\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        # Parse Blood Table\n",
    "        # table class=\"blood_table\"\n",
    "        \n",
    "        data = {\n",
    "            \"father\": \"\",\n",
    "            \"mother\": \"\",\n",
    "            \"bms\": \"\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            table = soup.select_one(\"table.blood_table\")\n",
    "            if table:\n",
    "                rows = table.find_all(\"tr\")\n",
    "                # 5-generation table has 32 rows usually\n",
    "                # Father at Row 0 (rowspan 16)\n",
    "                # Mother at Row 16 (rowspan 16)\n",
    "                \n",
    "                if len(rows) >= 17:\n",
    "                    # Father: Row 0, Col 0\n",
    "                    r0 = rows[0].find_all(\"td\")\n",
    "                    if r0:\n",
    "                        txt = r0[0].text.strip()\n",
    "                        # Clean: \"ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ãƒ’ãƒ¼ãƒ­ãƒ¼\\n2004 æ —æ¯›...\" -> \"ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ãƒ’ãƒ¼ãƒ­ãƒ¼\"\n",
    "                        # Take first line\n",
    "                        data[\"father\"] = txt.split('\\n')[0].strip()\n",
    "                        \n",
    "                    # Mother & BMS: Row 16\n",
    "                    r16 = rows[16].find_all(\"td\")\n",
    "                    if len(r16) >= 2:\n",
    "                        # Mother\n",
    "                        m_txt = r16[0].text.strip()\n",
    "                        data[\"mother\"] = m_txt.split('\\n')[0].strip()\n",
    "                        \n",
    "                        # BMS (Mother's Father)\n",
    "                        bms_txt = r16[1].text.strip()\n",
    "                        data[\"bms\"] = bms_txt.split('\\n')[0].strip()\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing profile for {horse_id}: {e}\")\n",
    "            \n",
    "        return data\n",
    "\n",
    "    def get_race_metadata(self, race_id):\n",
    "        \"\"\"\n",
    "        Fetches metadata for a specific race ID from Netkeiba.\n",
    "        Returns dict with: race_name, date, venue, course_type, distance, weather, condition, turn\n",
    "        \"\"\"\n",
    "        url = f\"https://race.netkeiba.com/race/result.html?race_id={race_id}\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "            \n",
    "        data = {\n",
    "            \"race_name\": \"\",\n",
    "            \"date\": \"\",\n",
    "            \"venue\": \"\",\n",
    "            \"course_type\": \"\",\n",
    "            \"distance\": \"\",\n",
    "            \"weather\": \"\",\n",
    "            \"condition\": \"\",\n",
    "            \"turn\": \"\", # New: Dictionary key for turn direction\n",
    "            \"race_id\": race_id\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Race Name\n",
    "            title_elem = soup.select_one(\".RaceName\")\n",
    "            if title_elem:\n",
    "                data[\"race_name\"] = title_elem.text.strip()\n",
    "                \n",
    "            # Date & Venue & Conditions\n",
    "            # <div class=\"RaceData01\">... 2023å¹´1æœˆ5æ—¥ ... 1å›ä¸­å±±1æ—¥ ...</div>\n",
    "            # Content: \"15:35ç™ºèµ° / èŠ1600m (å³ å¤–) / å¤©å€™:æ™´ / é¦¬å ´:è‰¯\"\n",
    "            \n",
    "            rd1 = soup.select_one(\".RaceData01\")\n",
    "            \n",
    "            if rd1:\n",
    "                txt = rd1.text.strip()\n",
    "                \n",
    "                # Weather\n",
    "                if \"å¤©å€™:æ™´\" in txt: data[\"weather\"] = \"æ™´\"\n",
    "                elif \"å¤©å€™:æ›‡\" in txt: data[\"weather\"] = \"æ›‡\"\n",
    "                elif \"å¤©å€™:å°é›¨\" in txt: data[\"weather\"] = \"å°é›¨\"\n",
    "                elif \"å¤©å€™:é›¨\" in txt: data[\"weather\"] = \"é›¨\"\n",
    "                elif \"å¤©å€™:é›ª\" in txt: data[\"weather\"] = \"é›ª\"\n",
    "                \n",
    "                # Condition\n",
    "                if \"é¦¬å ´:è‰¯\" in txt: data[\"condition\"] = \"è‰¯\"\n",
    "                elif \"é¦¬å ´:ç¨\" in txt: data[\"condition\"] = \"ç¨é‡\" # Covers ç¨é‡\n",
    "                elif \"é¦¬å ´:é‡\" in txt: data[\"condition\"] = \"é‡\"\n",
    "                elif \"é¦¬å ´:ä¸è‰¯\" in txt: data[\"condition\"] = \"ä¸è‰¯\"\n",
    "                \n",
    "                # Course & Distance (\"èŠ1600m\")\n",
    "                # Regex for \"èŠ\", \"ãƒ€\", \"éšœ\" followed by digits\n",
    "                match = re.search(r'(èŠ|ãƒ€|éšœ)(\\d+)m', txt)\n",
    "                if match:\n",
    "                    ctype_raw = match.group(1)\n",
    "                    if ctype_raw == \"èŠ\": data[\"course_type\"] = \"èŠ\"\n",
    "                    elif ctype_raw == \"ãƒ€\": data[\"course_type\"] = \"ãƒ€ãƒ¼ãƒˆ\"\n",
    "                    elif ctype_raw == \"éšœ\": data[\"course_type\"] = \"éšœå®³\"\n",
    "                    \n",
    "                    data[\"distance\"] = match.group(2)\n",
    "                \n",
    "                # Turn Direction (\"å³\", \"å·¦\", \"ç›´ç·š\")\n",
    "                # Usually in parentheses like \"(å³)\" or \"(å·¦)\" or \"(èŠ å·¦)\"\n",
    "                if \"å³\" in txt: data[\"turn\"] = \"å³\"\n",
    "                elif \"å·¦\" in txt: data[\"turn\"] = \"å·¦\"\n",
    "                elif \"ç›´ç·š\" in txt: data[\"turn\"] = \"ç›´\"\n",
    "\n",
    "            # Date\n",
    "            # Try finding date in Title or dedicated element\n",
    "            date_elem = soup.select_one(\"dl#RaceList_DateList dd.Active\") \n",
    "            if date_elem:\n",
    "                 # Usually \"1æœˆ5æ—¥(é‡‘)\" - needs Year\n",
    "                 # We can rely on the fact that race_id contains year (2025...)\n",
    "                 # But let's look for YYYYå¹´ in the whole text or title\n",
    "                 pass\n",
    "            \n",
    "            # Fallback Date from Title Tag or Meta\n",
    "            if not data[\"date\"]:\n",
    "                 meta_title = soup.title.text if soup.title else \"\"\n",
    "                 match_date = re.search(r'(\\d{4}å¹´\\d{1,2}æœˆ\\d{1,2}æ—¥)', meta_title)\n",
    "                 if match_date:\n",
    "                     data[\"date\"] = match_date.group(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing metadata for {race_id}: {e}\")\n",
    "            \n",
    "        return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test\n",
    "    scraper = RaceScraper()\n",
    "    print(\"Running test...\")\n",
    "    # Example: Do Deuce (2019105219)\n",
    "    # url = \"https://db.netkeiba.com/horse/2019105219/\"\n",
    "    # print(f\"Fetching {url}\")\n",
    "    df = scraper.get_past_races(\"2019105219\")\n",
    "    if df.empty:\n",
    "        print(\"DF is empty. Checking raw soup for 'db_h_race_results'...\")\n",
    "        soup = scraper._get_soup(f\"https://db.netkeiba.com/horse/result/2019105219/\")\n",
    "        if soup:\n",
    "             t = soup.select_one(\"table.db_h_race_results\")\n",
    "             print(f\"Selector 'table.db_h_race_results' found: {t is not None}\")\n",
    "             if not t:\n",
    "                 print(\"Trying fallback 'table' with 'ç€é †'...\")\n",
    "                 tables = soup.find_all(\"table\")\n",
    "                 found = False\n",
    "                 for i, tbl in enumerate(tables):\n",
    "                     print(f\"Table {i} classes: {tbl.get('class')}\")\n",
    "                     if \"ç€é †\" in tbl.text or \"ç€ é †\" in tbl.text or \"æ—¥ä»˜\" in tbl.text:\n",
    "                         print(\"Found a table with 'ç€é †/æ—¥ä»˜'.\")\n",
    "                         # print(str(tbl)[:200])\n",
    "                         t = tbl\n",
    "                         found = True\n",
    "                         break\n",
    "                 if not found:\n",
    "                     print(\"No table with 'ç€é †' found in soup.\")\n",
    "                     print(\"Soup snippet:\", soup.text[:500])\n",
    "                 else:\n",
    "                    # Retry parsing with found table\n",
    "                     try:\n",
    "                        df = pd.read_html(str(t))[0]\n",
    "                        print(\"Retry DF Head:\")\n",
    "                        print(df.head())\n",
    "                     except Exception as e:\n",
    "                        print(f\"Retry parsing failed: {e}\")\n",
    "        else:\n",
    "            print(\"Soup is None.\")\n",
    "    else:\n",
    "        print(df.head())\n",
    "        print(\"Columns:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "from datetime import datetime\n",
    "import urllib.parse\n",
    "import time\n",
    "\n",
    "def scrape_jra_race(url, existing_race_ids=None):\n",
    "    \"\"\"\n",
    "    Scrapes a single race page from JRA website.\n",
    "    Returns a pandas DataFrame matching the schema of database.csv.\n",
    "    If existing_race_ids is provided and the race ID is found, returns None (skip).\n",
    "    \"\"\"\n",
    "    print(f\"Accessing URL: {url}...\")\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" \n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        # Netkeiba usually uses EUC-JP, JRA uses Shift_JIS. \n",
    "        # Since this function is mostly used for Netkeiba (NAR/JRA-Backfill), default to EUC-JP.\n",
    "        # response.encoding = 'cp932' \n",
    "        response.encoding = 'EUC-JP'\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: Status code {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # --- Metadata Extraction ---\n",
    "        h1_elem = soup.select_one(\"div.header_line h1 .txt\")\n",
    "        full_text = h1_elem.text.strip() if h1_elem else \"\"\n",
    "        if not full_text and soup.h1:\n",
    "            full_text = soup.h1.text.strip()\n",
    "\n",
    "        date_text = \"\"\n",
    "        venue_text = \"\"\n",
    "        race_num_text = \"\"\n",
    "        kai = \"01\"\n",
    "        day = \"01\"\n",
    "        \n",
    "        # Extract Date\n",
    "        match_date = re.search(r'(\\d{4}å¹´\\d{1,2}æœˆ\\d{1,2}æ—¥)', full_text)\n",
    "        if match_date:\n",
    "            date_text = match_date.group(1)\n",
    "            \n",
    "        # Extract Venue, Kai, Day\n",
    "        venues_str = \"æœ­å¹Œ|å‡½é¤¨|ç¦å³¶|æ–°æ½Ÿ|æ±äº¬|ä¸­å±±|ä¸­äº¬|äº¬éƒ½|é˜ªç¥|å°å€‰\"\n",
    "        match_meta = re.search(rf'(\\d+)å›({venues_str})(\\d+)æ—¥', full_text)\n",
    "        if match_meta:\n",
    "            kai = f\"{int(match_meta.group(1)):02}\"\n",
    "            venue_text = match_meta.group(2)\n",
    "            day = f\"{int(match_meta.group(3)):02}\"\n",
    "            \n",
    "        # Extract Race Num\n",
    "        match_race = re.search(r'(\\d+)ãƒ¬ãƒ¼ã‚¹', full_text)\n",
    "        if match_race:\n",
    "            r_val = int(match_race.group(1))\n",
    "            race_num_text = f\"{r_val}R\"\n",
    "            r_num = f\"{r_val:02}\"\n",
    "        else:\n",
    "            race_num_text = \"10R\" # Fallback\n",
    "            r_num = \"10\"\n",
    "            \n",
    "        # Race Name\n",
    "        race_name_text = \"\"\n",
    "        name_elem = soup.select_one(\".race_name\")\n",
    "        if name_elem:\n",
    "            race_name_text = name_elem.text.strip()\n",
    "\n",
    "        # Grade\n",
    "        grade_text = \"\"\n",
    "        if \"G1\" in str(soup) or \"ï¼§â… \" in str(soup): grade_text = \"G1\"\n",
    "        elif \"G2\" in str(soup) or \"ï¼§â…¡\" in str(soup): grade_text = \"G2\"\n",
    "        elif \"G3\" in str(soup) or \"ï¼§â…¢\" in str(soup): grade_text = \"G3\"\n",
    "\n",
    "        # --- Added: Course, Distance, Weather, Condition ---\n",
    "        # JRA HTML structure varies, but often contained in specific divs or text lines.\n",
    "        # We will scan the entire header text or specific class for these patterns.\n",
    "        \n",
    "        # 1. Course & Distance (e.g., \"èŠ2000ãƒ¡ãƒ¼ãƒˆãƒ«\", \"ãƒ€ãƒ¼ãƒˆ1800ãƒ¡ãƒ¼ãƒˆãƒ«\", \"èŠãƒ»1600m\")\n",
    "        # Usually in the same block as race name or just below.\n",
    "        # We search the whole header area text.\n",
    "        header_text = soup.select_one(\"div.header_line\").text if soup.select_one(\"div.header_line\") else soup.text\n",
    "        \n",
    "        # Regex: Allow spaces, dots, etc. between Type and Dist\n",
    "        dist_type_match = re.search(r'(èŠ|ãƒ€|ãƒ€ãƒ¼ãƒˆ|éšœå®³)[^0-9]*(\\d+)', header_text)\n",
    "        course_type = \"\"\n",
    "        distance = \"\"\n",
    "        \n",
    "        if dist_type_match:\n",
    "            c_val = dist_type_match.group(1)\n",
    "            d_val = dist_type_match.group(2)\n",
    "            \n",
    "            if \"èŠ\" in c_val: course_type = \"èŠ\"\n",
    "            elif \"ãƒ€\" in c_val: course_type = \"ãƒ€ãƒ¼ãƒˆ\"\n",
    "            elif \"éšœ\" in c_val: course_type = \"éšœå®³\"\n",
    "            \n",
    "            distance = int(d_val)\n",
    "        \n",
    "        # 1.5 Rotation (Right/Left/Straight)\n",
    "        rotation = \"\"\n",
    "        # Often formatted as \"(å³)\" or \"ï¼ˆå·¦ï¼‰\" \n",
    "        rot_match = re.search(r'[ï¼ˆ\\(](å³|å·¦|ç›´ç·š)[ï¼‰\\)]', header_text)\n",
    "        if rot_match:\n",
    "            rotation = rot_match.group(1)\n",
    "        else:\n",
    "             # Fallback: Inference based on Venue\n",
    "             # Tokyo, Chukyo, Niigata -> Left (Default), others Right\n",
    "             # Niigata 1000m -> Straight\n",
    "             if \"å·¦\" in header_text: rotation = \"å·¦\"\n",
    "             elif \"å³\" in header_text: rotation = \"å³\"\n",
    "             elif \"ç›´ç·š\" in header_text: rotation = \"ç›´ç·š\"\n",
    "\n",
    "        \n",
    "        # 2. Weather (e.g., \"å¤©å€™ï¼šæ™´\")\n",
    "        weather = \"\"\n",
    "        w_match = re.search(r'å¤©å€™\\s*[:ï¼š]\\s*(\\S+)', soup.text)\n",
    "        if w_match:\n",
    "            weather = w_match.group(1).strip()\n",
    "            \n",
    "        # 3. Condition (e.g., \"èŠï¼šè‰¯\", \"ãƒ€ãƒ¼ãƒˆï¼šç¨é‡\")\n",
    "        # Note: A race can have both if it's mixed, but usually we care about the main one or the one matching course_type.\n",
    "        condition = \"\"\n",
    "        \n",
    "        # Try specific pattern based on course type\n",
    "        if course_type == \"èŠ\":\n",
    "             c_match = re.search(r'èŠ\\s*[:ï¼š]\\s*(\\S+)', soup.text)\n",
    "             if c_match: condition = c_match.group(1).strip()\n",
    "        elif course_type == \"ãƒ€ãƒ¼ãƒˆ\":\n",
    "             c_match = re.search(r'ãƒ€ãƒ¼ãƒˆ\\s*[:ï¼š]\\s*(\\S+)', soup.text)\n",
    "             if c_match: condition = c_match.group(1).strip()\n",
    "        \n",
    "        # Fallback if generic or course type unknown, grab first one found\n",
    "        if not condition:\n",
    "             c_match_gen = re.search(r'(?:èŠ|ãƒ€ãƒ¼ãƒˆ)\\s*[:ï¼š]\\s*(\\S+)', soup.text)\n",
    "             if c_match_gen: condition = c_match_gen.group(1).strip()\n",
    "\n",
    "        # --- Table Extraction (Custom BS4 Parsing) ---\n",
    "        # Find table with \"ç€é †\"\n",
    "        tables = soup.find_all('table')\n",
    "        target_table = None\n",
    "        for tbl in tables:\n",
    "            if \"ç€é †\" in tbl.text and \"é¦¬å\" in tbl.text:\n",
    "                target_table = tbl\n",
    "                break\n",
    "        \n",
    "        if not target_table:\n",
    "            print(f\"Warning: Result table not found in {url} (Encoding: {response.encoding})\")\n",
    "            return None\n",
    "\n",
    "        # Parse Rows\n",
    "        rows = target_table.find_all('tr')\n",
    "        data = []\n",
    "        \n",
    "        for row in rows:\n",
    "            # Skip header (usually th) or invalid rows\n",
    "            if row.find('th'):\n",
    "                continue\n",
    "                \n",
    "            cells = row.find_all('td')\n",
    "            if not cells:\n",
    "                continue\n",
    "\n",
    "            # Need to robustly map cells. \n",
    "            # We can use class names if available, or index.\n",
    "            # Based on debug:\n",
    "            # 0: place (ç€é †)\n",
    "            # 1: waku (æ ) -> img alt\n",
    "            # 2: num (é¦¬ç•ª)\n",
    "            # 3: horse (é¦¬å)\n",
    "            # 4: age (æ€§é½¢)\n",
    "            # 5: weight (æ–¤é‡)\n",
    "            # 6: jockey (é¨æ‰‹)\n",
    "            # 7: time (ã‚¿ã‚¤ãƒ )\n",
    "            # 8: margin (ç€å·®)\n",
    "            # 9: corner (é€šé)\n",
    "            # 10: f_time (ä¸Šã‚Š)\n",
    "            # 11: h_weight (é¦¬ä½“é‡)\n",
    "            # 12: trainer (èª¿æ•™å¸«)\n",
    "            # 13: pop (äººæ°—)\n",
    "            # * Odds is missing *\n",
    "\n",
    "            def get_text(idx):\n",
    "                if idx < len(cells):\n",
    "                    return cells[idx].get_text(strip=True)\n",
    "                return \"\"\n",
    "\n",
    "            # Extract Frame (Waku) from Image\n",
    "            waku_text = \"\"\n",
    "            if len(cells) > 1:\n",
    "                img = cells[1].find('img')\n",
    "                if img and 'alt' in img.attrs:\n",
    "                    # Example: \"æ 6ç·‘\" -> Extract number\n",
    "                    alt = img['alt']\n",
    "                    m = re.search(r'æ (\\d+)', alt)\n",
    "                    if m:\n",
    "                        waku_text = m.group(1)\n",
    "                    else:\n",
    "                        waku_text = alt # Fallback\n",
    "            \n",
    "            # Extract Horse ID\n",
    "            horse_id = \"\"\n",
    "            if len(cells) > 3:\n",
    "                a_tag = cells[3].find('a')\n",
    "                if a_tag and 'href' in a_tag.attrs:\n",
    "                    href = a_tag['href']\n",
    "                    # /horse/2018105247/\n",
    "                    m = re.search(r'/horse/(\\d+)', href)\n",
    "                    if m:\n",
    "                        horse_id = m.group(1)\n",
    "\n",
    "            row_data = {\n",
    "                'ç€ é †': get_text(0),\n",
    "                'æ ': waku_text,\n",
    "                'é¦¬ ç•ª': get_text(2),\n",
    "                'é¦¬å': get_text(3),\n",
    "                'horse_id': horse_id, # Added\n",
    "                'æ€§é½¢': get_text(4),\n",
    "                'æ–¤é‡': get_text(5),\n",
    "                'é¨æ‰‹': get_text(6),\n",
    "                'ã‚¿ã‚¤ãƒ ': get_text(7),\n",
    "                'ç€å·®': get_text(8),\n",
    "                'ã‚³ãƒ¼ãƒŠãƒ¼ é€šéé †': get_text(9),\n",
    "                'å¾Œ3F': get_text(10),\n",
    "                'é¦¬ä½“é‡ (å¢—æ¸›)': get_text(11),\n",
    "                'å©èˆ': get_text(12),\n",
    "                'äºº æ°—': get_text(13),\n",
    "                'å˜å‹ ã‚ªãƒƒã‚º': \"0.0\" # Missing in source\n",
    "            }\n",
    "            data.append(row_data)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Add Metadata Columns\n",
    "        df['æ—¥ä»˜'] = date_text\n",
    "        df['ä¼šå ´'] = venue_text\n",
    "        df['ãƒ¬ãƒ¼ã‚¹ç•ªå·'] = race_num_text\n",
    "        df['ãƒ¬ãƒ¼ã‚¹å'] = race_name_text\n",
    "        df['é‡è³'] = grade_text\n",
    "        df['è·é›¢'] = distance\n",
    "        df['ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—'] = course_type\n",
    "        df['å¤©å€™'] = weather\n",
    "        df['å¤©å€™'] = weather\n",
    "        df['é¦¬å ´çŠ¶æ…‹'] = condition\n",
    "        df['å›ã‚Š'] = rotation\n",
    "        \n",
    "        # ID Generation\n",
    "        place_map = {\n",
    "            \"æœ­å¹Œ\": \"01\", \"å‡½é¤¨\": \"02\", \"ç¦å³¶\": \"03\", \"æ–°æ½Ÿ\": \"04\", \"æ±äº¬\": \"05\",\n",
    "            \"ä¸­å±±\": \"06\", \"ä¸­äº¬\": \"07\", \"äº¬éƒ½\": \"08\", \"é˜ªç¥\": \"09\", \"å°å€‰\": \"10\"\n",
    "        }\n",
    "        p_code = place_map.get(venue_text, \"00\")\n",
    "        \n",
    "        year = \"2025\"\n",
    "        if date_text:\n",
    "            year = date_text[:4]\n",
    "            \n",
    "        generated_id = f\"{year}{p_code}{kai}{day}{r_num}\"\n",
    "        \n",
    "        # SKIP CHECK\n",
    "        if existing_race_ids and generated_id in existing_race_ids:\n",
    "            print(f\"Skipping {generated_id} (Already exists)\")\n",
    "            return None\n",
    "\n",
    "        df['race_id'] = generated_id\n",
    "\n",
    "        # Cleanups\n",
    "        if 'å˜å‹ ã‚ªãƒƒã‚º' in df.columns:\n",
    "            df['å˜å‹ ã‚ªãƒƒã‚º'] = pd.to_numeric(df['å˜å‹ ã‚ªãƒƒã‚º'], errors='coerce').fillna(0.0)\n",
    "\n",
    "        standard_columns = [\n",
    "            \"æ—¥ä»˜\",\"ä¼šå ´\",\"ãƒ¬ãƒ¼ã‚¹ç•ªå·\",\"ãƒ¬ãƒ¼ã‚¹å\",\"é‡è³\",\"ç€ é †\",\"æ \",\"é¦¬ ç•ª\",\"é¦¬å\",\"æ€§é½¢\",\"æ–¤é‡\",\"é¨æ‰‹\",\n",
    "            \"ã‚¿ã‚¤ãƒ \",\"ç€å·®\",\"äºº æ°—\",\"å˜å‹ ã‚ªãƒƒã‚º\",\"å¾Œ3F\",\"ã‚³ãƒ¼ãƒŠãƒ¼ é€šéé †\",\"å©èˆ\",\"é¦¬ä½“é‡ (å¢—æ¸›)\",\"race_id\",\n",
    "            \"è·é›¢\",\"ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—\",\"å¤©å€™\",\"é¦¬å ´çŠ¶æ…‹\",\"å›ã‚Š\"\n",
    "        ]\n",
    "        \n",
    "        for col in standard_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = \"\"\n",
    "                \n",
    "        df = df[standard_columns]\n",
    "        \n",
    "        print(f\"Scraped {len(df)} rows.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping JRA URL: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Parameter Map for Monthly Results (Reverse Engineered)\n",
    "JRA_MONTH_PARAMS = {\n",
    "    \"2026\": { \"01\": \"E4\", \"02\": \"B2\", \"03\": \"80\", \"04\": \"4E\", \"05\": \"1C\", \"06\": \"EA\", \"07\": \"B8\", \"08\": \"86\", \"09\": \"54\", \"10\": \"22\", \"11\": \"F0\", \"12\": \"BE\" },\n",
    "    \"2025\": { \"01\": \"3F\", \"02\": \"0D\", \"03\": \"DB\", \"04\": \"A9\", \"05\": \"77\", \"06\": \"45\", \"07\": \"13\", \"08\": \"E1\", \"09\": \"AF\", \"10\": \"1E\", \"11\": \"EC\", \"12\": \"D3\" },\n",
    "    \"2024\": { \"01\": \"B3\", \"02\": \"81\", \"03\": \"4F\", \"04\": \"1D\", \"05\": \"EB\", \"06\": \"B9\", \"07\": \"87\", \"08\": \"55\", \"09\": \"23\", \"10\": \"92\", \"11\": \"60\", \"12\": \"2E\" },\n",
    "    \"2023\": { \"01\": \"27\", \"02\": \"F5\", \"03\": \"C3\", \"04\": \"91\", \"05\": \"5F\", \"06\": \"2D\", \"07\": \"FB\", \"08\": \"C9\", \"09\": \"97\", \"10\": \"06\", \"11\": \"D4\", \"12\": \"A2\" },\n",
    "    \"2022\": { \"01\": \"9B\", \"02\": \"69\", \"03\": \"37\", \"04\": \"05\", \"05\": \"D3\", \"06\": \"A1\", \"07\": \"6F\", \"08\": \"3D\", \"09\": \"0B\", \"10\": \"7A\", \"11\": \"48\", \"12\": \"16\" },\n",
    "    \"2021\": { \"01\": \"0F\", \"02\": \"DD\", \"03\": \"AB\", \"04\": \"79\", \"05\": \"47\", \"06\": \"15\", \"07\": \"E3\", \"08\": \"B1\", \"09\": \"7F\", \"10\": \"EE\", \"11\": \"BC\", \"12\": \"8A\" },\n",
    "    \"2020\": { \"01\": \"83\", \"02\": \"51\", \"03\": \"1F\", \"04\": \"ED\", \"05\": \"BB\", \"06\": \"89\", \"07\": \"57\", \"08\": \"25\", \"09\": \"F3\", \"10\": \"62\", \"11\": \"30\", \"12\": \"FE\" }\n",
    "}\n",
    "\n",
    "def scrape_jra_year(year_str, start_date=None, end_date=None, save_callback=None, existing_race_ids=None):\n",
    "    \"\"\"\n",
    "    Scrapes races for a given year and date range.\n",
    "    year_str: \"2024\" or \"2025\"\n",
    "    start_date: datetime.date (optional)\n",
    "    end_date: datetime.date (optional)\n",
    "    save_callback: function(df) to save progress\n",
    "    \"\"\"\n",
    "    \n",
    "    if year_str not in JRA_MONTH_PARAMS:\n",
    "        print(f\"Year {year_str} not supported in parameter map.\")\n",
    "        return\n",
    "\n",
    "    params = JRA_MONTH_PARAMS[year_str]\n",
    "    base_url = \"https://www.jra.go.jp/JRADB/accessS.html\"\n",
    "    \n",
    "    # Determine months to iterate\n",
    "    start_m = 1\n",
    "    end_m = 12\n",
    "    \n",
    "    if start_date:\n",
    "        start_m = start_date.month\n",
    "    if end_date:\n",
    "        end_m = end_date.month\n",
    "    \n",
    "    # Cap at Today to prevent future scraping\n",
    "    from datetime import date\n",
    "    today = date.today()\n",
    "    \n",
    "    # If explicit end_date is used, respect it, but also respect today if it is earlier?\n",
    "    # Usually for results, we never want future.\n",
    "    if end_date:\n",
    "        actual_end_date = min(end_date, today)\n",
    "    else:\n",
    "        actual_end_date = today\n",
    "        \n",
    "    print(f\"=== Starting JRA Bulk Scraping for {year_str} (Period: {start_date or 'Start'} - {actual_end_date}) ===\")\n",
    "\n",
    "    # Adjust end_m based on today if we are in target year\n",
    "    if int(year_str) == today.year:\n",
    "        end_m = min(end_m, today.month)\n",
    "    elif int(year_str) > today.year:\n",
    "        print(f\"Year {year_str} is in the future. Stopping.\")\n",
    "        return\n",
    "    \n",
    "    for m in range(start_m, end_m + 1):\n",
    "        month = f\"{m:02}\"\n",
    "        if month not in params:\n",
    "            continue\n",
    "            \n",
    "        suffix = params[month]\n",
    "        # Logic for skl00 vs skl10\n",
    "        try:\n",
    "            ym = int(year_str + month)\n",
    "            prefix = \"pw01skl00\" if ym >= 202512 else \"pw01skl10\"\n",
    "        except:\n",
    "            prefix = \"pw01skl10\"\n",
    "\n",
    "        cname = f\"{prefix}{year_str}{month}/{suffix}\"\n",
    "        \n",
    "        print(f\"Fetching list for {year_str}/{month} (CNAME={cname})...\")\n",
    "        \n",
    "        try:\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" \n",
    "            }\n",
    "            response = requests.post(base_url, data={\"cname\": cname}, headers=headers, timeout=10)\n",
    "            response.encoding = 'cp932'\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch {cname} (Status {response.status_code})\")\n",
    "                continue\n",
    "                \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            race_cnames = []\n",
    "            links = soup.find_all('a')\n",
    "            for link in links:\n",
    "                onclick = link.get('onclick', '')\n",
    "                match = re.search(r\"doAction\\('[^']+',\\s*'([^']+)'\\)\", onclick)\n",
    "                if match:\n",
    "                    c = match.group(1)\n",
    "                    if c.startswith('pw01srl'):\n",
    "                        race_cnames.append(c)\n",
    "            \n",
    "            race_cnames = sorted(list(set(race_cnames)))\n",
    "            print(f\"  Found {len(race_cnames)} race days in month.\")\n",
    "            \n",
    "            for day_cname in race_cnames:\n",
    "                resp_day = requests.post(base_url, data={\"cname\": day_cname}, headers=headers, timeout=10)\n",
    "                resp_day.encoding = 'cp932'\n",
    "                soup_day = BeautifulSoup(resp_day.text, 'html.parser')\n",
    "                \n",
    "                # Check date of this day page\n",
    "                day_date_text = \"\"\n",
    "                d_h1 = soup_day.select_one(\"div.header_line h1 .txt\")\n",
    "                full_d_text = d_h1.text.strip() if d_h1 else (soup_day.h1.text.strip() if soup_day.h1 else \"\")\n",
    "                \n",
    "                # Parse date from \"2025å¹´1æœˆ5æ—¥ï¼ˆæ—¥æ›œï¼‰1å›ä¸­å±±1æ—¥\"\n",
    "                # Need to match Date AND Venue/Kai/Day info for ID generation\n",
    "                # Pattern: YYYYå¹´MæœˆDæ—¥ ... Kå›VenueDæ—¥\n",
    "                \n",
    "                current_day_date = None\n",
    "                kai_str = \"01\"\n",
    "                day_str = \"01\"\n",
    "                venue_str = \"\"\n",
    "                p_code = \"00\"\n",
    "                \n",
    "                match_day_date = re.search(r'(\\d{4})å¹´(\\d{1,2})æœˆ(\\d{1,2})æ—¥', full_d_text)\n",
    "                if match_day_date:\n",
    "                    y, mo, d_day = map(int, match_day_date.groups())\n",
    "                    current_day_date = datetime(y, mo, d_day).date()\n",
    "                    \n",
    "                    # Filtering\n",
    "                    if start_date and current_day_date < start_date:\n",
    "                        print(f\"    Skipping day {current_day_date} (Before start date)\")\n",
    "                        continue\n",
    "                    if end_date and current_day_date > end_date:\n",
    "                        print(f\"    Skipping day {current_day_date} (After end date)\")\n",
    "                        continue\n",
    "                    print(f\"    Processing Day: {current_day_date} ({full_d_text})\")\n",
    "                else:\n",
    "                    print(f\"    Processing Day (Date unknown): {full_d_text[:20]}...\")\n",
    "\n",
    "                # Parse Venue Info for ID Generation\n",
    "                venues_ptn = \"æœ­å¹Œ|å‡½é¤¨|ç¦å³¶|æ–°æ½Ÿ|æ±äº¬|ä¸­å±±|ä¸­äº¬|äº¬éƒ½|é˜ªç¥|å°å€‰\"\n",
    "                match_meta = re.search(rf'(\\d+)å›({venues_ptn})(\\d+)æ—¥', full_d_text)\n",
    "                if match_meta:\n",
    "                    kai_str = f\"{int(match_meta.group(1)):02}\"\n",
    "                    venue_str = match_meta.group(2)\n",
    "                    day_str = f\"{int(match_meta.group(3)):02}\"\n",
    "                    \n",
    "                    place_map = {\n",
    "                        \"æœ­å¹Œ\": \"01\", \"å‡½é¤¨\": \"02\", \"ç¦å³¶\": \"03\", \"æ–°æ½Ÿ\": \"04\", \"æ±äº¬\": \"05\",\n",
    "                        \"ä¸­å±±\": \"06\", \"ä¸­äº¬\": \"07\", \"äº¬éƒ½\": \"08\", \"é˜ªç¥\": \"09\", \"å°å€‰\": \"10\"\n",
    "                    }\n",
    "                    p_code = place_map.get(venue_str, \"00\")\n",
    "                \n",
    "                # Collect Race Links AND Race Numbers\n",
    "                # Need to pair Link with Race Number\n",
    "                race_list_items = []\n",
    "\n",
    "                all_anchors = soup_day.find_all('a')\n",
    "                for a in all_anchors:\n",
    "                    onclick = a.get('onclick', '')\n",
    "                    # Check for doAction with robust regex (handles single/double quotes, whitespace)\n",
    "                    # Pattern: doAction('FormName', 'CNAME')\n",
    "                    match_sde = re.search(r\"doAction\\s*\\(\\s*['\\\"][^'\\\"]+['\\\"]\\s*,\\s*['\\\"](pw01sde[^'\\\"]+)['\\\"]\\s*\\)\", onclick)\n",
    "                    href = a.get('href', '')\n",
    "                    \n",
    "                    final_url = \"\"\n",
    "                    if match_sde:\n",
    "                        final_url = f\"{base_url}?CNAME={match_sde.group(1)}\"\n",
    "                    elif 'pw01sde' in href:\n",
    "                        # Fallback for simple hrefs\n",
    "                        if 'CNAME=' in href:\n",
    "                             final_url = urllib.parse.urljoin(base_url, href)\n",
    "                        else:\n",
    "                             # If href=\"accessS.html?CNAME=...\"\n",
    "                             # or just \"?CNAME=...\"\n",
    "                             final_url = urllib.parse.urljoin(base_url, href)\n",
    "                    \n",
    "                    if final_url:\n",
    "                        # Extract Race Number from anchor text (e.g. \"1R\", \"11R\")\n",
    "                        # Or generic image alt?\n",
    "                        # Usually text is \"1R\" or img alt=\"1R\"\n",
    "                        txt = a.text.strip()\n",
    "                        img = a.find('img')\n",
    "                        if not txt and img and 'alt' in img.attrs:\n",
    "                            txt = img['alt']\n",
    "                        \n",
    "                        r_num = -1\n",
    "                        r_num_match = re.search(r'(\\d+)R', txt)\n",
    "                        if r_num_match:\n",
    "                             r_num = int(r_num_match.group(1))\n",
    "                             \n",
    "                        # Append even if Race Num is not found (fix for missing races)\n",
    "                        race_list_items.append((final_url, r_num))\n",
    "                \n",
    "                # Deduplicate by URL (keep first found usually fine)\n",
    "                # Sort by Race Number (unknowns (-1) first or last?)\n",
    "                seen_urls = set()\n",
    "                unique_races = []\n",
    "                for url, r_num in race_list_items:\n",
    "                    if url not in seen_urls:\n",
    "                        unique_races.append((url, r_num))\n",
    "                        seen_urls.add(url)\n",
    "                \n",
    "                unique_races.sort(key=lambda x: x[1]) # Sort by race num (-1 will be first)\n",
    "                \n",
    "                print(f\"      -> {len(unique_races)} races found.\")\n",
    "\n",
    "                for r_link, r_num in unique_races:\n",
    "                    # PRE-FETCH OPTIMIZATION\n",
    "                    # Construct ID\n",
    "                    # Only if we successfully extracted Race Num and Venue info\n",
    "                    if r_num != -1 and p_code != \"00\" and current_day_date:\n",
    "                         # ID: YYYY PP KK DD RR\n",
    "                         # y is from match_day_date loop var (int)\n",
    "                         # p_code, kai_str, day_str strings\n",
    "                         \n",
    "                         # Ensure year is from the day page date\n",
    "                         y_str = str(y)\n",
    "                         r_num_str = f\"{r_num:02}\"\n",
    "                         \n",
    "                         generated_id = f\"{y_str}{p_code}{kai_str}{day_str}{r_num_str}\"\n",
    "                         \n",
    "                         if existing_race_ids and generated_id in existing_race_ids:\n",
    "                             # print(f\"        [Skip] {generated_id} (Pre-check)\")\n",
    "                             continue\n",
    "                    \n",
    "                    # If not skipped, fetch\n",
    "                    df = scrape_jra_race(r_link, existing_race_ids=existing_race_ids)\n",
    "                    \n",
    "                    if df is not None and not df.empty:\n",
    "                        if save_callback:\n",
    "                            save_callback(df)\n",
    "                        time.sleep(1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing month {month}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAR ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date, timedelta\n",
    "import calendar\n",
    "import time\n",
    "import os\n",
    "\n",
    "def run_nar_scraping(year, start_month=1, end_month=12, save_dir='data/raw'):\n",
    "    start_date = date(int(year), int(start_month), 1)\n",
    "    last_day = calendar.monthrange(int(year), int(end_month))[1]\n",
    "    end_date = date(int(year), int(end_month), last_day)\n",
    "    \n",
    "    today = date.today()\n",
    "    if end_date > today: end_date = today\n",
    "    \n",
    "    print(f'NARãƒ‡ãƒ¼ã‚¿ã‚’ {start_date} ã‹ã‚‰ {end_date} ã¾ã§å–å¾—ã—ã¾ã™...')\n",
    "    print(f'ä¿å­˜å…ˆ: {os.path.join(save_dir, \"database_nar.csv\")}')\n",
    "    \n",
    "    curr = start_date\n",
    "    # scraper = RaceScraper() # Not used directly if we use scrape_jra_race\n",
    "    \n",
    "    while curr <= end_date:\n",
    "        d_str = curr.strftime('%Y%m%d')\n",
    "        url = f'https://nar.netkeiba.com/top/race_list_sub.html?kaisai_date={d_str}'\n",
    "        try:\n",
    "             time.sleep(0.5)\n",
    "             headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "             resp = requests.get(url, headers=headers)\n",
    "             resp.encoding = 'EUC-JP'\n",
    "             soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "             links = soup.select('a[href*=\"race/result.html\"]')\n",
    "             \n",
    "             if links:\n",
    "                 print(f'{curr}: {len(links)} ä»¶ã®ãƒ¬ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚')\n",
    "                 for link in links:\n",
    "                     href = link.get('href')\n",
    "                     if href.startswith('../'):\n",
    "                         full_url = f'https://nar.netkeiba.com/{href.replace(\"../\", \"\")}'\n",
    "                     elif href.startswith('http'):\n",
    "                         full_url = href\n",
    "                     else:\n",
    "                         full_url = f'https://nar.netkeiba.com{href}'\n",
    "                     \n",
    "                     # scrape_jra_race is compatible with netkeiba structure\n",
    "                     try:\n",
    "                         df = scrape_jra_race(full_url, existing_race_ids=None)\n",
    "                         if df is not None and not df.empty:\n",
    "                             # Save immediately\n",
    "                             os.makedirs(save_dir, exist_ok=True)\n",
    "                             mode = 'a'\n",
    "                             csv_file = os.path.join(save_dir, 'database_nar.csv')\n",
    "                             header = not os.path.exists(csv_file)\n",
    "                             df.to_csv(csv_file, mode=mode, header=header, index=False)\n",
    "                         time.sleep(1)\n",
    "                     except Exception as e_race:\n",
    "                         print(f'  Error scraping race {full_url}: {e_race}')\n",
    "             else:\n",
    "                 # print(f'{curr}: ãƒ¬ãƒ¼ã‚¹ãªã—')\n",
    "                 pass\n",
    "        except Exception as e: print(e)\n",
    "        curr += timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®š (ã“ã“ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„)\n",
    "YEAR = 2024          # å¯¾è±¡å¹´åº¦\n",
    "START_MONTH = 1      # é–‹å§‹æœˆ\n",
    "END_MONTH = 12       # çµ‚äº†æœˆ\n",
    "SAVE_DIR = '/content/drive/MyDrive/dai-keiba/data/raw' # ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€\n",
    "\n",
    "# å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯\n",
    "if YEAR:\n",
    "    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    run_nar_scraping(YEAR, START_MONTH, END_MONTH, save_dir=SAVE_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}