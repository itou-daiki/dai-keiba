{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ‡ NAR å…¨ãƒ¬ãƒ¼ã‚¹å–å¾— (Smart Differential)\n",
    "è¨­å®šå¤‰æ•°ã‚’å¤‰æ›´ã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚**ã™ã§ã«å–å¾—æ¸ˆã¿ã§æ­£å½“æ€§ãŒç¢ºèªã•ã‚ŒãŸãƒ¬ãƒ¼ã‚¹ã¯ã‚¹ã‚­ãƒƒãƒ—**ã—ã€æœªå–å¾—ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ä¸å‚™ã®ã‚ã‚‹ãƒ¬ãƒ¼ã‚¹ã®ã¿å–å¾—ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆã™ã‚‹å ´åˆã®ã¿å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®š (ã“ã“ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„)\n",
    "YEAR = 2025          # å¯¾è±¡å¹´åº¦\n",
    "START_MONTH = 1      # é–‹å§‹æœˆ\n",
    "END_MONTH = 12       # çµ‚äº†æœˆ\n",
    "SAVE_DIR = '/content/drive/MyDrive/dai-keiba/data/raw' # ä¿å­˜å…ˆ\n",
    "TARGET_CSV = 'database_nar.csv'\n",
    "TARGET_ID_CSV = 'race_ids_nar.csv' # å–å¾—æ¸ˆã¿ãƒ¬ãƒ¼ã‚¹IDãƒªã‚¹ãƒˆ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class RaceScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "    def _get_soup(self, url):\n",
    "        try:\n",
    "            time.sleep(1) # Be polite\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.encoding = response.apparent_encoding\n",
    "            if response.status_code == 200:\n",
    "                return BeautifulSoup(response.text, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def get_past_races(self, horse_id, target_date=None, n_samples=5):\n",
    "        \"\"\"\n",
    "        Fetches past n_samples race results for a given horse_id from netkeiba db.\n",
    "        If target_date is provided, filters for races STRICTLY BEFORE that date.\n",
    "        Returns a DataFrame of past races.\n",
    "        \"\"\"\n",
    "        url = f\"https://db.netkeiba.com/horse/result/{horse_id}/\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # The results are usually in a table with class \"db_h_race_results\"\n",
    "        table = soup.select_one(\"table.db_h_race_results\")\n",
    "        if not table:\n",
    "            # Try to find any table with \"ç€é †\"\n",
    "            tables = soup.find_all(\"table\")\n",
    "            for t in tables:\n",
    "                if \"ç€é †\" in t.text:\n",
    "                    table = t\n",
    "                    break\n",
    "        \n",
    "        if not table:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Parse table\n",
    "        try:\n",
    "            df = pd.read_html(io.StringIO(str(table)))[0]\n",
    "            \n",
    "            # Basic cleaning\n",
    "            df = df.dropna(how='all')\n",
    "            \n",
    "            # Normalize column names (remove spaces/newlines)\n",
    "            df.columns = df.columns.astype(str).str.replace(r'\\s+', '', regex=True)\n",
    "\n",
    "            # Filter rows that look like actual races (Date column exists)\n",
    "            if 'æ—¥ä»˜' in df.columns:\n",
    "                df['date_obj'] = pd.to_datetime(df['æ—¥ä»˜'], format='%Y/%m/%d', errors='coerce')\n",
    "                df = df.dropna(subset=['date_obj'])\n",
    "                \n",
    "                # Leakage Prevention: Filter races strictly before target_date\n",
    "                if target_date:\n",
    "                    if isinstance(target_date, str):\n",
    "                        target_dt = pd.to_datetime(target_date, errors='coerce')\n",
    "                    else:\n",
    "                        target_dt = pd.to_datetime(target_date) # handle date/datetime\n",
    "                        \n",
    "                    if target_dt is not None:\n",
    "                         # Use strictly less than (<) to exclude future and current race (if in DB)\n",
    "                         df = df[df['date_obj'] < target_dt]\n",
    "\n",
    "                df = df.sort_values('date_obj', ascending=False)\n",
    "                \n",
    "            # Take top N\n",
    "            if n_samples:\n",
    "                df = df.head(n_samples)\n",
    "            \n",
    "            # Process Run Style (Leg Type)\n",
    "            if 'é€šé' in df.columns:\n",
    "                df['run_style_val'] = df['é€šé'].apply(self.extract_run_style)\n",
    "            else:\n",
    "                df['run_style_val'] = 3 # Unknown\n",
    "\n",
    "            # Extract/Rename Columns\n",
    "            # We want: æ—¥ä»˜, é–‹å‚¬, å¤©æ°—, R, ãƒ¬ãƒ¼ã‚¹å, æ˜ åƒ, é ­æ•°, æ ç•ª, ... ç€é †, ... é€šé, ...\n",
    "            # Important: 'ä¸Šã‚Š' (3F), 'é¦¬ä½“é‡', 'é¨æ‰‹'\n",
    "            \n",
    "            # Map standard columns if they exist\n",
    "            column_map = {\n",
    "                'æ—¥ä»˜': 'date',\n",
    "                'é–‹å‚¬': 'venue',\n",
    "                'å¤©æ°—': 'weather',\n",
    "                'ãƒ¬ãƒ¼ã‚¹å': 'race_name',\n",
    "                'ç€é †': 'rank',\n",
    "                'æ ç•ª': 'waku',\n",
    "                'é¦¬ç•ª': 'umaban',\n",
    "                'é¨æ‰‹': 'jockey',\n",
    "                'æ–¤é‡': 'weight_carried',\n",
    "                'é¦¬å ´': 'condition', # è‰¯/é‡/ç¨é‡ etc.\n",
    "                'ã‚¿ã‚¤ãƒ ': 'time',\n",
    "                'ç€å·®': 'margin',\n",
    "                'ä¸Šã‚Š': 'last_3f',\n",
    "                'é€šé': 'passing',\n",
    "                'é¦¬ä½“é‡': 'horse_weight',\n",
    "                'run_style_val': 'run_style',\n",
    "                'å˜å‹': 'odds',\n",
    "                'ã‚ªãƒƒã‚º': 'odds',\n",
    "                'è·é›¢': 'raw_distance' # e.g. \"èŠ1600\"\n",
    "            }\n",
    "            \n",
    "            # Rename available columns\n",
    "            df.rename(columns=column_map, inplace=True)\n",
    "            \n",
    "            # Extract Surface and Distance from 'raw_distance'\n",
    "            if 'raw_distance' in df.columns:\n",
    "                def parse_dist(x):\n",
    "                    if not isinstance(x, str): return None, None\n",
    "                    # \"èŠ1600\", \"ãƒ€1200\", \"éšœ3000\"\n",
    "                    # Sometimes \"èŠ1600\" or just \"1600\"\n",
    "                    surf = None\n",
    "                    dist = None\n",
    "                    if 'èŠ' in x: surf = 'èŠ'\n",
    "                    elif 'ãƒ€' in x: surf = 'ãƒ€'\n",
    "                    elif 'éšœ' in x: surf = 'éšœ'\n",
    "                    \n",
    "                    # Extract number\n",
    "                    match = re.search(r'(\\d+)', x)\n",
    "                    if match:\n",
    "                        dist = int(match.group(1))\n",
    "                    return surf, dist\n",
    "\n",
    "                parsed = df['raw_distance'].apply(parse_dist)\n",
    "                df['course_type'] = parsed.apply(lambda x: x[0])\n",
    "                df['distance'] = parsed.apply(lambda x: x[1])\n",
    "            else:\n",
    "                df['course_type'] = None\n",
    "                df['distance'] = None\n",
    "\n",
    "            # Coerce numeric\n",
    "            if 'rank' in df.columns:\n",
    "                df['rank'] = pd.to_numeric(df['rank'], errors='coerce')\n",
    "            \n",
    "            if 'odds' in df.columns:\n",
    "                 df['odds'] = pd.to_numeric(df['odds'], errors='coerce')\n",
    "            \n",
    "            # Fill missing\n",
    "            for target_col in list(column_map.values()) + ['course_type', 'distance']:\n",
    "                if target_col not in df.columns:\n",
    "                    df[target_col] = None\n",
    "                \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing past races for {horse_id}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def extract_run_style(self, passing_str):\n",
    "        \"\"\"\n",
    "        Converts passing order string (e.g., \"1-1-1\", \"10-10-12\") to run style (1,2,3,4).\n",
    "        1: Nige (Escape) - Lead at 1st corner\n",
    "        2: Senkou (Leader) - Within first ~4 or so\n",
    "        3: Sashi (Mid) - Midpack\n",
    "        4: Oikomi (Chaser) - Back\n",
    "        Returns integer code.\n",
    "        \"\"\"\n",
    "        if not isinstance(passing_str, str):\n",
    "            return 3 # Default to Mid\n",
    "            \n",
    "        # Clean string \"1-1-1\" -> [1, 1, 1]\n",
    "        try:\n",
    "            cleaned = re.sub(r'[^0-9-]', '', passing_str)\n",
    "            parts = [int(p) for p in cleaned.split('-') if p]\n",
    "            \n",
    "            if not parts:\n",
    "                return 3\n",
    "                \n",
    "            first_corner = parts[0]\n",
    "            \n",
    "            # Heuristics\n",
    "            if first_corner == 1:\n",
    "                return 1 # Nige\n",
    "            elif first_corner <= 4:\n",
    "                return 2 # Senkou\n",
    "            elif first_corner <= 9: # Assuming standard field size of 10-16, 9 is mid-ish limit? \n",
    "                # Actually \"Sashi\" is usually mid-rear. \n",
    "                # Let's say: 1=Lead, 2-4=Front, 5-10=Mid, >10=Back\n",
    "                return 3 # Sashi\n",
    "            else:\n",
    "                return 4 # Oikomi\n",
    "                \n",
    "        except:\n",
    "            return 3\n",
    "\n",
    "    def scrape_race_with_history(self, race_id):\n",
    "        \"\"\"\n",
    "        Detailed scraper that enters a race_result page, finding horse IDs, \n",
    "        then fetches history for each horse.\n",
    "        Returns a dictionary or structured object with the race result + history.\n",
    "        \"\"\"\n",
    "        url = f\"https://race.netkeiba.com/race/result.html?race_id={race_id}\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "            \n",
    "        # 0. Extract Race Date for Leakage Prevention\n",
    "        race_date = None\n",
    "        try:\n",
    "            # Try to find date in .RaceData01 or similar\n",
    "            # Example text: \"10:10æ›‡è‰¯2021å¹´1æœˆ5æ—¥...\"\n",
    "            # Netkeiba often puts date in the title tag too like \"2021å¹´1æœˆ5æ—¥...\"\n",
    "            \n",
    "            # Strategy 1: Title\n",
    "            if soup.title:\n",
    "                title_text = soup.title.text\n",
    "                match = re.search(r'(\\d{4})å¹´(\\d{1,2})æœˆ(\\d{1,2})æ—¥', title_text)\n",
    "                if match:\n",
    "                    y, m, d = match.groups()\n",
    "                    race_date = datetime(int(y), int(m), int(d))\n",
    "            \n",
    "            # Strategy 2: .RaceData01 (Common in Result page)\n",
    "            if not race_date:\n",
    "                rd1 = soup.find(\"div\", class_=\"RaceData01\")\n",
    "                if rd1:\n",
    "                    match = re.search(r'(\\d{4})å¹´(\\d{1,2})æœˆ(\\d{1,2})æ—¥', rd1.text)\n",
    "                    if match:\n",
    "                        y, m, d = match.groups()\n",
    "                        race_date = datetime(int(y), int(m), int(d))\n",
    "            \n",
    "            # Strategy 3: URL (kaisai_date=YYYYMMDD) - Though URL input is race_id, result page might link to kaisai\n",
    "            if not race_date:\n",
    "                # Some list links contain kaisai_date, but here we only have race_id\n",
    "                pass\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not extract race date: {e}\")\n",
    "\n",
    "        # 1. Parse Main Result Table\n",
    "        result_data = []\n",
    "        table = soup.find(\"table\", id=\"All_Result_Table\")\n",
    "        if not table:\n",
    "            return None\n",
    "            \n",
    "        rows = table.find_all(\"tr\", class_=\"HorseList\")\n",
    "        print(f\"Found {len(rows)} horses in race {race_id} ({race_date.date() if race_date else 'Unknown Date'}). Fetching histories...\")\n",
    "        \n",
    "        for row in rows:\n",
    "            # Extract basic info\n",
    "            rank_elem = row.select_one(\".Rank\")\n",
    "            rank = rank_elem.text.strip() if rank_elem else \"\"\n",
    "            \n",
    "            horse_name_elem = row.select_one(\".Horse_Name a\")\n",
    "            horse_name = horse_name_elem.text.strip() if horse_name_elem else \"\"\n",
    "            horse_url = horse_name_elem.get(\"href\") if horse_name_elem else \"\"\n",
    "            \n",
    "            # Extract ID from URL\n",
    "            horse_id = None\n",
    "            if horse_url:\n",
    "                match = re.search(r'/horse/(\\d+)', horse_url)\n",
    "                if match:\n",
    "                    horse_id = match.group(1)\n",
    "            \n",
    "            if not horse_id:\n",
    "                print(f\"  Skipping {horse_name} (No ID)\")\n",
    "                continue\n",
    "\n",
    "            # print(f\"  Fetching history for {horse_name} ({horse_id})...\")\n",
    "            \n",
    "            # 2. Get Past History (with Leakage Prevention)\n",
    "            df_past = self.get_past_races(horse_id, target_date=race_date, n_samples=5)\n",
    "            \n",
    "            # 3. Structure Data\n",
    "            # converting df_past to a list of dicts or flattened fields\n",
    "            history = []\n",
    "            if not df_past.empty:\n",
    "                for idx, r in df_past.iterrows():\n",
    "                    # Extract relevant columns\n",
    "                    # We need at least: Rank, RunStyle, Time(Seconds?), Pace?\n",
    "                    # For now just dump raw-ish data\n",
    "                    hist_item = {\n",
    "                        \"date\": r.get('æ—¥ä»˜'),\n",
    "                        \"race_name\": r.get('ãƒ¬ãƒ¼ã‚¹å'),\n",
    "                        \"rank\": r.get('ç€é †'),\n",
    "                        \"passing\": r.get('é€šé'),\n",
    "                        \"run_style\": r.get('run_style_val'),\n",
    "                        \"time\": r.get('ã‚¿ã‚¤ãƒ '),\n",
    "                        # Add more as needed for Feature Engineering\n",
    "                    }\n",
    "                    history.append(hist_item)\n",
    "            \n",
    "            entry = {\n",
    "                \"race_id\": race_id,\n",
    "                \"horse_id\": horse_id,\n",
    "                \"horse_name\": horse_name,\n",
    "                \"rank\": rank,\n",
    "                \"history\": history\n",
    "            }\n",
    "            result_data.append(entry)\n",
    "            \n",
    "        return result_data\n",
    "\n",
    "    def get_horse_profile(self, horse_id):\n",
    "        \"\"\"\n",
    "        Fetches horse profile to get pedigree (Father, Mother, Grandfather(BMS)).\n",
    "        Returns a dictionary or None.\n",
    "        \"\"\"\n",
    "        # Use pedigree page for reliable bloodline data\n",
    "        url = f\"https://db.netkeiba.com/horse/ped/{horse_id}/\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        # Parse Blood Table\n",
    "        # table class=\"blood_table\"\n",
    "        \n",
    "        data = {\n",
    "            \"father\": \"\",\n",
    "            \"mother\": \"\",\n",
    "            \"bms\": \"\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            table = soup.select_one(\"table.blood_table\")\n",
    "            if table:\n",
    "                rows = table.find_all(\"tr\")\n",
    "                # 5-generation table has 32 rows usually\n",
    "                # Father at Row 0 (rowspan 16)\n",
    "                # Mother at Row 16 (rowspan 16)\n",
    "                \n",
    "                if len(rows) >= 17:\n",
    "                    # Father: Row 0, Col 0\n",
    "                    r0 = rows[0].find_all(\"td\")\n",
    "                    if r0:\n",
    "                        txt = r0[0].text.strip()\n",
    "                        # Clean: \"ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ãƒ’ãƒ¼ãƒ­ãƒ¼\\n2004 æ —æ¯›...\" -> \"ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ãƒ’ãƒ¼ãƒ­ãƒ¼\"\n",
    "                        # Take first line\n",
    "                        data[\"father\"] = txt.split('\\n')[0].strip()\n",
    "                        \n",
    "                    # Mother & BMS: Row 16\n",
    "                    r16 = rows[16].find_all(\"td\")\n",
    "                    if len(r16) >= 2:\n",
    "                        # Mother\n",
    "                        m_txt = r16[0].text.strip()\n",
    "                        data[\"mother\"] = m_txt.split('\\n')[0].strip()\n",
    "                        \n",
    "                        # BMS (Mother's Father)\n",
    "                        bms_txt = r16[1].text.strip()\n",
    "                        data[\"bms\"] = bms_txt.split('\\n')[0].strip()\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing profile for {horse_id}: {e}\")\n",
    "            \n",
    "        return data\n",
    "\n",
    "    def get_race_metadata(self, race_id):\n",
    "        \"\"\"\n",
    "        Fetches metadata for a specific race ID from Netkeiba.\n",
    "        Returns dict with: race_name, date, venue, course_type, distance, weather, condition, turn\n",
    "        \"\"\"\n",
    "        url = f\"https://race.netkeiba.com/race/result.html?race_id={race_id}\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "            \n",
    "        data = {\n",
    "            \"race_name\": \"\",\n",
    "            \"date\": \"\",\n",
    "            \"venue\": \"\",\n",
    "            \"course_type\": \"\",\n",
    "            \"distance\": \"\",\n",
    "            \"weather\": \"\",\n",
    "            \"condition\": \"\",\n",
    "            \"turn\": \"\", # New: Dictionary key for turn direction\n",
    "            \"race_id\": race_id\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Race Name\n",
    "            title_elem = soup.select_one(\".RaceName\")\n",
    "            if title_elem:\n",
    "                data[\"race_name\"] = title_elem.text.strip()\n",
    "                \n",
    "            # Date & Venue & Conditions\n",
    "            # <div class=\"RaceData01\">... 2023å¹´1æœˆ5æ—¥ ... 1å›ä¸­å±±1æ—¥ ...</div>\n",
    "            # Content: \"15:35ç™ºèµ° / èŠ1600m (å³ å¤–) / å¤©å€™:æ™´ / é¦¬å ´:è‰¯\"\n",
    "            \n",
    "            rd1 = soup.select_one(\".RaceData01\")\n",
    "            \n",
    "            if rd1:\n",
    "                txt = rd1.text.strip()\n",
    "                \n",
    "                # Weather\n",
    "                if \"å¤©å€™:æ™´\" in txt: data[\"weather\"] = \"æ™´\"\n",
    "                elif \"å¤©å€™:æ›‡\" in txt: data[\"weather\"] = \"æ›‡\"\n",
    "                elif \"å¤©å€™:å°é›¨\" in txt: data[\"weather\"] = \"å°é›¨\"\n",
    "                elif \"å¤©å€™:é›¨\" in txt: data[\"weather\"] = \"é›¨\"\n",
    "                elif \"å¤©å€™:é›ª\" in txt: data[\"weather\"] = \"é›ª\"\n",
    "                \n",
    "                # Condition\n",
    "                if \"é¦¬å ´:è‰¯\" in txt: data[\"condition\"] = \"è‰¯\"\n",
    "                elif \"é¦¬å ´:ç¨\" in txt: data[\"condition\"] = \"ç¨é‡\" # Covers ç¨é‡\n",
    "                elif \"é¦¬å ´:é‡\" in txt: data[\"condition\"] = \"é‡\"\n",
    "                elif \"é¦¬å ´:ä¸è‰¯\" in txt: data[\"condition\"] = \"ä¸è‰¯\"\n",
    "                \n",
    "                # Course & Distance (\"èŠ1600m\")\n",
    "                # Regex for \"èŠ\", \"ãƒ€\", \"éšœ\" followed by digits\n",
    "                match = re.search(r'(èŠ|ãƒ€|éšœ)(\\d+)m', txt)\n",
    "                if match:\n",
    "                    ctype_raw = match.group(1)\n",
    "                    if ctype_raw == \"èŠ\": data[\"course_type\"] = \"èŠ\"\n",
    "                    elif ctype_raw == \"ãƒ€\": data[\"course_type\"] = \"ãƒ€ãƒ¼ãƒˆ\"\n",
    "                    elif ctype_raw == \"éšœ\": data[\"course_type\"] = \"éšœå®³\"\n",
    "                    \n",
    "                    data[\"distance\"] = match.group(2)\n",
    "                \n",
    "                # Turn Direction (\"å³\", \"å·¦\", \"ç›´ç·š\")\n",
    "                # Usually in parentheses like \"(å³)\" or \"(å·¦)\" or \"(èŠ å·¦)\"\n",
    "                if \"å³\" in txt: data[\"turn\"] = \"å³\"\n",
    "                elif \"å·¦\" in txt: data[\"turn\"] = \"å·¦\"\n",
    "                elif \"ç›´ç·š\" in txt: data[\"turn\"] = \"ç›´\"\n",
    "\n",
    "            # Date\n",
    "            # Try finding date in Title or dedicated element\n",
    "            date_elem = soup.select_one(\"dl#RaceList_DateList dd.Active\") \n",
    "            if date_elem:\n",
    "                 # Usually \"1æœˆ5æ—¥(é‡‘)\" - needs Year\n",
    "                 # We can rely on the fact that race_id contains year (2025...)\n",
    "                 # But let's look for YYYYå¹´ in the whole text or title\n",
    "                 pass\n",
    "            \n",
    "            # Fallback Date from Title Tag or Meta\n",
    "            if not data[\"date\"]:\n",
    "                 meta_title = soup.title.text if soup.title else \"\"\n",
    "                 match_date = re.search(r'(\\d{4}å¹´\\d{1,2}æœˆ\\d{1,2}æ—¥)', meta_title)\n",
    "                 if match_date:\n",
    "                     data[\"date\"] = match_date.group(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing metadata for {race_id}: {e}\")\n",
    "            \n",
    "        return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test\n",
    "    scraper = RaceScraper()\n",
    "    print(\"Running test...\")\n",
    "    # Example: Do Deuce (2019105219)\n",
    "    # url = \"https://db.netkeiba.com/horse/2019105219/\"\n",
    "    # print(f\"Fetching {url}\")\n",
    "    df = scraper.get_past_races(\"2019105219\")\n",
    "    if df.empty:\n",
    "        print(\"DF is empty. Checking raw soup for 'db_h_race_results'...\")\n",
    "        soup = scraper._get_soup(f\"https://db.netkeiba.com/horse/result/2019105219/\")\n",
    "        if soup:\n",
    "             t = soup.select_one(\"table.db_h_race_results\")\n",
    "             print(f\"Selector 'table.db_h_race_results' found: {t is not None}\")\n",
    "             if not t:\n",
    "                 print(\"Trying fallback 'table' with 'ç€é †'...\")\n",
    "                 tables = soup.find_all(\"table\")\n",
    "                 found = False\n",
    "                 for i, tbl in enumerate(tables):\n",
    "                     print(f\"Table {i} classes: {tbl.get('class')}\")\n",
    "                     if \"ç€é †\" in tbl.text or \"ç€ é †\" in tbl.text or \"æ—¥ä»˜\" in tbl.text:\n",
    "                         print(\"Found a table with 'ç€é †/æ—¥ä»˜'.\")\n",
    "                         # print(str(tbl)[:200])\n",
    "                         t = tbl\n",
    "                         found = True\n",
    "                         break\n",
    "                 if not found:\n",
    "                     print(\"No table with 'ç€é †' found in soup.\")\n",
    "                     print(\"Soup snippet:\", soup.text[:500])\n",
    "                 else:\n",
    "                    # Retry parsing with found table\n",
    "                     try:\n",
    "                        df = pd.read_html(str(t))[0]\n",
    "                        print(\"Retry DF Head:\")\n",
    "                        print(df.head())\n",
    "                     except Exception as e:\n",
    "                        print(f\"Retry parsing failed: {e}\")\n",
    "        else:\n",
    "            print(\"Soup is None.\")\n",
    "    else:\n",
    "        print(df.head())\n",
    "        print(\"Columns:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2026-01-08: Rich Scraping Logic (JRA/NAR Universal)\n",
    "# This code handles scraping of Race Result + Horse History + Pedigree in one pass.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "from datetime import datetime\n",
    "import urllib.parse\n",
    "import time\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- RaceScraper Helper Class (Embedded) ---\n",
    "class RaceScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "    def _get_soup(self, url):\n",
    "        try:\n",
    "            time.sleep(1) # Be polite\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.encoding = response.apparent_encoding\n",
    "            if response.status_code == 200:\n",
    "                return BeautifulSoup(response.text, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def get_horse_profile(self, horse_id):\n",
    "        \"\"\"\n",
    "        Fetches horse profile to get pedigree (Father, Mother, Grandfather(BMS)).\n",
    "        Returns a dictionary or None.\n",
    "        \"\"\"\n",
    "        url = f\"https://db.netkeiba.com/horse/ped/{horse_id}/\"\n",
    "        soup = self._get_soup(url)\n",
    "        data = {\"father\": \"\", \"mother\": \"\", \"bms\": \"\"}\n",
    "        if not soup: return data\n",
    "\n",
    "        try:\n",
    "            table = soup.select_one(\"table.blood_table\")\n",
    "            if table:\n",
    "                rows = table.find_all(\"tr\")\n",
    "                if len(rows) >= 17:\n",
    "                    # Father: Row 0\n",
    "                    r0 = rows[0].find_all(\"td\")\n",
    "                    if r0:\n",
    "                        txt = r0[0].text.strip()\n",
    "                        data[\"father\"] = txt.split('\\n')[0].strip()\n",
    "\n",
    "                    # Mother & BMS: Row 16\n",
    "                    r16 = rows[16].find_all(\"td\")\n",
    "                    if len(r16) >= 2:\n",
    "                        m_txt = r16[0].text.strip()\n",
    "                        data[\"mother\"] = m_txt.split('\\n')[0].strip()\n",
    "                        bms_txt = r16[1].text.strip()\n",
    "                        data[\"bms\"] = bms_txt.split('\\n')[0].strip()\n",
    "        except Exception as e:\n",
    "            pass # Silent fail for profile\n",
    "        return data\n",
    "\n",
    "    def extract_run_style(self, passing_str):\n",
    "        if not isinstance(passing_str, str): return 3\n",
    "        try:\n",
    "            cleaned = re.sub(r'[^0-9-]', '', passing_str)\n",
    "            parts = [int(p) for p in cleaned.split('-') if p]\n",
    "            if not parts: return 3\n",
    "            first_corner = parts[0]\n",
    "            if first_corner == 1: return 1 # Nige\n",
    "            elif first_corner <= 4: return 2 # Senkou\n",
    "            elif first_corner <= 9: return 3 # Sashi\n",
    "            else: return 4 # Oikomi\n",
    "        except: return 3\n",
    "\n",
    "    def get_past_races(self, horse_id, current_race_date, n_samples=5):\n",
    "        \"\"\"\n",
    "        Fetches past n_samples race results.\n",
    "        Filters out races AFTER current_race_date.\n",
    "        Returns a DataFrame.\n",
    "        \"\"\"\n",
    "        url = f\"https://db.netkeiba.com/horse/result/{horse_id}/\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup: return pd.DataFrame()\n",
    "\n",
    "        table = soup.select_one(\"table.db_h_race_results\")\n",
    "        if not table:\n",
    "             tables = soup.find_all(\"table\")\n",
    "             for t in tables:\n",
    "                 if \"ç€é †\" in t.text:\n",
    "                     table = t\n",
    "                     break\n",
    "        if not table: return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            df = pd.read_html(io.StringIO(str(table)))[0]\n",
    "            df = df.dropna(how='all')\n",
    "            # Normalize columns\n",
    "            df.columns = df.columns.astype(str).str.replace(r'\\s+', '', regex=True)\n",
    "            \n",
    "            # Date Parsing\n",
    "            if 'æ—¥ä»˜' in df.columns:\n",
    "                df['date_obj'] = pd.to_datetime(df['æ—¥ä»˜'], format='%Y/%m/%d', errors='coerce')\n",
    "                df = df.dropna(subset=['date_obj'])\n",
    "                \n",
    "                # Filter past races only\n",
    "                if isinstance(current_race_date, str):\n",
    "                    current_date = pd.to_datetime(current_race_date)\n",
    "                else:\n",
    "                    current_date = pd.to_datetime(current_race_date)\n",
    "                    \n",
    "                df = df[df['date_obj'] < current_date]\n",
    "                df = df.sort_values('date_obj', ascending=False)\n",
    "            \n",
    "            if df.empty: return df\n",
    "\n",
    "            # Limit to N\n",
    "            df = df.head(n_samples)\n",
    "\n",
    "            # Extract Run Style\n",
    "            if 'é€šé' in df.columns:\n",
    "                df['run_style_val'] = df['é€šé'].apply(self.extract_run_style)\n",
    "            else:\n",
    "                df['run_style_val'] = 3\n",
    "\n",
    "            # Column Mapping\n",
    "            column_map = {\n",
    "                'æ—¥ä»˜': 'date', 'é–‹å‚¬': 'venue', 'å¤©æ°—': 'weather', 'ãƒ¬ãƒ¼ã‚¹å': 'race_name',\n",
    "                'ç€é †': 'rank', 'æ ç•ª': 'waku', 'é¦¬ç•ª': 'umaban', 'é¨æ‰‹': 'jockey',\n",
    "                'æ–¤é‡': 'weight_carried', 'é¦¬å ´': 'condition', 'ã‚¿ã‚¤ãƒ ': 'time',\n",
    "                'ç€å·®': 'margin', 'ä¸Šã‚Š': 'last_3f', 'é€šé': 'passing', 'é¦¬ä½“é‡': 'horse_weight',\n",
    "                'run_style_val': 'run_style', 'å˜å‹': 'odds', 'ã‚ªãƒƒã‚º': 'odds', 'è·é›¢': 'raw_distance'\n",
    "            }\n",
    "            df.rename(columns=column_map, inplace=True)\n",
    "            \n",
    "            # Parse Distance/Course\n",
    "            if 'raw_distance' in df.columns:\n",
    "                def parse_dist(x):\n",
    "                    if not isinstance(x, str): return None, None\n",
    "                    surf = None; dist = None\n",
    "                    if 'èŠ' in x: surf = 'èŠ'\n",
    "                    elif 'ãƒ€' in x: surf = 'ãƒ€'\n",
    "                    elif 'éšœ' in x: surf = 'éšœ'\n",
    "                    match = re.search(r'(\\d+)', x)\n",
    "                    if match: dist = int(match.group(1))\n",
    "                    return surf, dist\n",
    "                \n",
    "                parsed = df['raw_distance'].apply(parse_dist)\n",
    "                df['course_type'] = parsed.apply(lambda x: x[0])\n",
    "                df['distance'] = parsed.apply(lambda x: x[1])\n",
    "            else:\n",
    "                df['course_type'] = None; df['distance'] = None\n",
    "\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# --- Main Rich Scraper Function ---\n",
    "def scrape_race_rich(url, existing_race_ids=None, max_retries=3):\n",
    "    \"\"\"\n",
    "    Scrapes Race + History + Pedigree in one go.\n",
    "    \"\"\"\n",
    "    print(f\"  ãƒ¬ãƒ¼ã‚¹è§£æä¸­: {url}\")\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    \n",
    "    # 1. Fetch Race Page\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=15)\n",
    "        resp.encoding = 'EUC-JP'\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        \n",
    "        # --- Metadata (Date, Venue, Race Name, etc) ---\n",
    "        # (This part reuses logic from original scrape_jra_race)\n",
    "        h1_elem = soup.select_one(\"div.header_line h1 .txt\")\n",
    "        full_text = h1_elem.text.strip() if h1_elem else (soup.h1.text.strip() if soup.h1 else \"\")\n",
    "        \n",
    "        date_text = \"\"; venue_text = \"\"; kai = \"01\"; day = \"01\"; r_num = \"10\"\n",
    "        match_date = re.search(r'(\\d{4}å¹´\\d{1,2}æœˆ\\d{1,2}æ—¥)', full_text)\n",
    "        if match_date: date_text = match_date.group(1)\n",
    "        \n",
    "        venues_str = \"æœ­å¹Œ|å‡½é¤¨|ç¦å³¶|æ–°æ½Ÿ|æ±äº¬|ä¸­å±±|ä¸­äº¬|äº¬éƒ½|é˜ªç¥|å°å€‰\"\n",
    "        match_meta = re.search(rf'(\\d+)å›({venues_str})(\\\\d+)æ—¥', full_text)\n",
    "        if match_meta:\n",
    "            kai = f\"{int(match_meta.group(1)):02}\"\n",
    "            venue_text = match_meta.group(2)\n",
    "            day = f\"{int(match_meta.group(3)):02}\"\n",
    "            \n",
    "        match_race = re.search(r'(\\d+)ãƒ¬ãƒ¼ã‚¹', full_text)\n",
    "        if match_race: r_num = f\"{int(match_race.group(1)):02}\"\n",
    "        \n",
    "        place_map = {\n",
    "            \"æœ­å¹Œ\": \"01\", \"å‡½é¤¨\": \"02\", \"ç¦å³¶\": \"03\", \"æ–°æ½Ÿ\": \"04\", \"æ±äº¬\": \"05\",\n",
    "            \"ä¸­å±±\": \"06\", \"ä¸­äº¬\": \"07\", \"äº¬éƒ½\": \"08\", \"é˜ªç¥\": \"09\", \"å°å€‰\": \"10\"\n",
    "        }\n",
    "        p_code = place_map.get(venue_text, \"00\")\n",
    "        year = date_text[:4] if date_text else \"2025\"\n",
    "        \n",
    "        race_id = f\"{year}{p_code}{kai}{day}{r_num}\"\n",
    "        \n",
    "        # SKIP Check\n",
    "        if existing_race_ids and race_id in existing_race_ids:\n",
    "            return None\n",
    "\n",
    "        # Basic Race Info\n",
    "        race_name = soup.select_one(\".race_name\").text.strip() if soup.select_one(\".race_name\") else \"\"\n",
    "        header_text = soup.select_one(\"div.header_line\").text if soup.select_one(\"div.header_line\") else full_text\n",
    "        \n",
    "        # Course/Dist\n",
    "        course_type = \"\"; distance = \"\"\n",
    "        dt_match = re.search(r'(èŠ|ãƒ€|ãƒ€ãƒ¼ãƒˆ|éšœå®³)[^0-9]*(\\d+)', header_text)\n",
    "        if dt_match:\n",
    "            c = dt_match.group(1)\n",
    "            course_type = 'èŠ' if 'èŠ' in c else ('ãƒ€ãƒ¼ãƒˆ' if 'ãƒ€' in c else 'éšœå®³')\n",
    "            distance = dt_match.group(2)\n",
    "            \n",
    "        # Weather/Condition\n",
    "        weather = \"\"; condition = \"\"; rotation = \"\"\n",
    "        w_match = re.search(r'å¤©å€™\\s*[:ï¼š]\\s*(\\S+)', soup.text)\n",
    "        if w_match: weather = w_match.group(1)\n",
    "        \n",
    "        c_match = re.search(r'(?:èŠ|ãƒ€ãƒ¼ãƒˆ)\\s*[:ï¼š]\\s*(\\S+)', soup.text)\n",
    "        if c_match: condition = c_match.group(1)\n",
    "        \n",
    "        if \"å³\" in header_text: rotation = \"å³\"\n",
    "        elif \"å·¦\" in header_text: rotation = \"å·¦\"\n",
    "        elif \"ç›´\" in header_text: rotation = \"ç›´ç·š\"\n",
    "\n",
    "        # --- Parse Result Table ---\n",
    "        tables = soup.find_all('table')\n",
    "        target_table = None\n",
    "        for t in tables:\n",
    "            if \"ç€é †\" in t.text and \"é¦¬å\" in t.text:\n",
    "                target_table = t\n",
    "                break\n",
    "        \n",
    "        if not target_table: return None\n",
    "        \n",
    "        rows = target_table.find_all('tr')\n",
    "        race_scraper = RaceScraper()\n",
    "        \n",
    "        data_list = []\n",
    "        \n",
    "        # Pre-convert date for filtering\n",
    "        d_obj = pd.to_datetime(date_text, format='%Yå¹´%mæœˆ%dæ—¥') if date_text else datetime.now()\n",
    "        \n",
    "        print(f\"    Fetching details for {len(rows)-1} horses...\")\n",
    "        \n",
    "        for row in rows:\n",
    "            if row.find('th'): continue # Skip header\n",
    "            cells = row.find_all('td')\n",
    "            if not cells: continue\n",
    "            \n",
    "            # Basic info\n",
    "            rank = cells[0].text.strip()\n",
    "            waku = \"\"\n",
    "            if cells[1].find('img'): \n",
    "                alt = cells[1].find('img').get('alt', '')\n",
    "                m = re.search(r'æ (\\d+)', alt)\n",
    "                waku = m.group(1) if m else alt\n",
    "            umaban = cells[2].text.strip()\n",
    "            horse_name_elem = cells[3].find('a')\n",
    "            horse_name = cells[3].text.strip()\n",
    "            horse_id = \"\"\n",
    "            if horse_name_elem and 'href' in horse_name_elem.attrs:\n",
    "                hm = re.search(r'/horse/(\\d+)', horse_name_elem['href'])\n",
    "                if hm: horse_id = hm.group(1)\n",
    "            \n",
    "            jockey = cells[6].text.strip()\n",
    "            time_val = cells[7].text.strip()\n",
    "            # ... other basic fields\n",
    "            \n",
    "            # --- Rich Fetching (History & Pedigree) ---\n",
    "            blood_data = {\"father\": \"\", \"mother\": \"\", \"bms\": \"\"}\n",
    "            past_data = {}\n",
    "            \n",
    "            if horse_id:\n",
    "                # 1. Pedigree\n",
    "                blood_data = race_scraper.get_horse_profile(horse_id)\n",
    "                \n",
    "                # 2. History\n",
    "                df_past = race_scraper.get_past_races(horse_id, d_obj, n_samples=5)\n",
    "                \n",
    "                # Flatten History\n",
    "                for i in range(5):\n",
    "                    prefix = f\"past_{i+1}\"\n",
    "                    if i < len(df_past):\n",
    "                        r = df_past.iloc[i]\n",
    "                        past_data[f\"{prefix}_date\"] = r.get('date', '')\n",
    "                        past_data[f\"{prefix}_rank\"] = r.get('rank', '')\n",
    "                        past_data[f\"{prefix}_time\"] = r.get('time', '')\n",
    "                        past_data[f\"{prefix}_run_style\"] = r.get('run_style', '')\n",
    "                        past_data[f\"{prefix}_race_name\"] = r.get('race_name', '')\n",
    "                        past_data[f\"{prefix}_last_3f\"] = r.get('last_3f', '')\n",
    "                        past_data[f\"{prefix}_horse_weight\"] = r.get('horse_weight', '')\n",
    "                        past_data[f\"{prefix}_jockey\"] = r.get('jockey', '')\n",
    "                        past_data[f\"{prefix}_condition\"] = r.get('condition', '')\n",
    "                        past_data[f\"{prefix}_odds\"] = r.get('odds', '')\n",
    "                        past_data[f\"{prefix}_weather\"] = r.get('weather', '')\n",
    "                        past_data[f\"{prefix}_distance\"] = r.get('distance', '')\n",
    "                        past_data[f\"{prefix}_course_type\"] = r.get('course_type', '')\n",
    "                    else:\n",
    "                        # Fill Empty\n",
    "                        for col in ['date','rank','time','run_style','race_name','last_3f','horse_weight','jockey','condition','odds','weather','distance','course_type']:\n",
    "                            past_data[f\"{prefix}_{col}\"] = \"\"\n",
    "\n",
    "                # Be polite between horses\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            # Combine\n",
    "            row_dict = {\n",
    "                \"æ—¥ä»˜\": date_text, \"ä¼šå ´\": venue_text, \"ãƒ¬ãƒ¼ã‚¹ç•ªå·\": f\"{r_num}R\", \"ãƒ¬ãƒ¼ã‚¹å\": race_name, \"é‡è³\": \"\",\n",
    "                \"ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—\": course_type, \"è·é›¢\": distance, \"å›ã‚Š\": rotation, \"å¤©å€™\": weather, \"é¦¬å ´çŠ¶æ…‹\": condition,\n",
    "                \"ç€é †\": rank, \"æ \": waku, \"é¦¬ç•ª\": umaban, \"é¦¬å\": horse_name, \n",
    "                \"æ€§é½¢\": cells[4].text.strip(), \"æ–¤é‡\": cells[5].text.strip(), \"é¨æ‰‹\": jockey, \n",
    "                \"ã‚¿ã‚¤ãƒ \": time_val, \"ç€å·®\": cells[8].text.strip(), \"äººæ°—\": cells[13].text.strip(), \n",
    "                \"å˜å‹ã‚ªãƒƒã‚º\": cells[14].text.strip() if len(cells)>14 else \"0.0\", \n",
    "                \"å¾Œ3F\": cells[10].text.strip(), \"å©èˆ\": cells[12].text.strip(), \n",
    "                \"é¦¬ä½“é‡(å¢—æ¸›)\": cells[11].text.strip(),\n",
    "                \"race_id\": race_id, \"horse_id\": horse_id,\n",
    "                **blood_data,\n",
    "                **past_data\n",
    "            }\n",
    "            data_list.append(row_dict)\n",
    "            \n",
    "        df = pd.DataFrame(data_list)\n",
    "        \n",
    "        # Enforce User-Specified Column Order\n",
    "        ordered_columns = [\n",
    "            \"æ—¥ä»˜\", \"ä¼šå ´\", \"ãƒ¬ãƒ¼ã‚¹ç•ªå·\", \"ãƒ¬ãƒ¼ã‚¹å\", \"é‡è³\", \"ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—\", \"è·é›¢\", \"å›ã‚Š\", \"å¤©å€™\", \"é¦¬å ´çŠ¶æ…‹\",\n",
    "            \"ç€é †\", \"æ \", \"é¦¬ç•ª\", \"é¦¬å\", \"æ€§é½¢\", \"æ–¤é‡\", \"é¨æ‰‹\", \"ã‚¿ã‚¤ãƒ \", \"ç€å·®\", \"äººæ°—\", \"å˜å‹ã‚ªãƒƒã‚º\",\n",
    "            \"å¾Œ3F\", \"å©èˆ\", \"é¦¬ä½“é‡(å¢—æ¸›)\", \"race_id\", \"horse_id\"\n",
    "        ]\n",
    "        # rich data columns\n",
    "        for i in range(1, 6):\n",
    "            p = f\"past_{i}\"\n",
    "            ordered_columns.extend([\n",
    "                f\"{p}_date\", f\"{p}_rank\", f\"{p}_time\", f\"{p}_run_style\", f\"{p}_race_name\",\n",
    "                f\"{p}_last_3f\", f\"{p}_horse_weight\", f\"{p}_jockey\", f\"{p}_condition\",\n",
    "                f\"{p}_odds\", f\"{p}_weather\", f\"{p}_distance\", f\"{p}_course_type\"\n",
    "            ])\n",
    "        ordered_columns.extend([\"father\", \"mother\", \"bms\"])\n",
    "        \n",
    "        # Add missing cols with empty string, remove extras (if any/optional)\n",
    "        # reindex handles this safely\n",
    "        df_ordered = df.reindex(columns=ordered_columns, fill_value=\"\")\n",
    "        \n",
    "        return df_ordered\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping race {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Year/Month Iteration Logic (Scrape Year Rich) ---\n",
    "def scrape_jra_year_rich(year_str, start_date=None, end_date=None, save_callback=None, existing_race_ids=None):\n",
    "    # Parameter Map for Monthly Results\n",
    "    JRA_MONTH_PARAMS = {\n",
    "        \"2026\": { \"01\": \"E4\", \"02\": \"B2\", \"03\": \"80\", \"04\": \"4E\", \"05\": \"1C\", \"06\": \"EA\", \"07\": \"B8\", \"08\": \"86\", \"09\": \"54\", \"10\": \"22\", \"11\": \"F0\", \"12\": \"BE\" },\n",
    "        \"2025\": { \"01\": \"3F\", \"02\": \"0D\", \"03\": \"DB\", \"04\": \"A9\", \"05\": \"77\", \"06\": \"45\", \"07\": \"13\", \"08\": \"E1\", \"09\": \"AF\", \"10\": \"1E\", \"11\": \"EC\", \"12\": \"D3\" },\n",
    "        \"2024\": { \"01\": \"B3\", \"02\": \"81\", \"03\": \"4F\", \"04\": \"1D\", \"05\": \"EB\", \"06\": \"B9\", \"07\": \"87\", \"08\": \"55\", \"09\": \"23\", \"10\": \"92\", \"11\": \"60\", \"12\": \"2E\" },\n",
    "        \"2023\": { \"01\": \"27\", \"02\": \"F5\", \"03\": \"C3\", \"04\": \"91\", \"05\": \"5F\", \"06\": \"2D\", \"07\": \"FB\", \"08\": \"C9\", \"09\": \"97\", \"10\": \"06\", \"11\": \"D4\", \"12\": \"A2\" },\n",
    "        \"2022\": { \"01\": \"9B\", \"02\": \"69\", \"03\": \"37\", \"04\": \"05\", \"05\": \"D3\", \"06\": \"A1\", \"07\": \"6F\", \"08\": \"3D\", \"09\": \"0B\", \"10\": \"7A\", \"11\": \"48\", \"12\": \"16\" },\n",
    "        \"2021\": { \"01\": \"0F\", \"02\": \"DD\", \"03\": \"AB\", \"04\": \"79\", \"05\": \"47\", \"06\": \"15\", \"07\": \"E3\", \"08\": \"B1\", \"09\": \"7F\", \"10\": \"EE\", \"11\": \"BC\", \"12\": \"8A\" },\n",
    "        \"2020\": { \"01\": \"83\", \"02\": \"51\", \"03\": \"1F\", \"04\": \"ED\", \"05\": \"BB\", \"06\": \"89\", \"07\": \"57\", \"08\": \"25\", \"09\": \"F3\", \"10\": \"62\", \"11\": \"30\", \"12\": \"FE\" }\n",
    "    }\n",
    "    \n",
    "    if year_str not in JRA_MONTH_PARAMS:\n",
    "        print(f\"å¹´åº¦ {year_str} ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\")\n",
    "        return\n",
    "\n",
    "    params = JRA_MONTH_PARAMS[year_str]\n",
    "    base_url = \"https://www.jra.go.jp/JRADB/accessS.html\"\n",
    "\n",
    "    # Determine months to iterate\n",
    "    start_m = 1\n",
    "    end_m = 12\n",
    "\n",
    "    if start_date:\n",
    "        start_m = start_date.month\n",
    "    if end_date:\n",
    "        end_m = end_date.month\n",
    "\n",
    "    # Cap at Today\n",
    "    from datetime import date\n",
    "    today = date.today()\n",
    "\n",
    "    if end_date:\n",
    "        actual_end_date = min(end_date, today)\n",
    "    else:\n",
    "        actual_end_date = today\n",
    "\n",
    "    print(f\"=== JRA ä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ {year_str} (ãƒªãƒƒãƒãƒ¢ãƒ¼ãƒ‰) ===\")\n",
    "    print(f\"æœŸé–“: {start_date or 'Start'} - {actual_end_date}\")\n",
    "    \n",
    "    if int(year_str) == today.year:\n",
    "        end_m = min(end_m, today.month)\n",
    "\n",
    "    total_processed = 0\n",
    "\n",
    "    for m in range(start_m, end_m + 1):\n",
    "        month = f\"{m:02}\"\n",
    "        if month not in params:\n",
    "            continue\n",
    "\n",
    "        suffix = params[month]\n",
    "        try:\n",
    "            ym = int(year_str + month)\n",
    "            prefix = \"pw01skl00\" if ym >= 202512 else \"pw01skl10\"\n",
    "        except:\n",
    "            prefix = \"pw01skl10\"\n",
    "\n",
    "        cname = f\"{prefix}{year_str}{month}/{suffix}\"\n",
    "\n",
    "        print(f\"\\nğŸ“… {year_str}/{month} ã‚’å–å¾—ä¸­...\")\n",
    "\n",
    "        try:\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "            }\n",
    "            response = requests.post(base_url, data={\"cname\": cname}, headers=headers, timeout=15)\n",
    "            response.encoding = 'cp932'\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(f\"âŒ {cname} ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ (Status {response.status_code})\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            race_cnames = []\n",
    "            links = soup.find_all('a')\n",
    "            for link in links:\n",
    "                onclick = link.get('onclick', '')\n",
    "                match = re.search(r\"doAction\\('[^']+',\\s*'([^']+)'\\)\", onclick)\n",
    "                if match:\n",
    "                    c = match.group(1)\n",
    "                    if c.startswith('pw01srl'):\n",
    "                        race_cnames.append(c)\n",
    "\n",
    "            race_cnames = sorted(list(set(race_cnames)))\n",
    "            print(f\"  {len(race_cnames)} é–‹å‚¬æ—¥ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ\")\n",
    "\n",
    "            for day_cname in tqdm(race_cnames, desc=f\"  {year_str}/{month}\", leave=False):\n",
    "                resp_day = requests.post(base_url, data={\"cname\": day_cname}, headers=headers, timeout=15)\n",
    "                resp_day.encoding = 'cp932'\n",
    "                soup_day = BeautifulSoup(resp_day.text, 'html.parser')\n",
    "\n",
    "                race_list_items = []\n",
    "                all_anchors = soup_day.find_all('a')\n",
    "                for a in all_anchors:\n",
    "                    onclick = a.get('onclick', '')\n",
    "                    match_sde = re.search(r\"doAction\\s*\\(\\s*['\\\"][^'\\\"]+['\\\"]\\s*,\\s*['\\\"](pw01sde[^'\\\"]+)['\\\"]\\s*\\)\", onclick)\n",
    "                    href = a.get('href', '')\n",
    "\n",
    "                    final_url = \"\"\n",
    "                    if match_sde:\n",
    "                        final_url = f\"{base_url}?CNAME={match_sde.group(1)}\"\n",
    "                    elif 'pw01sde' in href:\n",
    "                         if 'CNAME=' in href:\n",
    "                             final_url = urllib.parse.urljoin(base_url, href)\n",
    "                         else:\n",
    "                             final_url = urllib.parse.urljoin(base_url, href)\n",
    "\n",
    "                    if final_url:\n",
    "                        race_list_items.append(final_url)\n",
    "\n",
    "                unique_races = sorted(list(set(race_list_items)))\n",
    "\n",
    "                daily_data = []\n",
    "\n",
    "                for r_link in unique_races:\n",
    "                    # Fetch with Rich Scraper\n",
    "                    df = scrape_race_rich(r_link, existing_race_ids=existing_race_ids)\n",
    "\n",
    "                    if df is not None and not df.empty:\n",
    "                        daily_data.append(df)\n",
    "                        total_processed += 1\n",
    "                    \n",
    "                    # Rate limiting\n",
    "                    time.sleep(1.0) \n",
    "                \n",
    "                # Save daily batch\n",
    "                if daily_data and save_callback:\n",
    "                    df_day = pd.concat(daily_data, ignore_index=True)\n",
    "                    save_callback(df_day) \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {month}æœˆã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "            \n",
    "    print(\"å®Œäº†ã—ã¾ã—ãŸã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAR ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ (Smart Differential)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date, timedelta\n",
    "import calendar\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "def check_integrity(df_chunk):\n",
    "    CRITICAL_COLS = ['race_id', 'horse_id', 'horse_name', 'jockey', 'date', 'rank']\n",
    "    if not all(col in df_chunk.columns for col in CRITICAL_COLS):\n",
    "        return False\n",
    "    if df_chunk[CRITICAL_COLS].isnull().any().any():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def run_nar_scraping(year, start_month=1, end_month=12, save_dir='data/raw', target_csv='database_nar.csv', target_id_csv='race_ids_nar.csv'):\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    s_date = date(int(year), int(start_month), 1)\n",
    "    last_day_e = calendar.monthrange(int(year), int(end_month))[1]\n",
    "    e_date = date(int(year), int(end_month), last_day_e)\n",
    "    today = date.today()\n",
    "    if e_date > today: e_date = today\n",
    "    \n",
    "    save_path = os.path.join(save_dir, target_csv)\n",
    "    id_path = os.path.join(save_dir, target_id_csv)\n",
    "    \n",
    "    print(f'{year}å¹´ã®NARãƒ‡ãƒ¼ã‚¿ã‚’ {s_date} ã‹ã‚‰ {e_date} ã¾ã§å–å¾—ã—ã¾ã™(å·®åˆ†æ›´æ–°)...')\n",
    "    print(f'ãƒ‡ãƒ¼ã‚¿: {save_path}')\n",
    "    print(f'IDç®¡ç†: {id_path}')\n",
    "    \n",
    "    # --- IDãƒªã‚¹ãƒˆåˆæœŸåŒ– ---\n",
    "    verified_ids = set()\n",
    "    if os.path.exists(id_path):\n",
    "        try:\n",
    "            df_ids = pd.read_csv(id_path, dtype=str)\n",
    "            if 'race_id' in df_ids.columns:\n",
    "                verified_ids = set(df_ids['race_id'].unique())\n",
    "            print(f\"ğŸ“– IDãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(verified_ids)}ä»¶\")\n",
    "        except: pass\n",
    "        \n",
    "    if os.path.exists(save_path):\n",
    "        print(\"ğŸ” æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ä¸­...\")\n",
    "        try:\n",
    "            # Chunk/All read\n",
    "            df_exist = pd.read_csv(save_path, dtype=str)\n",
    "            groups = df_exist.groupby('race_id')\n",
    "            new_valid = []\n",
    "            for rid, group in tqdm(groups, desc='Checking DB'):\n",
    "                if rid in verified_ids: continue\n",
    "                if check_integrity(group):\n",
    "                    verified_ids.add(rid)\n",
    "                    new_valid.append(rid)\n",
    "            \n",
    "            if new_valid:\n",
    "                print(f\"âœ¨ {len(new_valid)}ä»¶ã®æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ã‚’IDãƒªã‚¹ãƒˆã«è¿½åŠ ã—ã¾ã™\")\n",
    "                mode = 'a' if os.path.exists(id_path) else 'w'\n",
    "                pd.DataFrame({'race_id': new_valid}).to_csv(id_path, mode=mode, header=(not os.path.exists(id_path)), index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Warn read DB: {e}\")\n",
    "            \n",
    "    # --- Main Loop ---\n",
    "    def safe_append_csv(df_chunk, path):\n",
    "        if not os.path.exists(path):\n",
    "            df_chunk.to_csv(path, index=False)\n",
    "        else:\n",
    "            try:\n",
    "                existing_cols = pd.read_csv(path, nrows=0).columns.tolist()\n",
    "                df_aligned = df_chunk.reindex(columns=existing_cols)\n",
    "                df_aligned.to_csv(path, mode='a', header=False, index=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Save Error: {e}\")\n",
    "    \n",
    "    for m in range(int(start_month), int(end_month) + 1):\n",
    "        m_start = date(int(year), m, 1)\n",
    "        m_last = calendar.monthrange(int(year), m)[1]\n",
    "        m_end = date(int(year), m, m_last)\n",
    "        if m_end < s_date or m_start > e_date: continue\n",
    "        curr_s, curr_e = max(m_start, s_date), min(m_end, e_date)\n",
    "        if curr_s > curr_e: continue\n",
    "        \n",
    "        days = []\n",
    "        c = curr_s\n",
    "        while c <= curr_e:\n",
    "            days.append(c)\n",
    "            c += timedelta(days=1)\n",
    "        \n",
    "        print(f'\\nğŸ“… {year}/{m:02} ã‚’å‡¦ç†ä¸­...')\n",
    "        for d in tqdm(days, desc=f'  {year}/{m:02}'):\n",
    "            d_str = d.strftime('%Y%m%d')\n",
    "            url = f'https://nar.netkeiba.com/top/race_list_sub.html?kaisai_date={d_str}'\n",
    "            daily_data_to_save = []\n",
    "            daily_ids_to_save = []\n",
    "            \n",
    "            try:\n",
    "                 time.sleep(0.5)\n",
    "                 headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "                 resp = requests.get(url, headers=headers)\n",
    "                 resp.encoding = 'EUC-JP'\n",
    "                 soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "                 links = soup.select('a[href*=\"race/result.html\"]')\n",
    "                 \n",
    "                 if links:\n",
    "                     for link in links:\n",
    "                         href = link.get('href')\n",
    "                         if href.startswith('../'): full_url = f'https://nar.netkeiba.com/{href.replace(\"../\", \"\")}'\n",
    "                         elif href.startswith('http'): full_url = href\n",
    "                         else: full_url = f'https://nar.netkeiba.com{href}'\n",
    "                         \n",
    "                         # Extract ID? NAR IDs usually in URL?\n",
    "                         # NAR URL format often: race_id=xxxxxxxxxxxx\n",
    "                         rid_match = re.search(r'race_id=(\\d+)', full_url)\n",
    "                         if rid_match:\n",
    "                             rid = rid_match.group(1)\n",
    "                             if rid in verified_ids:\n",
    "                                 continue\n",
    "                             \n",
    "                             # Scrape\n",
    "                             try:\n",
    "                                 df = scrape_race_rich(full_url, existing_race_ids=None)\n",
    "                                 if df is not None and not df.empty:\n",
    "                                     if check_integrity(df):\n",
    "                                         daily_data_to_save.append(df)\n",
    "                                         daily_ids_to_save.append(rid)\n",
    "                                         verified_ids.add(rid)\n",
    "                                 time.sleep(1)\n",
    "                             except: pass\n",
    "                 \n",
    "                 if daily_data_to_save:\n",
    "                     df_day = pd.concat(daily_data_to_save, ignore_index=True)\n",
    "                     safe_append_csv(df_day, save_path)\n",
    "                     \n",
    "                     mode = 'a' if os.path.exists(id_path) else 'w'\n",
    "                     pd.DataFrame({'race_id': daily_ids_to_save}).to_csv(id_path, mode=mode, header=(not os.path.exists(id_path)), index=False)\n",
    "                     \n",
    "                     if len(daily_ids_to_save) > 0:\n",
    "                         print(f\"    Saved {len(daily_ids_to_save)} new races.\")\n",
    "                     \n",
    "                     del daily_data_to_save\n",
    "                     del df_day\n",
    "                     gc.collect()\n",
    "            \n",
    "            except Exception as e_day:\n",
    "                print(f'  Err {d}: {e_day}')\n",
    "    \n",
    "    print('å®Œäº†ã—ã¾ã—ãŸã€‚')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯\n",
    "if YEAR:\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    run_nar_scraping(YEAR, START_MONTH, END_MONTH, save_dir=SAVE_DIR, target_csv=TARGET_CSV, target_id_csv=TARGET_ID_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€çµ‚æ•´ç†\n",
    "import pandas as pd\n",
    "import os\n",
    "csv_path = os.path.join(SAVE_DIR, TARGET_CSV)\n",
    "if os.path.exists(csv_path):\n",
    "    try:\n",
    "        df_final = pd.read_csv(csv_path, dtype=str)\n",
    "        if 'race_id' in df_final.columns and 'horse_id' in df_final.columns:\n",
    "            df_final.drop_duplicates(subset=['race_id', 'horse_id'], keep='last', inplace=True)\n",
    "        df_final.to_csv(save_path, index=False)\n",
    "        print('å®Œäº†')\n",
    "    except Exception: pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}