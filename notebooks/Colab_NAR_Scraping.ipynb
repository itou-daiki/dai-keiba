{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèá NAR ÂÖ®„É¨„Éº„ÇπÂèñÂæó\n",
    "‰ª•‰∏ã„ÅÆË®≠ÂÆöÂ§âÊï∞„ÇíÂ§âÊõ¥„Åó„Å¶ÂÆüË°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇNARÔºàÂú∞ÊñπÁ´∂È¶¨Ôºâ„ÅÆ„Éá„Éº„Çø„ÇíÊó•‰ªòÈ†Ü„Å´ÂèñÂæó„Åó„Åæ„Åô„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class RaceScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "    def _get_soup(self, url):\n",
    "        try:\n",
    "            time.sleep(1) # Be polite\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.encoding = response.apparent_encoding\n",
    "            if response.status_code == 200:\n",
    "                return BeautifulSoup(response.text, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def get_past_races(self, horse_id, n_samples=5):\n",
    "        \"\"\"\n",
    "        Fetches past n_samples race results for a given horse_id from netkeiba db.\n",
    "        Returns a DataFrame of past races.\n",
    "        \"\"\"\n",
    "        url = f\"https://db.netkeiba.com/horse/result/{horse_id}/\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # The results are usually in a table with class \"db_h_race_results\"\n",
    "        table = soup.select_one(\"table.db_h_race_results\")\n",
    "        if not table:\n",
    "            # Try to find any table with \"ÁùÄÈ†Ü\"\n",
    "            tables = soup.find_all(\"table\")\n",
    "            for t in tables:\n",
    "                if \"ÁùÄÈ†Ü\" in t.text:\n",
    "                    table = t\n",
    "                    break\n",
    "        \n",
    "        if not table:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Parse table\n",
    "        # We need to manually parse to get clean data and handle links if needed (though for past data, text is mostly fine)\n",
    "        # pd.read_html is easier for the table\n",
    "        try:\n",
    "            df = pd.read_html(io.StringIO(str(table)))[0]\n",
    "            \n",
    "            # Basic cleaning\n",
    "            df = df.dropna(how='all')\n",
    "            \n",
    "            # The columns in db.netkeiba are roughly:\n",
    "            # Êó•‰ªò, ÈñãÂÇ¨, Â§©Ê∞ó, R, „É¨„Éº„ÇπÂêç, Êò†ÂÉè, È†≠Êï∞, Êû†Áï™, ... ÁùÄÈ†Ü, ... ÈÄöÈÅé, ...\n",
    "            \n",
    "            # We want to keep: Date, Race Name, Course info, Rank, Time, Passing (Style)\n",
    "            \n",
    "            # Normalize column names (remove spaces/newlines)\n",
    "            df.columns = df.columns.astype(str).str.replace(r'\\s+', '', regex=True)\n",
    "\n",
    "            # Filter rows that look like actual races (Date column exists)\n",
    "            if 'Êó•‰ªò' in df.columns:\n",
    "                df['date_obj'] = pd.to_datetime(df['Êó•‰ªò'], format='%Y/%m/%d', errors='coerce')\n",
    "                df = df.dropna(subset=['date_obj'])\n",
    "                df = df.sort_values('date_obj', ascending=False)\n",
    "                \n",
    "            # Take top N\n",
    "            if n_samples:\n",
    "                df = df.head(n_samples)\n",
    "            \n",
    "            # Process Run Style (Leg Type)\n",
    "            if 'ÈÄöÈÅé' in df.columns:\n",
    "                df['run_style_val'] = df['ÈÄöÈÅé'].apply(self.extract_run_style)\n",
    "            else:\n",
    "                df['run_style_val'] = 3 # Unknown\n",
    "\n",
    "            # Extract/Rename Columns\n",
    "            # We want: Êó•‰ªò, ÈñãÂÇ¨, Â§©Ê∞ó, R, „É¨„Éº„ÇπÂêç, Êò†ÂÉè, È†≠Êï∞, Êû†Áï™, ... ÁùÄÈ†Ü, ... ÈÄöÈÅé, ...\n",
    "            # Important: '‰∏ä„Çä' (3F), 'È¶¨‰ΩìÈáç', 'È®éÊâã'\n",
    "            \n",
    "            # Map standard columns if they exist\n",
    "            column_map = {\n",
    "                'Êó•‰ªò': 'date',\n",
    "                'ÈñãÂÇ¨': 'venue',\n",
    "                'Â§©Ê∞ó': 'weather',\n",
    "                '„É¨„Éº„ÇπÂêç': 'race_name',\n",
    "                'ÁùÄÈ†Ü': 'rank',\n",
    "                'Êû†Áï™': 'waku',\n",
    "                'È¶¨Áï™': 'umaban',\n",
    "                'È®éÊâã': 'jockey',\n",
    "                'Êñ§Èáè': 'weight_carried',\n",
    "                'È¶¨Â†¥': 'condition', # ËâØ/Èáç/Á®çÈáç etc.\n",
    "                '„Çø„Ç§„É†': 'time',\n",
    "                'ÁùÄÂ∑Æ': 'margin',\n",
    "                '‰∏ä„Çä': 'last_3f',\n",
    "                'ÈÄöÈÅé': 'passing',\n",
    "                'È¶¨‰ΩìÈáç': 'horse_weight',\n",
    "                'run_style_val': 'run_style',\n",
    "                'ÂçòÂãù': 'odds',\n",
    "                '„Ç™„ÉÉ„Ç∫': 'odds',\n",
    "                'Ë∑ùÈõ¢': 'raw_distance' # e.g. \"Ëäù1600\"\n",
    "            }\n",
    "            \n",
    "            # Rename available columns\n",
    "            df.rename(columns=column_map, inplace=True)\n",
    "            \n",
    "            # Extract Surface and Distance from 'raw_distance'\n",
    "            if 'raw_distance' in df.columns:\n",
    "                def parse_dist(x):\n",
    "                    if not isinstance(x, str): return None, None\n",
    "                    # \"Ëäù1600\", \"„ÉÄ1200\", \"Èöú3000\"\n",
    "                    # Sometimes \"Ëäù1600\" or just \"1600\"\n",
    "                    surf = None\n",
    "                    dist = None\n",
    "                    if 'Ëäù' in x: surf = 'Ëäù'\n",
    "                    elif '„ÉÄ' in x: surf = '„ÉÄ'\n",
    "                    elif 'Èöú' in x: surf = 'Èöú'\n",
    "                    \n",
    "                    # Extract number\n",
    "                    match = re.search(r'(\\d+)', x)\n",
    "                    if match:\n",
    "                        dist = int(match.group(1))\n",
    "                    return surf, dist\n",
    "\n",
    "                parsed = df['raw_distance'].apply(parse_dist)\n",
    "                df['course_type'] = parsed.apply(lambda x: x[0])\n",
    "                df['distance'] = parsed.apply(lambda x: x[1])\n",
    "            else:\n",
    "                df['course_type'] = None\n",
    "                df['distance'] = None\n",
    "\n",
    "            # Coerce numeric\n",
    "            if 'rank' in df.columns:\n",
    "                df['rank'] = pd.to_numeric(df['rank'], errors='coerce')\n",
    "            \n",
    "            if 'odds' in df.columns:\n",
    "                 df['odds'] = pd.to_numeric(df['odds'], errors='coerce')\n",
    "            \n",
    "            # Fill missing\n",
    "            for target_col in list(column_map.values()) + ['course_type', 'distance']:\n",
    "                if target_col not in df.columns:\n",
    "                    df[target_col] = None\n",
    "                \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing past races for {horse_id}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def extract_run_style(self, passing_str):\n",
    "        \"\"\"\n",
    "        Converts passing order string (e.g., \"1-1-1\", \"10-10-12\") to run style (1,2,3,4).\n",
    "        1: Nige (Escape) - Lead at 1st corner\n",
    "        2: Senkou (Leader) - Within first ~4 or so\n",
    "        3: Sashi (Mid) - Midpack\n",
    "        4: Oikomi (Chaser) - Back\n",
    "        Returns integer code.\n",
    "        \"\"\"\n",
    "        if not isinstance(passing_str, str):\n",
    "            return 3 # Default to Mid\n",
    "            \n",
    "        # Clean string \"1-1-1\" -> [1, 1, 1]\n",
    "        try:\n",
    "            cleaned = re.sub(r'[^0-9-]', '', passing_str)\n",
    "            parts = [int(p) for p in cleaned.split('-') if p]\n",
    "            \n",
    "            if not parts:\n",
    "                return 3\n",
    "                \n",
    "            first_corner = parts[0]\n",
    "            \n",
    "            # Heuristics\n",
    "            if first_corner == 1:\n",
    "                return 1 # Nige\n",
    "            elif first_corner <= 4:\n",
    "                return 2 # Senkou\n",
    "            elif first_corner <= 9: # Assuming standard field size of 10-16, 9 is mid-ish limit? \n",
    "                # Actually \"Sashi\" is usually mid-rear. \n",
    "                # Let's say: 1=Lead, 2-4=Front, 5-10=Mid, >10=Back\n",
    "                return 3 # Sashi\n",
    "            else:\n",
    "                return 4 # Oikomi\n",
    "                \n",
    "        except:\n",
    "            return 3\n",
    "\n",
    "    def scrape_race_with_history(self, race_id):\n",
    "        \"\"\"\n",
    "        Detailed scraper that enters a race_result page, finding horse IDs, \n",
    "        then fetches history for each horse.\n",
    "        Returns a dictionary or structured object with the race result + history.\n",
    "        \"\"\"\n",
    "        url = f\"https://race.netkeiba.com/race/result.html?race_id={race_id}\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "            \n",
    "        # 1. Parse Main Result Table to get Horse IDs and basic result\n",
    "        # Note: auto_scraper already does some of this, but we need Horse IDs specifically.\n",
    "        # \"All_Result_Table\"\n",
    "        \n",
    "        result_data = []\n",
    "        \n",
    "        table = soup.find(\"table\", id=\"All_Result_Table\")\n",
    "        if not table:\n",
    "            return None\n",
    "            \n",
    "        rows = table.find_all(\"tr\", class_=\"HorseList\")\n",
    "        \n",
    "        print(f\"Found {len(rows)} horses in race {race_id}. Fetching histories...\")\n",
    "        \n",
    "        for row in rows:\n",
    "            # Extract basic info\n",
    "            rank_elem = row.select_one(\".Rank\")\n",
    "            rank = rank_elem.text.strip() if rank_elem else \"\"\n",
    "            \n",
    "            horse_name_elem = row.select_one(\".Horse_Name a\")\n",
    "            horse_name = horse_name_elem.text.strip() if horse_name_elem else \"\"\n",
    "            horse_url = horse_name_elem.get(\"href\") if horse_name_elem else \"\"\n",
    "            \n",
    "            # Extract ID from URL\n",
    "            # https://db.netkeiba.com/horse/2018105027\n",
    "            horse_id = None\n",
    "            if horse_url:\n",
    "                match = re.search(r'/horse/(\\d+)', horse_url)\n",
    "                if match:\n",
    "                    horse_id = match.group(1)\n",
    "            \n",
    "            if not horse_id:\n",
    "                print(f\"  Skipping {horse_name} (No ID)\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  Fetching history for {horse_name} ({horse_id})...\")\n",
    "            \n",
    "            # 2. Get Past History\n",
    "            df_past = self.get_past_races(horse_id, n_samples=5)\n",
    "            \n",
    "            # 3. Structure Data\n",
    "            # converting df_past to a list of dicts or flattened fields\n",
    "            history = []\n",
    "            if not df_past.empty:\n",
    "                for idx, r in df_past.iterrows():\n",
    "                    # Extract relevant columns\n",
    "                    # We need at least: Rank, RunStyle, Time(Seconds?), Pace?\n",
    "                    # For now just dump raw-ish data\n",
    "                    hist_item = {\n",
    "                        \"date\": r.get('Êó•‰ªò'),\n",
    "                        \"race_name\": r.get('„É¨„Éº„ÇπÂêç'),\n",
    "                        \"rank\": r.get('ÁùÄÈ†Ü'),\n",
    "                        \"passing\": r.get('ÈÄöÈÅé'),\n",
    "                        \"run_style\": r.get('run_style_val'),\n",
    "                        \"time\": r.get('„Çø„Ç§„É†'),\n",
    "                        # Add more as needed for Feature Engineering\n",
    "                    }\n",
    "                    history.append(hist_item)\n",
    "            \n",
    "            entry = {\n",
    "                \"race_id\": race_id,\n",
    "                \"horse_id\": horse_id,\n",
    "                \"horse_name\": horse_name,\n",
    "                \"rank\": rank,\n",
    "                \"history\": history\n",
    "            }\n",
    "            result_data.append(entry)\n",
    "            \n",
    "        return result_data\n",
    "\n",
    "    def get_horse_profile(self, horse_id):\n",
    "        \"\"\"\n",
    "        Fetches horse profile to get pedigree (Father, Mother, Grandfather(BMS)).\n",
    "        Returns a dictionary or None.\n",
    "        \"\"\"\n",
    "        # Use pedigree page for reliable bloodline data\n",
    "        url = f\"https://db.netkeiba.com/horse/ped/{horse_id}/\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        # Parse Blood Table\n",
    "        # table class=\"blood_table\"\n",
    "        \n",
    "        data = {\n",
    "            \"father\": \"\",\n",
    "            \"mother\": \"\",\n",
    "            \"bms\": \"\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            table = soup.select_one(\"table.blood_table\")\n",
    "            if table:\n",
    "                rows = table.find_all(\"tr\")\n",
    "                # 5-generation table has 32 rows usually\n",
    "                # Father at Row 0 (rowspan 16)\n",
    "                # Mother at Row 16 (rowspan 16)\n",
    "                \n",
    "                if len(rows) >= 17:\n",
    "                    # Father: Row 0, Col 0\n",
    "                    r0 = rows[0].find_all(\"td\")\n",
    "                    if r0:\n",
    "                        txt = r0[0].text.strip()\n",
    "                        # Clean: \"„Çπ„ÇØ„É™„Éº„É≥„Éí„Éº„É≠„Éº\\n2004 Ê†óÊØõ...\" -> \"„Çπ„ÇØ„É™„Éº„É≥„Éí„Éº„É≠„Éº\"\n",
    "                        # Take first line\n",
    "                        data[\"father\"] = txt.split('\\n')[0].strip()\n",
    "                        \n",
    "                    # Mother & BMS: Row 16\n",
    "                    r16 = rows[16].find_all(\"td\")\n",
    "                    if len(r16) >= 2:\n",
    "                        # Mother\n",
    "                        m_txt = r16[0].text.strip()\n",
    "                        data[\"mother\"] = m_txt.split('\\n')[0].strip()\n",
    "                        \n",
    "                        # BMS (Mother's Father)\n",
    "                        bms_txt = r16[1].text.strip()\n",
    "                        data[\"bms\"] = bms_txt.split('\\n')[0].strip()\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing profile for {horse_id}: {e}\")\n",
    "            \n",
    "        return data\n",
    "\n",
    "    def get_race_metadata(self, race_id):\n",
    "        \"\"\"\n",
    "        Fetches metadata for a specific race ID from Netkeiba.\n",
    "        Returns dict with: race_name, date, venue, course_type, distance, weather, condition, turn\n",
    "        \"\"\"\n",
    "        url = f\"https://race.netkeiba.com/race/result.html?race_id={race_id}\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "            \n",
    "        data = {\n",
    "            \"race_name\": \"\",\n",
    "            \"date\": \"\",\n",
    "            \"venue\": \"\",\n",
    "            \"course_type\": \"\",\n",
    "            \"distance\": \"\",\n",
    "            \"weather\": \"\",\n",
    "            \"condition\": \"\",\n",
    "            \"turn\": \"\", # New: Dictionary key for turn direction\n",
    "            \"race_id\": race_id\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Race Name\n",
    "            title_elem = soup.select_one(\".RaceName\")\n",
    "            if title_elem:\n",
    "                data[\"race_name\"] = title_elem.text.strip()\n",
    "                \n",
    "            # Date & Venue & Conditions\n",
    "            # <div class=\"RaceData01\">... 2023Âπ¥1Êúà5Êó• ... 1Âõû‰∏≠Â±±1Êó• ...</div>\n",
    "            # Content: \"15:35Áô∫Ëµ∞ / Ëäù1600m (Âè≥ Â§ñ) / Â§©ÂÄô:Êô¥ / È¶¨Â†¥:ËâØ\"\n",
    "            \n",
    "            rd1 = soup.select_one(\".RaceData01\")\n",
    "            \n",
    "            if rd1:\n",
    "                txt = rd1.text.strip()\n",
    "                \n",
    "                # Weather\n",
    "                if \"Â§©ÂÄô:Êô¥\" in txt: data[\"weather\"] = \"Êô¥\"\n",
    "                elif \"Â§©ÂÄô:Êõá\" in txt: data[\"weather\"] = \"Êõá\"\n",
    "                elif \"Â§©ÂÄô:Â∞èÈõ®\" in txt: data[\"weather\"] = \"Â∞èÈõ®\"\n",
    "                elif \"Â§©ÂÄô:Èõ®\" in txt: data[\"weather\"] = \"Èõ®\"\n",
    "                elif \"Â§©ÂÄô:Èõ™\" in txt: data[\"weather\"] = \"Èõ™\"\n",
    "                \n",
    "                # Condition\n",
    "                if \"È¶¨Â†¥:ËâØ\" in txt: data[\"condition\"] = \"ËâØ\"\n",
    "                elif \"È¶¨Â†¥:Á®ç\" in txt: data[\"condition\"] = \"Á®çÈáç\" # Covers Á®çÈáç\n",
    "                elif \"È¶¨Â†¥:Èáç\" in txt: data[\"condition\"] = \"Èáç\"\n",
    "                elif \"È¶¨Â†¥:‰∏çËâØ\" in txt: data[\"condition\"] = \"‰∏çËâØ\"\n",
    "                \n",
    "                # Course & Distance (\"Ëäù1600m\")\n",
    "                # Regex for \"Ëäù\", \"„ÉÄ\", \"Èöú\" followed by digits\n",
    "                match = re.search(r'(Ëäù|„ÉÄ|Èöú)(\\d+)m', txt)\n",
    "                if match:\n",
    "                    ctype_raw = match.group(1)\n",
    "                    if ctype_raw == \"Ëäù\": data[\"course_type\"] = \"Ëäù\"\n",
    "                    elif ctype_raw == \"„ÉÄ\": data[\"course_type\"] = \"„ÉÄ„Éº„Éà\"\n",
    "                    elif ctype_raw == \"Èöú\": data[\"course_type\"] = \"ÈöúÂÆ≥\"\n",
    "                    \n",
    "                    data[\"distance\"] = match.group(2)\n",
    "                \n",
    "                # Turn Direction (\"Âè≥\", \"Â∑¶\", \"Áõ¥Á∑ö\")\n",
    "                # Usually in parentheses like \"(Âè≥)\" or \"(Â∑¶)\" or \"(Ëäù Â∑¶)\"\n",
    "                if \"Âè≥\" in txt: data[\"turn\"] = \"Âè≥\"\n",
    "                elif \"Â∑¶\" in txt: data[\"turn\"] = \"Â∑¶\"\n",
    "                elif \"Áõ¥Á∑ö\" in txt: data[\"turn\"] = \"Áõ¥\"\n",
    "\n",
    "            # Date\n",
    "            # Try finding date in Title or dedicated element\n",
    "            date_elem = soup.select_one(\"dl#RaceList_DateList dd.Active\") \n",
    "            if date_elem:\n",
    "                 # Usually \"1Êúà5Êó•(Èáë)\" - needs Year\n",
    "                 # We can rely on the fact that race_id contains year (2025...)\n",
    "                 # But let's look for YYYYÂπ¥ in the whole text or title\n",
    "                 pass\n",
    "            \n",
    "            # Fallback Date from Title Tag or Meta\n",
    "            if not data[\"date\"]:\n",
    "                 meta_title = soup.title.text if soup.title else \"\"\n",
    "                 match_date = re.search(r'(\\d{4}Âπ¥\\d{1,2}Êúà\\d{1,2}Êó•)', meta_title)\n",
    "                 if match_date:\n",
    "                     data[\"date\"] = match_date.group(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing metadata for {race_id}: {e}\")\n",
    "            \n",
    "        return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test\n",
    "    scraper = RaceScraper()\n",
    "    print(\"Running test...\")\n",
    "    # Example: Do Deuce (2019105219)\n",
    "    # url = \"https://db.netkeiba.com/horse/2019105219/\"\n",
    "    # print(f\"Fetching {url}\")\n",
    "    df = scraper.get_past_races(\"2019105219\")\n",
    "    if df.empty:\n",
    "        print(\"DF is empty. Checking raw soup for 'db_h_race_results'...\")\n",
    "        soup = scraper._get_soup(f\"https://db.netkeiba.com/horse/result/2019105219/\")\n",
    "        if soup:\n",
    "             t = soup.select_one(\"table.db_h_race_results\")\n",
    "             print(f\"Selector 'table.db_h_race_results' found: {t is not None}\")\n",
    "             if not t:\n",
    "                 print(\"Trying fallback 'table' with 'ÁùÄÈ†Ü'...\")\n",
    "                 tables = soup.find_all(\"table\")\n",
    "                 found = False\n",
    "                 for i, tbl in enumerate(tables):\n",
    "                     print(f\"Table {i} classes: {tbl.get('class')}\")\n",
    "                     if \"ÁùÄÈ†Ü\" in tbl.text or \"ÁùÄ È†Ü\" in tbl.text or \"Êó•‰ªò\" in tbl.text:\n",
    "                         print(\"Found a table with 'ÁùÄÈ†Ü/Êó•‰ªò'.\")\n",
    "                         # print(str(tbl)[:200])\n",
    "                         t = tbl\n",
    "                         found = True\n",
    "                         break\n",
    "                 if not found:\n",
    "                     print(\"No table with 'ÁùÄÈ†Ü' found in soup.\")\n",
    "                     print(\"Soup snippet:\", soup.text[:500])\n",
    "                 else:\n",
    "                    # Retry parsing with found table\n",
    "                     try:\n",
    "                        df = pd.read_html(str(t))[0]\n",
    "                        print(\"Retry DF Head:\")\n",
    "                        print(df.head())\n",
    "                     except Exception as e:\n",
    "                        print(f\"Retry parsing failed: {e}\")\n",
    "        else:\n",
    "            print(\"Soup is None.\")\n",
    "    else:\n",
    "        print(df.head())\n",
    "        print(\"Columns:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAR „Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„É≠„Ç∏„ÉÉ„ÇØ\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date, timedelta\n",
    "import calendar\n",
    "import time\n",
    "import os\n",
    "\n",
    "def run_nar_scraping(year, start_month=1, end_month=12):\n",
    "    start_date = date(int(year), int(start_month), 1)\n",
    "    last_day = calendar.monthrange(int(year), int(end_month))[1]\n",
    "    end_date = date(int(year), int(end_month), last_day)\n",
    "    \n",
    "    today = date.today()\n",
    "    if end_date > today: end_date = today\n",
    "    \n",
    "    print(f'NAR„Éá„Éº„Çø„Çí {start_date} „Åã„Çâ {end_date} „Åæ„ÅßÂèñÂæó„Åó„Åæ„Åô...')\n",
    "    \n",
    "    curr = start_date\n",
    "    scraper = RaceScraper()\n",
    "    \n",
    "    while curr <= end_date:\n",
    "        d_str = curr.strftime('%Y%m%d')\n",
    "        url = f'https://nar.netkeiba.com/top/race_list_sub.html?kaisai_date={d_str}'\n",
    "        try:\n",
    "             time.sleep(0.5)\n",
    "             resp = requests.get(url)\n",
    "             resp.encoding = 'EUC-JP'\n",
    "             soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "             links = soup.select('a[href*=\"race/result.html\"]')\n",
    "             if links:\n",
    "                 print(f'{curr}: {len(links)} ‰ª∂„ÅÆ„É¨„Éº„Çπ„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü„ÄÇ(ÂèñÂæóÂá¶ÁêÜ„ÅØÊú™ÂÆüË£Ö„Åß„Åô)')\n",
    "                 # ÂÆüÈöõ„ÅÆ„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞Âá¶ÁêÜ„ÅØ„Åì„Åì„Å´Ë®òËø∞\n",
    "        except Exception as e: print(e)\n",
    "        curr += timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ë®≠ÂÆö („Åì„Åì„ÇíÂ§âÊõ¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ)\n",
    "YEAR = 2024          # ÂØæË±°Âπ¥Â∫¶\n",
    "START_MONTH = 1      # ÈñãÂßãÊúà\n",
    "END_MONTH = 12       # ÁµÇ‰∫ÜÊúà\n",
    "\n",
    "# ÂÆüË°å„Éñ„É≠„ÉÉ„ÇØ\n",
    "if YEAR:\n",
    "    # „Éá„Ç£„É¨„ÇØ„Éà„É™‰ΩúÊàê\n",
    "    os.makedirs('data/raw', exist_ok=True)\n",
    "    run_nar_scraping(YEAR, START_MONTH, END_MONTH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}