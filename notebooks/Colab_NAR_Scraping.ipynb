{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèá NAR ÂÖ®„É¨„Éº„ÇπÂèñÂæó\n",
    "‰ª•‰∏ã„ÅÆË®≠ÂÆöÂ§âÊï∞„ÇíÂ§âÊõ¥„Åó„Å¶ÂÆüË°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇNARÔºàÂú∞ÊñπÁ´∂È¶¨Ôºâ„ÅÆ„Éá„Éº„Çø„ÇíÊó•‰ªòÈ†Ü„Å´ÂèñÂæó„Åó„Åæ„Åô„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive„Çí„Éû„Ç¶„É≥„Éà„Åô„ÇãÂ†¥Âêà„ÅÆ„ÅøÂÆüË°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class RaceScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "    def _get_soup(self, url):\n",
    "        try:\n",
    "            time.sleep(1) # Be polite\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.encoding = response.apparent_encoding\n",
    "            if response.status_code == 200:\n",
    "                return BeautifulSoup(response.text, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def get_past_races(self, horse_id, n_samples=5):\n",
    "        \"\"\"\n",
    "        Fetches past n_samples race results for a given horse_id from netkeiba db.\n",
    "        Returns a DataFrame of past races.\n",
    "        \"\"\"\n",
    "        url = f\"https://db.netkeiba.com/horse/result/{horse_id}/\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # The results are usually in a table with class \"db_h_race_results\"\n",
    "        table = soup.select_one(\"table.db_h_race_results\")\n",
    "        if not table:\n",
    "            # Try to find any table with \"ÁùÄÈ†Ü\"\n",
    "            tables = soup.find_all(\"table\")\n",
    "            for t in tables:\n",
    "                if \"ÁùÄÈ†Ü\" in t.text:\n",
    "                    table = t\n",
    "                    break\n",
    "        \n",
    "        if not table:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Parse table\n",
    "        # We need to manually parse to get clean data and handle links if needed (though for past data, text is mostly fine)\n",
    "        # pd.read_html is easier for the table\n",
    "        try:\n",
    "            df = pd.read_html(io.StringIO(str(table)))[0]\n",
    "            \n",
    "            # Basic cleaning\n",
    "            df = df.dropna(how='all')\n",
    "            \n",
    "            # The columns in db.netkeiba are roughly:\n",
    "            # Êó•‰ªò, ÈñãÂÇ¨, Â§©Ê∞ó, R, „É¨„Éº„ÇπÂêç, Êò†ÂÉè, È†≠Êï∞, Êû†Áï™, ... ÁùÄÈ†Ü, ... ÈÄöÈÅé, ...\n",
    "            \n",
    "            # We want to keep: Date, Race Name, Course info, Rank, Time, Passing (Style)\n",
    "            \n",
    "            # Normalize column names (remove spaces/newlines)\n",
    "            df.columns = df.columns.astype(str).str.replace(r'\\s+', '', regex=True)\n",
    "\n",
    "            # Filter rows that look like actual races (Date column exists)\n",
    "            if 'Êó•‰ªò' in df.columns:\n",
    "                df['date_obj'] = pd.to_datetime(df['Êó•‰ªò'], format='%Y/%m/%d', errors='coerce')\n",
    "                df = df.dropna(subset=['date_obj'])\n",
    "                df = df.sort_values('date_obj', ascending=False)\n",
    "                \n",
    "            # Take top N\n",
    "            if n_samples:\n",
    "                df = df.head(n_samples)\n",
    "            \n",
    "            # Process Run Style (Leg Type)\n",
    "            if 'ÈÄöÈÅé' in df.columns:\n",
    "                df['run_style_val'] = df['ÈÄöÈÅé'].apply(self.extract_run_style)\n",
    "            else:\n",
    "                df['run_style_val'] = 3 # Unknown\n",
    "\n",
    "            # Extract/Rename Columns\n",
    "            # We want: Êó•‰ªò, ÈñãÂÇ¨, Â§©Ê∞ó, R, „É¨„Éº„ÇπÂêç, Êò†ÂÉè, È†≠Êï∞, Êû†Áï™, ... ÁùÄÈ†Ü, ... ÈÄöÈÅé, ...\n",
    "            # Important: '‰∏ä„Çä' (3F), 'È¶¨‰ΩìÈáç', 'È®éÊâã'\n",
    "            \n",
    "            # Map standard columns if they exist\n",
    "            column_map = {\n",
    "                'Êó•‰ªò': 'date',\n",
    "                'ÈñãÂÇ¨': 'venue',\n",
    "                'Â§©Ê∞ó': 'weather',\n",
    "                '„É¨„Éº„ÇπÂêç': 'race_name',\n",
    "                'ÁùÄÈ†Ü': 'rank',\n",
    "                'Êû†Áï™': 'waku',\n",
    "                'È¶¨Áï™': 'umaban',\n",
    "                'È®éÊâã': 'jockey',\n",
    "                'Êñ§Èáè': 'weight_carried',\n",
    "                'È¶¨Â†¥': 'condition', # ËâØ/Èáç/Á®çÈáç etc.\n",
    "                '„Çø„Ç§„É†': 'time',\n",
    "                'ÁùÄÂ∑Æ': 'margin',\n",
    "                '‰∏ä„Çä': 'last_3f',\n",
    "                'ÈÄöÈÅé': 'passing',\n",
    "                'È¶¨‰ΩìÈáç': 'horse_weight',\n",
    "                'run_style_val': 'run_style',\n",
    "                'ÂçòÂãù': 'odds',\n",
    "                '„Ç™„ÉÉ„Ç∫': 'odds',\n",
    "                'Ë∑ùÈõ¢': 'raw_distance' # e.g. \"Ëäù1600\"\n",
    "            }\n",
    "            \n",
    "            # Rename available columns\n",
    "            df.rename(columns=column_map, inplace=True)\n",
    "            \n",
    "            # Extract Surface and Distance from 'raw_distance'\n",
    "            if 'raw_distance' in df.columns:\n",
    "                def parse_dist(x):\n",
    "                    if not isinstance(x, str): return None, None\n",
    "                    # \"Ëäù1600\", \"„ÉÄ1200\", \"Èöú3000\"\n",
    "                    # Sometimes \"Ëäù1600\" or just \"1600\"\n",
    "                    surf = None\n",
    "                    dist = None\n",
    "                    if 'Ëäù' in x: surf = 'Ëäù'\n",
    "                    elif '„ÉÄ' in x: surf = '„ÉÄ'\n",
    "                    elif 'Èöú' in x: surf = 'Èöú'\n",
    "                    \n",
    "                    # Extract number\n",
    "                    match = re.search(r'(\\d+)', x)\n",
    "                    if match:\n",
    "                        dist = int(match.group(1))\n",
    "                    return surf, dist\n",
    "\n",
    "                parsed = df['raw_distance'].apply(parse_dist)\n",
    "                df['course_type'] = parsed.apply(lambda x: x[0])\n",
    "                df['distance'] = parsed.apply(lambda x: x[1])\n",
    "            else:\n",
    "                df['course_type'] = None\n",
    "                df['distance'] = None\n",
    "\n",
    "            # Coerce numeric\n",
    "            if 'rank' in df.columns:\n",
    "                df['rank'] = pd.to_numeric(df['rank'], errors='coerce')\n",
    "            \n",
    "            if 'odds' in df.columns:\n",
    "                 df['odds'] = pd.to_numeric(df['odds'], errors='coerce')\n",
    "            \n",
    "            # Fill missing\n",
    "            for target_col in list(column_map.values()) + ['course_type', 'distance']:\n",
    "                if target_col not in df.columns:\n",
    "                    df[target_col] = None\n",
    "                \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing past races for {horse_id}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def extract_run_style(self, passing_str):\n",
    "        \"\"\"\n",
    "        Converts passing order string (e.g., \"1-1-1\", \"10-10-12\") to run style (1,2,3,4).\n",
    "        1: Nige (Escape) - Lead at 1st corner\n",
    "        2: Senkou (Leader) - Within first ~4 or so\n",
    "        3: Sashi (Mid) - Midpack\n",
    "        4: Oikomi (Chaser) - Back\n",
    "        Returns integer code.\n",
    "        \"\"\"\n",
    "        if not isinstance(passing_str, str):\n",
    "            return 3 # Default to Mid\n",
    "            \n",
    "        # Clean string \"1-1-1\" -> [1, 1, 1]\n",
    "        try:\n",
    "            cleaned = re.sub(r'[^0-9-]', '', passing_str)\n",
    "            parts = [int(p) for p in cleaned.split('-') if p]\n",
    "            \n",
    "            if not parts:\n",
    "                return 3\n",
    "                \n",
    "            first_corner = parts[0]\n",
    "            \n",
    "            # Heuristics\n",
    "            if first_corner == 1:\n",
    "                return 1 # Nige\n",
    "            elif first_corner <= 4:\n",
    "                return 2 # Senkou\n",
    "            elif first_corner <= 9: # Assuming standard field size of 10-16, 9 is mid-ish limit? \n",
    "                # Actually \"Sashi\" is usually mid-rear. \n",
    "                # Let's say: 1=Lead, 2-4=Front, 5-10=Mid, >10=Back\n",
    "                return 3 # Sashi\n",
    "            else:\n",
    "                return 4 # Oikomi\n",
    "                \n",
    "        except:\n",
    "            return 3\n",
    "\n",
    "    def scrape_race_with_history(self, race_id):\n",
    "        \"\"\"\n",
    "        Detailed scraper that enters a race_result page, finding horse IDs, \n",
    "        then fetches history for each horse.\n",
    "        Returns a dictionary or structured object with the race result + history.\n",
    "        \"\"\"\n",
    "        url = f\"https://race.netkeiba.com/race/result.html?race_id={race_id}\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "            \n",
    "        # 1. Parse Main Result Table to get Horse IDs and basic result\n",
    "        # Note: auto_scraper already does some of this, but we need Horse IDs specifically.\n",
    "        # \"All_Result_Table\"\n",
    "        \n",
    "        result_data = []\n",
    "        \n",
    "        table = soup.find(\"table\", id=\"All_Result_Table\")\n",
    "        if not table:\n",
    "            return None\n",
    "            \n",
    "        rows = table.find_all(\"tr\", class_=\"HorseList\")\n",
    "        \n",
    "        print(f\"Found {len(rows)} horses in race {race_id}. Fetching histories...\")\n",
    "        \n",
    "        for row in rows:\n",
    "            # Extract basic info\n",
    "            rank_elem = row.select_one(\".Rank\")\n",
    "            rank = rank_elem.text.strip() if rank_elem else \"\"\n",
    "            \n",
    "            horse_name_elem = row.select_one(\".Horse_Name a\")\n",
    "            horse_name = horse_name_elem.text.strip() if horse_name_elem else \"\"\n",
    "            horse_url = horse_name_elem.get(\"href\") if horse_name_elem else \"\"\n",
    "            \n",
    "            # Extract ID from URL\n",
    "            # https://db.netkeiba.com/horse/2018105027\n",
    "            horse_id = None\n",
    "            if horse_url:\n",
    "                match = re.search(r'/horse/(\\d+)', horse_url)\n",
    "                if match:\n",
    "                    horse_id = match.group(1)\n",
    "            \n",
    "            if not horse_id:\n",
    "                print(f\"  Skipping {horse_name} (No ID)\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  Fetching history for {horse_name} ({horse_id})...\")\n",
    "            \n",
    "            # 2. Get Past History\n",
    "            df_past = self.get_past_races(horse_id, n_samples=5)\n",
    "            \n",
    "            # 3. Structure Data\n",
    "            # converting df_past to a list of dicts or flattened fields\n",
    "            history = []\n",
    "            if not df_past.empty:\n",
    "                for idx, r in df_past.iterrows():\n",
    "                    # Extract relevant columns\n",
    "                    # We need at least: Rank, RunStyle, Time(Seconds?), Pace?\n",
    "                    # For now just dump raw-ish data\n",
    "                    hist_item = {\n",
    "                        \"date\": r.get('Êó•‰ªò'),\n",
    "                        \"race_name\": r.get('„É¨„Éº„ÇπÂêç'),\n",
    "                        \"rank\": r.get('ÁùÄÈ†Ü'),\n",
    "                        \"passing\": r.get('ÈÄöÈÅé'),\n",
    "                        \"run_style\": r.get('run_style_val'),\n",
    "                        \"time\": r.get('„Çø„Ç§„É†'),\n",
    "                        # Add more as needed for Feature Engineering\n",
    "                    }\n",
    "                    history.append(hist_item)\n",
    "            \n",
    "            entry = {\n",
    "                \"race_id\": race_id,\n",
    "                \"horse_id\": horse_id,\n",
    "                \"horse_name\": horse_name,\n",
    "                \"rank\": rank,\n",
    "                \"history\": history\n",
    "            }\n",
    "            result_data.append(entry)\n",
    "            \n",
    "        return result_data\n",
    "\n",
    "    def get_horse_profile(self, horse_id):\n",
    "        \"\"\"\n",
    "        Fetches horse profile to get pedigree (Father, Mother, Grandfather(BMS)).\n",
    "        Returns a dictionary or None.\n",
    "        \"\"\"\n",
    "        # Use pedigree page for reliable bloodline data\n",
    "        url = f\"https://db.netkeiba.com/horse/ped/{horse_id}/\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        # Parse Blood Table\n",
    "        # table class=\"blood_table\"\n",
    "        \n",
    "        data = {\n",
    "            \"father\": \"\",\n",
    "            \"mother\": \"\",\n",
    "            \"bms\": \"\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            table = soup.select_one(\"table.blood_table\")\n",
    "            if table:\n",
    "                rows = table.find_all(\"tr\")\n",
    "                # 5-generation table has 32 rows usually\n",
    "                # Father at Row 0 (rowspan 16)\n",
    "                # Mother at Row 16 (rowspan 16)\n",
    "                \n",
    "                if len(rows) >= 17:\n",
    "                    # Father: Row 0, Col 0\n",
    "                    r0 = rows[0].find_all(\"td\")\n",
    "                    if r0:\n",
    "                        txt = r0[0].text.strip()\n",
    "                        # Clean: \"„Çπ„ÇØ„É™„Éº„É≥„Éí„Éº„É≠„Éº\\n2004 Ê†óÊØõ...\" -> \"„Çπ„ÇØ„É™„Éº„É≥„Éí„Éº„É≠„Éº\"\n",
    "                        # Take first line\n",
    "                        data[\"father\"] = txt.split('\\n')[0].strip()\n",
    "                        \n",
    "                    # Mother & BMS: Row 16\n",
    "                    r16 = rows[16].find_all(\"td\")\n",
    "                    if len(r16) >= 2:\n",
    "                        # Mother\n",
    "                        m_txt = r16[0].text.strip()\n",
    "                        data[\"mother\"] = m_txt.split('\\n')[0].strip()\n",
    "                        \n",
    "                        # BMS (Mother's Father)\n",
    "                        bms_txt = r16[1].text.strip()\n",
    "                        data[\"bms\"] = bms_txt.split('\\n')[0].strip()\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing profile for {horse_id}: {e}\")\n",
    "            \n",
    "        return data\n",
    "\n",
    "    def get_race_metadata(self, race_id):\n",
    "        \"\"\"\n",
    "        Fetches metadata for a specific race ID from Netkeiba.\n",
    "        Returns dict with: race_name, date, venue, course_type, distance, weather, condition, turn\n",
    "        \"\"\"\n",
    "        url = f\"https://race.netkeiba.com/race/result.html?race_id={race_id}\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "            \n",
    "        data = {\n",
    "            \"race_name\": \"\",\n",
    "            \"date\": \"\",\n",
    "            \"venue\": \"\",\n",
    "            \"course_type\": \"\",\n",
    "            \"distance\": \"\",\n",
    "            \"weather\": \"\",\n",
    "            \"condition\": \"\",\n",
    "            \"turn\": \"\", # New: Dictionary key for turn direction\n",
    "            \"race_id\": race_id\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Race Name\n",
    "            title_elem = soup.select_one(\".RaceName\")\n",
    "            if title_elem:\n",
    "                data[\"race_name\"] = title_elem.text.strip()\n",
    "                \n",
    "            # Date & Venue & Conditions\n",
    "            # <div class=\"RaceData01\">... 2023Âπ¥1Êúà5Êó• ... 1Âõû‰∏≠Â±±1Êó• ...</div>\n",
    "            # Content: \"15:35Áô∫Ëµ∞ / Ëäù1600m (Âè≥ Â§ñ) / Â§©ÂÄô:Êô¥ / È¶¨Â†¥:ËâØ\"\n",
    "            \n",
    "            rd1 = soup.select_one(\".RaceData01\")\n",
    "            \n",
    "            if rd1:\n",
    "                txt = rd1.text.strip()\n",
    "                \n",
    "                # Weather\n",
    "                if \"Â§©ÂÄô:Êô¥\" in txt: data[\"weather\"] = \"Êô¥\"\n",
    "                elif \"Â§©ÂÄô:Êõá\" in txt: data[\"weather\"] = \"Êõá\"\n",
    "                elif \"Â§©ÂÄô:Â∞èÈõ®\" in txt: data[\"weather\"] = \"Â∞èÈõ®\"\n",
    "                elif \"Â§©ÂÄô:Èõ®\" in txt: data[\"weather\"] = \"Èõ®\"\n",
    "                elif \"Â§©ÂÄô:Èõ™\" in txt: data[\"weather\"] = \"Èõ™\"\n",
    "                \n",
    "                # Condition\n",
    "                if \"È¶¨Â†¥:ËâØ\" in txt: data[\"condition\"] = \"ËâØ\"\n",
    "                elif \"È¶¨Â†¥:Á®ç\" in txt: data[\"condition\"] = \"Á®çÈáç\" # Covers Á®çÈáç\n",
    "                elif \"È¶¨Â†¥:Èáç\" in txt: data[\"condition\"] = \"Èáç\"\n",
    "                elif \"È¶¨Â†¥:‰∏çËâØ\" in txt: data[\"condition\"] = \"‰∏çËâØ\"\n",
    "                \n",
    "                # Course & Distance (\"Ëäù1600m\")\n",
    "                # Regex for \"Ëäù\", \"„ÉÄ\", \"Èöú\" followed by digits\n",
    "                match = re.search(r'(Ëäù|„ÉÄ|Èöú)(\\d+)m', txt)\n",
    "                if match:\n",
    "                    ctype_raw = match.group(1)\n",
    "                    if ctype_raw == \"Ëäù\": data[\"course_type\"] = \"Ëäù\"\n",
    "                    elif ctype_raw == \"„ÉÄ\": data[\"course_type\"] = \"„ÉÄ„Éº„Éà\"\n",
    "                    elif ctype_raw == \"Èöú\": data[\"course_type\"] = \"ÈöúÂÆ≥\"\n",
    "                    \n",
    "                    data[\"distance\"] = match.group(2)\n",
    "                \n",
    "                # Turn Direction (\"Âè≥\", \"Â∑¶\", \"Áõ¥Á∑ö\")\n",
    "                # Usually in parentheses like \"(Âè≥)\" or \"(Â∑¶)\" or \"(Ëäù Â∑¶)\"\n",
    "                if \"Âè≥\" in txt: data[\"turn\"] = \"Âè≥\"\n",
    "                elif \"Â∑¶\" in txt: data[\"turn\"] = \"Â∑¶\"\n",
    "                elif \"Áõ¥Á∑ö\" in txt: data[\"turn\"] = \"Áõ¥\"\n",
    "\n",
    "            # Date\n",
    "            # Try finding date in Title or dedicated element\n",
    "            date_elem = soup.select_one(\"dl#RaceList_DateList dd.Active\") \n",
    "            if date_elem:\n",
    "                 # Usually \"1Êúà5Êó•(Èáë)\" - needs Year\n",
    "                 # We can rely on the fact that race_id contains year (2025...)\n",
    "                 # But let's look for YYYYÂπ¥ in the whole text or title\n",
    "                 pass\n",
    "            \n",
    "            # Fallback Date from Title Tag or Meta\n",
    "            if not data[\"date\"]:\n",
    "                 meta_title = soup.title.text if soup.title else \"\"\n",
    "                 match_date = re.search(r'(\\d{4}Âπ¥\\d{1,2}Êúà\\d{1,2}Êó•)', meta_title)\n",
    "                 if match_date:\n",
    "                     data[\"date\"] = match_date.group(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing metadata for {race_id}: {e}\")\n",
    "            \n",
    "        return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test\n",
    "    scraper = RaceScraper()\n",
    "    print(\"Running test...\")\n",
    "    # Example: Do Deuce (2019105219)\n",
    "    # url = \"https://db.netkeiba.com/horse/2019105219/\"\n",
    "    # print(f\"Fetching {url}\")\n",
    "    df = scraper.get_past_races(\"2019105219\")\n",
    "    if df.empty:\n",
    "        print(\"DF is empty. Checking raw soup for 'db_h_race_results'...\")\n",
    "        soup = scraper._get_soup(f\"https://db.netkeiba.com/horse/result/2019105219/\")\n",
    "        if soup:\n",
    "             t = soup.select_one(\"table.db_h_race_results\")\n",
    "             print(f\"Selector 'table.db_h_race_results' found: {t is not None}\")\n",
    "             if not t:\n",
    "                 print(\"Trying fallback 'table' with 'ÁùÄÈ†Ü'...\")\n",
    "                 tables = soup.find_all(\"table\")\n",
    "                 found = False\n",
    "                 for i, tbl in enumerate(tables):\n",
    "                     print(f\"Table {i} classes: {tbl.get('class')}\")\n",
    "                     if \"ÁùÄÈ†Ü\" in tbl.text or \"ÁùÄ È†Ü\" in tbl.text or \"Êó•‰ªò\" in tbl.text:\n",
    "                         print(\"Found a table with 'ÁùÄÈ†Ü/Êó•‰ªò'.\")\n",
    "                         # print(str(tbl)[:200])\n",
    "                         t = tbl\n",
    "                         found = True\n",
    "                         break\n",
    "                 if not found:\n",
    "                     print(\"No table with 'ÁùÄÈ†Ü' found in soup.\")\n",
    "                     print(\"Soup snippet:\", soup.text[:500])\n",
    "                 else:\n",
    "                    # Retry parsing with found table\n",
    "                     try:\n",
    "                        df = pd.read_html(str(t))[0]\n",
    "                        print(\"Retry DF Head:\")\n",
    "                        print(df.head())\n",
    "                     except Exception as e:\n",
    "                        print(f\"Retry parsing failed: {e}\")\n",
    "        else:\n",
    "            print(\"Soup is None.\")\n",
    "    else:\n",
    "        print(df.head())\n",
    "        print(\"Columns:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2026-01-08: Rich Scraping Logic (JRA/NAR Universal)\n",
    "# This code handles scraping of Race Result + Horse History + Pedigree in one pass.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "from datetime import datetime\n",
    "import urllib.parse\n",
    "import time\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- RaceScraper Helper Class (Embedded) ---\n",
    "class RaceScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "    def _get_soup(self, url):\n",
    "        try:\n",
    "            time.sleep(1) # Be polite\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.encoding = response.apparent_encoding\n",
    "            if response.status_code == 200:\n",
    "                return BeautifulSoup(response.text, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def get_horse_profile(self, horse_id):\n",
    "        \"\"\"\n",
    "        Fetches horse profile to get pedigree (Father, Mother, Grandfather(BMS)).\n",
    "        Returns a dictionary or None.\n",
    "        \"\"\"\n",
    "        url = f\"https://db.netkeiba.com/horse/ped/{horse_id}/\"\n",
    "        soup = self._get_soup(url)\n",
    "        data = {\"father\": \"\", \"mother\": \"\", \"bms\": \"\"}\n",
    "        if not soup: return data\n",
    "\n",
    "        try:\n",
    "            table = soup.select_one(\"table.blood_table\")\n",
    "            if table:\n",
    "                rows = table.find_all(\"tr\")\n",
    "                if len(rows) >= 17:\n",
    "                    # Father: Row 0\n",
    "                    r0 = rows[0].find_all(\"td\")\n",
    "                    if r0:\n",
    "                        txt = r0[0].text.strip()\n",
    "                        data[\"father\"] = txt.split('\\n')[0].strip()\n",
    "\n",
    "                    # Mother & BMS: Row 16\n",
    "                    r16 = rows[16].find_all(\"td\")\n",
    "                    if len(r16) >= 2:\n",
    "                        m_txt = r16[0].text.strip()\n",
    "                        data[\"mother\"] = m_txt.split('\\n')[0].strip()\n",
    "                        bms_txt = r16[1].text.strip()\n",
    "                        data[\"bms\"] = bms_txt.split('\\n')[0].strip()\n",
    "        except Exception as e:\n",
    "            pass # Silent fail for profile\n",
    "        return data\n",
    "\n",
    "    def extract_run_style(self, passing_str):\n",
    "        if not isinstance(passing_str, str): return 3\n",
    "        try:\n",
    "            cleaned = re.sub(r'[^0-9-]', '', passing_str)\n",
    "            parts = [int(p) for p in cleaned.split('-') if p]\n",
    "            if not parts: return 3\n",
    "            first_corner = parts[0]\n",
    "            if first_corner == 1: return 1 # Nige\n",
    "            elif first_corner <= 4: return 2 # Senkou\n",
    "            elif first_corner <= 9: return 3 # Sashi\n",
    "            else: return 4 # Oikomi\n",
    "        except: return 3\n",
    "\n",
    "    def get_past_races(self, horse_id, current_race_date, n_samples=5):\n",
    "        \"\"\"\n",
    "        Fetches past n_samples race results.\n",
    "        Filters out races AFTER current_race_date.\n",
    "        Returns a DataFrame.\n",
    "        \"\"\"\n",
    "        url = f\"https://db.netkeiba.com/horse/result/{horse_id}/\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup: return pd.DataFrame()\n",
    "\n",
    "        table = soup.select_one(\"table.db_h_race_results\")\n",
    "        if not table:\n",
    "             tables = soup.find_all(\"table\")\n",
    "             for t in tables:\n",
    "                 if \"ÁùÄÈ†Ü\" in t.text:\n",
    "                     table = t\n",
    "                     break\n",
    "        if not table: return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            df = pd.read_html(io.StringIO(str(table)))[0]\n",
    "            df = df.dropna(how='all')\n",
    "            # Normalize columns\n",
    "            df.columns = df.columns.astype(str).str.replace(r'\\s+', '', regex=True)\n",
    "            \n",
    "            # Date Parsing\n",
    "            if 'Êó•‰ªò' in df.columns:\n",
    "                df['date_obj'] = pd.to_datetime(df['Êó•‰ªò'], format='%Y/%m/%d', errors='coerce')\n",
    "                df = df.dropna(subset=['date_obj'])\n",
    "                \n",
    "                # Filter past races only\n",
    "                if isinstance(current_race_date, str):\n",
    "                    current_date = pd.to_datetime(current_race_date)\n",
    "                else:\n",
    "                    current_date = pd.to_datetime(current_race_date)\n",
    "                    \n",
    "                df = df[df['date_obj'] < current_date]\n",
    "                df = df.sort_values('date_obj', ascending=False)\n",
    "            \n",
    "            if df.empty: return df\n",
    "\n",
    "            # Limit to N\n",
    "            df = df.head(n_samples)\n",
    "\n",
    "            # Extract Run Style\n",
    "            if 'ÈÄöÈÅé' in df.columns:\n",
    "                df['run_style_val'] = df['ÈÄöÈÅé'].apply(self.extract_run_style)\n",
    "            else:\n",
    "                df['run_style_val'] = 3\n",
    "\n",
    "            # Column Mapping\n",
    "            column_map = {\n",
    "                'Êó•‰ªò': 'date', 'ÈñãÂÇ¨': 'venue', 'Â§©Ê∞ó': 'weather', '„É¨„Éº„ÇπÂêç': 'race_name',\n",
    "                'ÁùÄÈ†Ü': 'rank', 'Êû†Áï™': 'waku', 'È¶¨Áï™': 'umaban', 'È®éÊâã': 'jockey',\n",
    "                'Êñ§Èáè': 'weight_carried', 'È¶¨Â†¥': 'condition', '„Çø„Ç§„É†': 'time',\n",
    "                'ÁùÄÂ∑Æ': 'margin', '‰∏ä„Çä': 'last_3f', 'ÈÄöÈÅé': 'passing', 'È¶¨‰ΩìÈáç': 'horse_weight',\n",
    "                'run_style_val': 'run_style', 'ÂçòÂãù': 'odds', '„Ç™„ÉÉ„Ç∫': 'odds', 'Ë∑ùÈõ¢': 'raw_distance'\n",
    "            }\n",
    "            df.rename(columns=column_map, inplace=True)\n",
    "            \n",
    "            # Parse Distance/Course\n",
    "            if 'raw_distance' in df.columns:\n",
    "                def parse_dist(x):\n",
    "                    if not isinstance(x, str): return None, None\n",
    "                    surf = None; dist = None\n",
    "                    if 'Ëäù' in x: surf = 'Ëäù'\n",
    "                    elif '„ÉÄ' in x: surf = '„ÉÄ'\n",
    "                    elif 'Èöú' in x: surf = 'Èöú'\n",
    "                    match = re.search(r'(\\d+)', x)\n",
    "                    if match: dist = int(match.group(1))\n",
    "                    return surf, dist\n",
    "                \n",
    "                parsed = df['raw_distance'].apply(parse_dist)\n",
    "                df['course_type'] = parsed.apply(lambda x: x[0])\n",
    "                df['distance'] = parsed.apply(lambda x: x[1])\n",
    "            else:\n",
    "                df['course_type'] = None; df['distance'] = None\n",
    "\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# --- Main Rich Scraper Function ---\n",
    "def scrape_race_rich(url, existing_race_ids=None, max_retries=3):\n",
    "    \"\"\"\n",
    "    Scrapes Race + History + Pedigree in one go.\n",
    "    \"\"\"\n",
    "    print(f\"  „É¨„Éº„ÇπËß£Êûê‰∏≠: {url}\")\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    \n",
    "    # 1. Fetch Race Page\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=15)\n",
    "        resp.encoding = 'EUC-JP'\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        \n",
    "        # --- Metadata (Date, Venue, Race Name, etc) ---\n",
    "        # (This part reuses logic from original scrape_jra_race)\n",
    "        h1_elem = soup.select_one(\"div.header_line h1 .txt\")\n",
    "        full_text = h1_elem.text.strip() if h1_elem else (soup.h1.text.strip() if soup.h1 else \"\")\n",
    "        \n",
    "        date_text = \"\"; venue_text = \"\"; kai = \"01\"; day = \"01\"; r_num = \"10\"\n",
    "        match_date = re.search(r'(\\d{4}Âπ¥\\d{1,2}Êúà\\d{1,2}Êó•)', full_text)\n",
    "        if match_date: date_text = match_date.group(1)\n",
    "        \n",
    "        venues_str = \"Êú≠Âπå|ÂáΩÈ§®|Á¶èÂ≥∂|Êñ∞ÊΩü|Êù±‰∫¨|‰∏≠Â±±|‰∏≠‰∫¨|‰∫¨ÈÉΩ|Èò™Á•û|Â∞èÂÄâ\"\n",
    "        match_meta = re.search(rf'(\\d+)Âõû({venues_str})(\\\\d+)Êó•', full_text)\n",
    "        if match_meta:\n",
    "            kai = f\"{int(match_meta.group(1)):02}\"\n",
    "            venue_text = match_meta.group(2)\n",
    "            day = f\"{int(match_meta.group(3)):02}\"\n",
    "            \n",
    "        match_race = re.search(r'(\\d+)„É¨„Éº„Çπ', full_text)\n",
    "        if match_race: r_num = f\"{int(match_race.group(1)):02}\"\n",
    "        \n",
    "        place_map = {\n",
    "            \"Êú≠Âπå\": \"01\", \"ÂáΩÈ§®\": \"02\", \"Á¶èÂ≥∂\": \"03\", \"Êñ∞ÊΩü\": \"04\", \"Êù±‰∫¨\": \"05\",\n",
    "            \"‰∏≠Â±±\": \"06\", \"‰∏≠‰∫¨\": \"07\", \"‰∫¨ÈÉΩ\": \"08\", \"Èò™Á•û\": \"09\", \"Â∞èÂÄâ\": \"10\"\n",
    "        }\n",
    "        p_code = place_map.get(venue_text, \"00\")\n",
    "        year = date_text[:4] if date_text else \"2025\"\n",
    "        \n",
    "        race_id = f\"{year}{p_code}{kai}{day}{r_num}\"\n",
    "        \n",
    "        # SKIP Check\n",
    "        if existing_race_ids and race_id in existing_race_ids:\n",
    "            return None\n",
    "\n",
    "        # Basic Race Info\n",
    "        race_name = soup.select_one(\".race_name\").text.strip() if soup.select_one(\".race_name\") else \"\"\n",
    "        header_text = soup.select_one(\"div.header_line\").text if soup.select_one(\"div.header_line\") else full_text\n",
    "        \n",
    "        # Course/Dist\n",
    "        course_type = \"\"; distance = \"\"\n",
    "        dt_match = re.search(r'(Ëäù|„ÉÄ|„ÉÄ„Éº„Éà|ÈöúÂÆ≥)[^0-9]*(\\d+)', header_text)\n",
    "        if dt_match:\n",
    "            c = dt_match.group(1)\n",
    "            course_type = 'Ëäù' if 'Ëäù' in c else ('„ÉÄ„Éº„Éà' if '„ÉÄ' in c else 'ÈöúÂÆ≥')\n",
    "            distance = dt_match.group(2)\n",
    "            \n",
    "        # Weather/Condition\n",
    "        weather = \"\"; condition = \"\"; rotation = \"\"\n",
    "        w_match = re.search(r'Â§©ÂÄô\\s*[:Ôºö]\\s*(\\S+)', soup.text)\n",
    "        if w_match: weather = w_match.group(1)\n",
    "        \n",
    "        c_match = re.search(r'(?:Ëäù|„ÉÄ„Éº„Éà)\\s*[:Ôºö]\\s*(\\S+)', soup.text)\n",
    "        if c_match: condition = c_match.group(1)\n",
    "        \n",
    "        if \"Âè≥\" in header_text: rotation = \"Âè≥\"\n",
    "        elif \"Â∑¶\" in header_text: rotation = \"Â∑¶\"\n",
    "        elif \"Áõ¥\" in header_text: rotation = \"Áõ¥Á∑ö\"\n",
    "\n",
    "        # --- Parse Result Table ---\n",
    "        tables = soup.find_all('table')\n",
    "        target_table = None\n",
    "        for t in tables:\n",
    "            if \"ÁùÄÈ†Ü\" in t.text and \"È¶¨Âêç\" in t.text:\n",
    "                target_table = t\n",
    "                break\n",
    "        \n",
    "        if not target_table: return None\n",
    "        \n",
    "        rows = target_table.find_all('tr')\n",
    "        race_scraper = RaceScraper()\n",
    "        \n",
    "        data_list = []\n",
    "        \n",
    "        # Pre-convert date for filtering\n",
    "        d_obj = pd.to_datetime(date_text, format='%YÂπ¥%mÊúà%dÊó•') if date_text else datetime.now()\n",
    "        \n",
    "        print(f\"    Fetching details for {len(rows)-1} horses...\")\n",
    "        \n",
    "        for row in rows:\n",
    "            if row.find('th'): continue # Skip header\n",
    "            cells = row.find_all('td')\n",
    "            if not cells: continue\n",
    "            \n",
    "            # Basic info\n",
    "            rank = cells[0].text.strip()\n",
    "            waku = \"\"\n",
    "            if cells[1].find('img'): \n",
    "                alt = cells[1].find('img').get('alt', '')\n",
    "                m = re.search(r'Êû†(\\d+)', alt)\n",
    "                waku = m.group(1) if m else alt\n",
    "            umaban = cells[2].text.strip()\n",
    "            horse_name_elem = cells[3].find('a')\n",
    "            horse_name = cells[3].text.strip()\n",
    "            horse_id = \"\"\n",
    "            if horse_name_elem and 'href' in horse_name_elem.attrs:\n",
    "                hm = re.search(r'/horse/(\\d+)', horse_name_elem['href'])\n",
    "                if hm: horse_id = hm.group(1)\n",
    "            \n",
    "            jockey = cells[6].text.strip()\n",
    "            time_val = cells[7].text.strip()\n",
    "            # ... other basic fields\n",
    "            \n",
    "            # --- Rich Fetching (History & Pedigree) ---\n",
    "            blood_data = {\"father\": \"\", \"mother\": \"\", \"bms\": \"\"}\n",
    "            past_data = {}\n",
    "            \n",
    "            if horse_id:\n",
    "                # 1. Pedigree\n",
    "                blood_data = race_scraper.get_horse_profile(horse_id)\n",
    "                \n",
    "                # 2. History\n",
    "                df_past = race_scraper.get_past_races(horse_id, d_obj, n_samples=5)\n",
    "                \n",
    "                # Flatten History\n",
    "                for i in range(5):\n",
    "                    prefix = f\"past_{i+1}\"\n",
    "                    if i < len(df_past):\n",
    "                        r = df_past.iloc[i]\n",
    "                        past_data[f\"{prefix}_date\"] = r.get('date', '')\n",
    "                        past_data[f\"{prefix}_rank\"] = r.get('rank', '')\n",
    "                        past_data[f\"{prefix}_time\"] = r.get('time', '')\n",
    "                        past_data[f\"{prefix}_run_style\"] = r.get('run_style', '')\n",
    "                        past_data[f\"{prefix}_race_name\"] = r.get('race_name', '')\n",
    "                        past_data[f\"{prefix}_last_3f\"] = r.get('last_3f', '')\n",
    "                        past_data[f\"{prefix}_horse_weight\"] = r.get('horse_weight', '')\n",
    "                        past_data[f\"{prefix}_jockey\"] = r.get('jockey', '')\n",
    "                        past_data[f\"{prefix}_condition\"] = r.get('condition', '')\n",
    "                        past_data[f\"{prefix}_odds\"] = r.get('odds', '')\n",
    "                        past_data[f\"{prefix}_weather\"] = r.get('weather', '')\n",
    "                        past_data[f\"{prefix}_distance\"] = r.get('distance', '')\n",
    "                        past_data[f\"{prefix}_course_type\"] = r.get('course_type', '')\n",
    "                    else:\n",
    "                        # Fill Empty\n",
    "                        for col in ['date','rank','time','run_style','race_name','last_3f','horse_weight','jockey','condition','odds','weather','distance','course_type']:\n",
    "                            past_data[f\"{prefix}_{col}\"] = \"\"\n",
    "\n",
    "                # Be polite between horses\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            # Combine\n",
    "            row_dict = {\n",
    "                \"Êó•‰ªò\": date_text, \"‰ºöÂ†¥\": venue_text, \"„É¨„Éº„ÇπÁï™Âè∑\": f\"{r_num}R\", \"„É¨„Éº„ÇπÂêç\": race_name, \"ÈáçË≥û\": \"\",\n",
    "                \"„Ç≥„Éº„Çπ„Çø„Ç§„Éó\": course_type, \"Ë∑ùÈõ¢\": distance, \"Âõû„Çä\": rotation, \"Â§©ÂÄô\": weather, \"È¶¨Â†¥Áä∂ÊÖã\": condition,\n",
    "                \"ÁùÄÈ†Ü\": rank, \"Êû†\": waku, \"È¶¨Áï™\": umaban, \"È¶¨Âêç\": horse_name, \n",
    "                \"ÊÄßÈΩ¢\": cells[4].text.strip(), \"Êñ§Èáè\": cells[5].text.strip(), \"È®éÊâã\": jockey, \n",
    "                \"„Çø„Ç§„É†\": time_val, \"ÁùÄÂ∑Æ\": cells[8].text.strip(), \"‰∫∫Ê∞ó\": cells[13].text.strip(), \n",
    "                \"ÂçòÂãù„Ç™„ÉÉ„Ç∫\": cells[14].text.strip() if len(cells)>14 else \"0.0\", \n",
    "                \"Âæå3F\": cells[10].text.strip(), \"Âé©Ëàé\": cells[12].text.strip(), \n",
    "                \"È¶¨‰ΩìÈáç(Â¢óÊ∏õ)\": cells[11].text.strip(),\n",
    "                \"race_id\": race_id, \"horse_id\": horse_id,\n",
    "                **blood_data,\n",
    "                **past_data\n",
    "            }\n",
    "            data_list.append(row_dict)\n",
    "            \n",
    "        df = pd.DataFrame(data_list)\n",
    "        \n",
    "        # Enforce User-Specified Column Order\n",
    "        ordered_columns = [\n",
    "            \"Êó•‰ªò\", \"‰ºöÂ†¥\", \"„É¨„Éº„ÇπÁï™Âè∑\", \"„É¨„Éº„ÇπÂêç\", \"ÈáçË≥û\", \"„Ç≥„Éº„Çπ„Çø„Ç§„Éó\", \"Ë∑ùÈõ¢\", \"Âõû„Çä\", \"Â§©ÂÄô\", \"È¶¨Â†¥Áä∂ÊÖã\",\n",
    "            \"ÁùÄÈ†Ü\", \"Êû†\", \"È¶¨Áï™\", \"È¶¨Âêç\", \"ÊÄßÈΩ¢\", \"Êñ§Èáè\", \"È®éÊâã\", \"„Çø„Ç§„É†\", \"ÁùÄÂ∑Æ\", \"‰∫∫Ê∞ó\", \"ÂçòÂãù„Ç™„ÉÉ„Ç∫\",\n",
    "            \"Âæå3F\", \"Âé©Ëàé\", \"È¶¨‰ΩìÈáç(Â¢óÊ∏õ)\", \"race_id\", \"horse_id\"\n",
    "        ]\n",
    "        # rich data columns\n",
    "        for i in range(1, 6):\n",
    "            p = f\"past_{i}\"\n",
    "            ordered_columns.extend([\n",
    "                f\"{p}_date\", f\"{p}_rank\", f\"{p}_time\", f\"{p}_run_style\", f\"{p}_race_name\",\n",
    "                f\"{p}_last_3f\", f\"{p}_horse_weight\", f\"{p}_jockey\", f\"{p}_condition\",\n",
    "                f\"{p}_odds\", f\"{p}_weather\", f\"{p}_distance\", f\"{p}_course_type\"\n",
    "            ])\n",
    "        ordered_columns.extend([\"father\", \"mother\", \"bms\"])\n",
    "        \n",
    "        # Add missing cols with empty string, remove extras (if any/optional)\n",
    "        # reindex handles this safely\n",
    "        df_ordered = df.reindex(columns=ordered_columns, fill_value=\"\")\n",
    "        \n",
    "        return df_ordered\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping race {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Year/Month Iteration Logic (Scrape Year Rich) ---\n",
    "def scrape_jra_year_rich(year_str, start_date=None, end_date=None, save_callback=None, existing_race_ids=None):\n",
    "    # Parameter Map for Monthly Results\n",
    "    JRA_MONTH_PARAMS = {\n",
    "        \"2026\": { \"01\": \"E4\", \"02\": \"B2\", \"03\": \"80\", \"04\": \"4E\", \"05\": \"1C\", \"06\": \"EA\", \"07\": \"B8\", \"08\": \"86\", \"09\": \"54\", \"10\": \"22\", \"11\": \"F0\", \"12\": \"BE\" },\n",
    "        \"2025\": { \"01\": \"3F\", \"02\": \"0D\", \"03\": \"DB\", \"04\": \"A9\", \"05\": \"77\", \"06\": \"45\", \"07\": \"13\", \"08\": \"E1\", \"09\": \"AF\", \"10\": \"1E\", \"11\": \"EC\", \"12\": \"D3\" },\n",
    "        \"2024\": { \"01\": \"B3\", \"02\": \"81\", \"03\": \"4F\", \"04\": \"1D\", \"05\": \"EB\", \"06\": \"B9\", \"07\": \"87\", \"08\": \"55\", \"09\": \"23\", \"10\": \"92\", \"11\": \"60\", \"12\": \"2E\" },\n",
    "        \"2023\": { \"01\": \"27\", \"02\": \"F5\", \"03\": \"C3\", \"04\": \"91\", \"05\": \"5F\", \"06\": \"2D\", \"07\": \"FB\", \"08\": \"C9\", \"09\": \"97\", \"10\": \"06\", \"11\": \"D4\", \"12\": \"A2\" },\n",
    "        \"2022\": { \"01\": \"9B\", \"02\": \"69\", \"03\": \"37\", \"04\": \"05\", \"05\": \"D3\", \"06\": \"A1\", \"07\": \"6F\", \"08\": \"3D\", \"09\": \"0B\", \"10\": \"7A\", \"11\": \"48\", \"12\": \"16\" },\n",
    "        \"2021\": { \"01\": \"0F\", \"02\": \"DD\", \"03\": \"AB\", \"04\": \"79\", \"05\": \"47\", \"06\": \"15\", \"07\": \"E3\", \"08\": \"B1\", \"09\": \"7F\", \"10\": \"EE\", \"11\": \"BC\", \"12\": \"8A\" },\n",
    "        \"2020\": { \"01\": \"83\", \"02\": \"51\", \"03\": \"1F\", \"04\": \"ED\", \"05\": \"BB\", \"06\": \"89\", \"07\": \"57\", \"08\": \"25\", \"09\": \"F3\", \"10\": \"62\", \"11\": \"30\", \"12\": \"FE\" }\n",
    "    }\n",
    "    \n",
    "    if year_str not in JRA_MONTH_PARAMS:\n",
    "        print(f\"Âπ¥Â∫¶ {year_str} „ÅØ„Çµ„Éù„Éº„Éà„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ\")\n",
    "        return\n",
    "\n",
    "    params = JRA_MONTH_PARAMS[year_str]\n",
    "    base_url = \"https://www.jra.go.jp/JRADB/accessS.html\"\n",
    "\n",
    "    # Determine months to iterate\n",
    "    start_m = 1\n",
    "    end_m = 12\n",
    "\n",
    "    if start_date:\n",
    "        start_m = start_date.month\n",
    "    if end_date:\n",
    "        end_m = end_date.month\n",
    "\n",
    "    # Cap at Today\n",
    "    from datetime import date\n",
    "    today = date.today()\n",
    "\n",
    "    if end_date:\n",
    "        actual_end_date = min(end_date, today)\n",
    "    else:\n",
    "        actual_end_date = today\n",
    "\n",
    "    print(f\"=== JRA ‰∏ÄÊã¨„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÈñãÂßã {year_str} („É™„ÉÉ„ÉÅ„É¢„Éº„Éâ) ===\")\n",
    "    print(f\"ÊúüÈñì: {start_date or 'Start'} - {actual_end_date}\")\n",
    "    \n",
    "    if int(year_str) == today.year:\n",
    "        end_m = min(end_m, today.month)\n",
    "\n",
    "    total_processed = 0\n",
    "\n",
    "    for m in range(start_m, end_m + 1):\n",
    "        month = f\"{m:02}\"\n",
    "        if month not in params:\n",
    "            continue\n",
    "\n",
    "        suffix = params[month]\n",
    "        try:\n",
    "            ym = int(year_str + month)\n",
    "            prefix = \"pw01skl00\" if ym >= 202512 else \"pw01skl10\"\n",
    "        except:\n",
    "            prefix = \"pw01skl10\"\n",
    "\n",
    "        cname = f\"{prefix}{year_str}{month}/{suffix}\"\n",
    "\n",
    "        print(f\"\\nüìÖ {year_str}/{month} „ÇíÂèñÂæó‰∏≠...\")\n",
    "\n",
    "        try:\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "            }\n",
    "            response = requests.post(base_url, data={\"cname\": cname}, headers=headers, timeout=15)\n",
    "            response.encoding = 'cp932'\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå {cname} „ÅÆÂèñÂæó„Å´Â§±Êïó„Åó„Åæ„Åó„Åü (Status {response.status_code})\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            race_cnames = []\n",
    "            links = soup.find_all('a')\n",
    "            for link in links:\n",
    "                onclick = link.get('onclick', '')\n",
    "                match = re.search(r\"doAction\\('[^']+',\\s*'([^']+)'\\)\", onclick)\n",
    "                if match:\n",
    "                    c = match.group(1)\n",
    "                    if c.startswith('pw01srl'):\n",
    "                        race_cnames.append(c)\n",
    "\n",
    "            race_cnames = sorted(list(set(race_cnames)))\n",
    "            print(f\"  {len(race_cnames)} ÈñãÂÇ¨Êó•„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü\")\n",
    "\n",
    "            for day_cname in tqdm(race_cnames, desc=f\"  {year_str}/{month}\", leave=False):\n",
    "                resp_day = requests.post(base_url, data={\"cname\": day_cname}, headers=headers, timeout=15)\n",
    "                resp_day.encoding = 'cp932'\n",
    "                soup_day = BeautifulSoup(resp_day.text, 'html.parser')\n",
    "\n",
    "                race_list_items = []\n",
    "                all_anchors = soup_day.find_all('a')\n",
    "                for a in all_anchors:\n",
    "                    onclick = a.get('onclick', '')\n",
    "                    match_sde = re.search(r\"doAction\\s*\\(\\s*['\\\"][^'\\\"]+['\\\"]\\s*,\\s*['\\\"](pw01sde[^'\\\"]+)['\\\"]\\s*\\)\", onclick)\n",
    "                    href = a.get('href', '')\n",
    "\n",
    "                    final_url = \"\"\n",
    "                    if match_sde:\n",
    "                        final_url = f\"{base_url}?CNAME={match_sde.group(1)}\"\n",
    "                    elif 'pw01sde' in href:\n",
    "                         if 'CNAME=' in href:\n",
    "                             final_url = urllib.parse.urljoin(base_url, href)\n",
    "                         else:\n",
    "                             final_url = urllib.parse.urljoin(base_url, href)\n",
    "\n",
    "                    if final_url:\n",
    "                        race_list_items.append(final_url)\n",
    "\n",
    "                unique_races = sorted(list(set(race_list_items)))\n",
    "\n",
    "                daily_data = []\n",
    "\n",
    "                for r_link in unique_races:\n",
    "                    # Fetch with Rich Scraper\n",
    "                    df = scrape_race_rich(r_link, existing_race_ids=existing_race_ids)\n",
    "\n",
    "                    if df is not None and not df.empty:\n",
    "                        daily_data.append(df)\n",
    "                        total_processed += 1\n",
    "                    \n",
    "                    # Rate limiting\n",
    "                    time.sleep(1.0) \n",
    "                \n",
    "                # Save daily batch\n",
    "                if daily_data and save_callback:\n",
    "                    df_day = pd.concat(daily_data, ignore_index=True)\n",
    "                    save_callback(df_day) \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {month}Êúà„ÅÆÂá¶ÁêÜ‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {e}\")\n",
    "            \n",
    "    print(\"ÂÆå‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAR „Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„É≠„Ç∏„ÉÉ„ÇØ\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date, timedelta\n",
    "import calendar\n",
    "import time\n",
    "import os\n",
    "\n",
    "def run_nar_scraping(year, start_month=1, end_month=12, save_dir='data/raw'):\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    s_date = date(int(year), int(start_month), 1)\n",
    "    last_day_e = calendar.monthrange(int(year), int(end_month))[1]\n",
    "    e_date = date(int(year), int(end_month), last_day_e)\n",
    "    \n",
    "    today = date.today()\n",
    "    if e_date > today: e_date = today\n",
    "    \n",
    "    print(f'NAR„Éá„Éº„Çø„Çí {s_date} „Åã„Çâ {e_date} „Åæ„ÅßÂèñÂæó„Åó„Åæ„Åô...')\n",
    "    print(f'‰øùÂ≠òÂÖà: {os.path.join(save_dir, \"database_nar.csv\")}')\n",
    "    \n",
    "    # Êúà„Åî„Å®„Å´„É´„Éº„Éó\n",
    "    for m in range(int(start_month), int(end_month) + 1):\n",
    "        # „Åù„ÅÆÊúà„ÅÆÊó•‰ªòÁØÑÂõ≤„ÇíÊ±∫ÂÆö\n",
    "        m_start = date(int(year), m, 1)\n",
    "        m_last = calendar.monthrange(int(year), m)[1]\n",
    "        m_end = date(int(year), m, m_last)\n",
    "        \n",
    "        # ÁØÑÂõ≤Â§ñ„Å™„Çâ„Çπ„Ç≠„ÉÉ„Éó\n",
    "        if m_end < s_date or m_start > e_date:\n",
    "            continue\n",
    "            \n",
    "        # ÂÆüÈöõ„ÅÆÈñãÂßã„ÉªÁµÇ‰∫ÜÔºà„ÇØ„É©„É≥„ÉóÔºâ\n",
    "        curr_s = max(m_start, s_date)\n",
    "        curr_e = min(m_end, e_date)\n",
    "        \n",
    "        if curr_s > curr_e: continue\n",
    "        \n",
    "        # Êó•‰ªò„É™„Çπ„Éà‰ΩúÊàê\n",
    "        days = []\n",
    "        c = curr_s\n",
    "        while c <= curr_e:\n",
    "            days.append(c)\n",
    "            c += timedelta(days=1)\n",
    "            \n",
    "        print(f'\\nüìÖ {year}/{m:02} „ÇíÂèñÂæó‰∏≠...')\n",
    "        print(f'  {len(days)} Êó•ÂàÜ„ÅÆÊó•‰ªòÂØæË±°')\n",
    "        \n",
    "        for d in tqdm(days, desc=f'  {year}/{m:02}'):\n",
    "            d_str = d.strftime('%Y%m%d')\n",
    "            url = f'https://nar.netkeiba.com/top/race_list_sub.html?kaisai_date={d_str}'\n",
    "            \n",
    "            daily_data = [] # 1Êó•ÂàÜ„ÅÆ„Éá„Éº„Çø„ÇíË≤Ø„ÇÅ„Çã\n",
    "            \n",
    "            try:\n",
    "                 time.sleep(0.5)\n",
    "                 headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "                 resp = requests.get(url, headers=headers)\n",
    "                 resp.encoding = 'EUC-JP'\n",
    "                 soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "                 links = soup.select('a[href*=\"race/result.html\"]')\n",
    "                 \n",
    "                 if links:\n",
    "                     # print(f'  {d}: {len(links)} „É¨„Éº„Çπ') # „É≠„Ç∞ÈÅéÂ§öÈò≤Ê≠¢„ÅÆ„Åü„ÇÅ„Ç≥„É°„É≥„Éà„Ç¢„Ç¶„Éà\n",
    "                     for link in links:\n",
    "                         href = link.get('href')\n",
    "                         if href.startswith('../'):\n",
    "                             full_url = f'https://nar.netkeiba.com/{href.replace(\"../\", \"\")}'\n",
    "                         elif href.startswith('http'):\n",
    "                             full_url = href\n",
    "                         else:\n",
    "                             full_url = f'https://nar.netkeiba.com{href}'\n",
    "                         \n",
    "                         try:\n",
    "                             df = scrape_race_rich(full_url, existing_race_ids=None)\n",
    "                             if df is not None and not df.empty:\n",
    "                                 daily_data.append(df)\n",
    "                             time.sleep(1)\n",
    "                         except Exception as e_race:\n",
    "                             pass # „Ç®„É©„Éº„ÅØÁÑ°Ë¶ñ„Åó„Å¶Ê¨°„Å∏\n",
    "                 \n",
    "                 # 1Êó•ÂàÜ„ÅÆ„É´„Éº„ÉóÁµÇ‰∫ÜÂæå„ÄÅ„Åæ„Å®„ÇÅ„Å¶‰øùÂ≠ò\n",
    "                 if daily_data:\n",
    "                     os.makedirs(save_dir, exist_ok=True)\n",
    "                     csv_file = os.path.join(save_dir, 'database_nar.csv')\n",
    "                     try:\n",
    "                         df_day = pd.concat(daily_data, ignore_index=True)\n",
    "                         \n",
    "                         if not os.path.exists(csv_file):\n",
    "                             df_day.to_csv(csv_file, index=False)\n",
    "                         else:\n",
    "                             existing_cols = pd.read_csv(csv_file, nrows=0).columns.tolist()\n",
    "                             df_aligned = df_day.reindex(columns=existing_cols)\n",
    "                             df_aligned.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "                     except Exception as e_save:\n",
    "                          print(f\"  ‰øùÂ≠ò„Ç®„É©„Éº ({d}): {e_save}\")\n",
    "            \n",
    "            except Exception as e_day:\n",
    "                print(f'  Êó•‰ªòÂá¶ÁêÜ„Ç®„É©„Éº {d}: {e_day}')\n    \n",
    "    print('ÂÆå‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ë®≠ÂÆö („Åì„Åì„ÇíÂ§âÊõ¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ)\n",
    "YEAR = 2024          # ÂØæË±°Âπ¥Â∫¶\n",
    "START_MONTH = 1      # ÈñãÂßãÊúà\n",
    "END_MONTH = 12       # ÁµÇ‰∫ÜÊúà\n",
    "SAVE_DIR = '/content/drive/MyDrive/dai-keiba/data/raw' # ‰øùÂ≠òÂÖà„Éï„Ç©„É´„ÉÄ\n",
    "\n",
    "# ÂÆüË°å„Éñ„É≠„ÉÉ„ÇØ\n",
    "if YEAR:\n",
    "    # „Éá„Ç£„É¨„ÇØ„Éà„É™‰ΩúÊàê\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    run_nar_scraping(YEAR, START_MONTH, END_MONTH, save_dir=SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Éá„Éº„ÇøÊï¥ÁêÜ„ÉªÈáçË§áÂâäÈô§„Éª„Ç´„É©„É†È†ÜÂ∫è‰øùË®º (NAR)\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_path = os.path.join(SAVE_DIR, 'database_nar.csv')\n",
    "if os.path.exists(csv_path):\n",
    "    print('„Éá„Éº„Çø„ÅÆÊï¥ÁêÜ„ÇíË°å„Å£„Å¶„ÅÑ„Åæ„Åô...')\n",
    "    try:\n",
    "        # ÂÖ®„Ç´„É©„É†„ÇíÊñáÂ≠óÂàó„Å®„Åó„Å¶Ë™≠„ÅøËæº„ÅøÔºàÂûã„Åö„ÇåÈò≤Ê≠¢Ôºâ\n",
    "        df_final = pd.read_csv(csv_path, dtype=str)\n",
    "        before_len = len(df_final)\n",
    "        \n",
    "        # ÈáçË§áÊéíÈô§: race_id„Å®horse_id„ÅåÂêå„Åò„Å™„Çâ„ÄÅÂæåÂãù„Å°ÔºàÊúÄÊñ∞Ôºâ„ÇíÊé°Áî®\n",
    "        if 'race_id' in df_final.columns and 'horse_id' in df_final.columns:\n",
    "            df_final.drop_duplicates(subset=['race_id', 'horse_id'], keep='last', inplace=True)\n",
    "        \n",
    "        after_len = len(df_final)\n",
    "        print(f'ÈáçË§áÂâäÈô§: {before_len} -> {after_len} ({before_len - after_len}‰ª∂ÂâäÈô§)')\n",
    "        \n",
    "        df_final.to_csv(csv_path, index=False)\n",
    "        print('ÂÆå‰∫Ü: „Éá„Éº„Çø„ÅÆÊï¥ÂêàÊÄß„ÇíÁ¢∫Ë™ç„Åó‰øùÂ≠ò„Åó„Åæ„Åó„Åü„ÄÇ')\n",
    "    except Exception as e:\n",
    "        print(f'„Éá„Éº„ÇøÊï¥ÁêÜ‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {e}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}