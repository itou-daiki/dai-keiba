{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# NAR Scraping Notebook\n",
                "\n",
                "地方競馬（NAR）のデータをスクレイピングし、Google Drive上のデータセットに追加します。"
            ],
            "metadata": {
                "id": "intro_md"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# 1. Google Driveのマウント\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# ★★★ 設定項目 ★★★\n",
                "# scraperフォルダが存在するパス (Google Drive上のパス)\n",
                "# 例: '/content/drive/MyDrive/dai-keiba'\n",
                "PROJECT_PATH = '/content/drive/MyDrive/dai-keiba'\n",
                "\n",
                "if not os.path.exists(PROJECT_PATH):\n",
                "    print(f\"Error: Path {PROJECT_PATH} does not exist. Please check your Drive structure.\")\n",
                "else:\n",
                "    print(f\"Project path found: {PROJECT_PATH}\")\n",
                "    os.chdir(PROJECT_PATH)\n",
                "    sys.path.append(PROJECT_PATH)\n"
            ],
            "metadata": {
                "id": "mount_drive"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 2. 必要なライブラリのインポート\n",
                "try:\n",
                "    import pandas as pd\n",
                "    import requests\n",
                "    import bs4\n",
                "except ImportError:\n",
                "    !pip install pandas requests beautifulsoup4\n",
                "    import pandas as pd\n",
                "    import requests\n",
                "    import bs4\n",
                "\n",
                "from datetime import datetime, date\n",
                "from scraper.auto_scraper import scrape_nar_year\n",
                "import time\n"
            ],
            "metadata": {
                "id": "imports"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 3. スクレイピング実行関数の定義\n",
                "\n",
                "def nar_scrape_execution(year_str, start_date=None, end_date=None):\n",
                "    CSV_FILE_PATH_NAR = os.path.join(PROJECT_PATH, \"database_nar.csv\")\n",
                "    print(f\"Using CSV Path: {CSV_FILE_PATH_NAR}\")\n",
                "\n",
                "    def save_callback(df_new):\n",
                "        if df_new is None or df_new.empty: return\n",
                "        \n",
                "        if os.path.exists(CSV_FILE_PATH_NAR):\n",
                "            try:\n",
                "                existing = pd.read_csv(CSV_FILE_PATH_NAR, dtype={'race_id': str, 'horse_id': str})\n",
                "                combined = pd.concat([existing, df_new], ignore_index=True)\n",
                "                # Deduplicate\n",
                "                if 'race_id' in combined.columns and '馬 番' in combined.columns:\n",
                "                    combined = combined.drop_duplicates(subset=['race_id', '馬 番'], keep='last')\n",
                "                combined.to_csv(CSV_FILE_PATH_NAR, index=False)\n",
                "                print(f\"  [Saved] {len(df_new)} rows added. Total: {len(combined)}\")\n",
                "            except Exception as e:\n",
                "                print(f\"Read Error: {e}, overwriting.\")\n",
                "                df_new.to_csv(CSV_FILE_PATH_NAR, index=False)\n",
                "        else:\n",
                "            df_new.to_csv(CSV_FILE_PATH_NAR, index=False)\n",
                "            print(f\"  [Created] {CSV_FILE_PATH_NAR} with {len(df_new)} rows.\")\n",
                "\n",
                "    print(f\"Starting NAR Scraping for {year_str} ({start_date} ~ {end_date})\")\n",
                "    \n",
                "    # Load existing IDs to skip\n",
                "    existing_ids = set()\n",
                "    if os.path.exists(CSV_FILE_PATH_NAR):\n",
                "        try:\n",
                "             df_e = pd.read_csv(CSV_FILE_PATH_NAR, usecols=['race_id'], dtype={'race_id': str})\n",
                "             existing_ids = set(df_e['race_id'].astype(str))\n",
                "             print(f\"  Loaded {len(existing_ids)} existing race IDs to skip.\")\n",
                "        except:\n",
                "             pass\n",
                "\n",
                "    scrape_nar_year(year_str, start_date=start_date, end_date=end_date, save_callback=save_callback, existing_race_ids=existing_ids)\n"
            ],
            "metadata": {
                "id": "def_exec"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 4. 実行パラメータの設定と開始\n",
                "# -----------------------------\n",
                "TARGET_YEAR = \"2024\"\n",
                "TARGET_MONTH = 1  # ★何月を取得するか指定 (Noneの場合は全期間、1〜12を指定)\n",
                "\n",
                "import calendar\n",
                "from datetime import date\n",
                "\n",
                "START_DATE = None\n",
                "END_DATE = None\n",
                "\n",
                "if TARGET_MONTH:\n",
                "    # 指定した月の1日〜末日を設定\n",
                "    _, last_day = calendar.monthrange(int(TARGET_YEAR), int(TARGET_MONTH))\n",
                "    START_DATE = date(int(TARGET_YEAR), int(TARGET_MONTH), 1)\n",
                "    END_DATE = date(int(TARGET_YEAR), int(TARGET_MONTH), last_day)\n",
                "    print(f\"Targeting specific month: {START_DATE} to {END_DATE}\")\n",
                "else:\n",
                "    # 自動判定ロジック (既存データの翌日から)\n",
                "    CSV_FILE_PATH_NAR = os.path.join(PROJECT_PATH, \"database_nar.csv\")\n",
                "    if os.path.exists(CSV_FILE_PATH_NAR):\n",
                "        try:\n",
                "            df_exist = pd.read_csv(CSV_FILE_PATH_NAR)\n",
                "            if '日付' in df_exist.columns and not df_exist.empty:\n",
                "                 df_exist['date_obj'] = pd.to_datetime(df_exist['日付'], format='%Y年%m月%d日', errors='coerce')\n",
                "                 last_date = df_exist['date_obj'].max()\n",
                "                 if pd.notna(last_date):\n",
                "                     # START_DATE = last_date.date() # 旧: 続きから\n",
                "                     # 新: 欠落補完のために、強制的にその年の1月1日からスキャンする (existing_idsでスキップされる)\n",
                "                     START_DATE = date(int(TARGET_YEAR), 1, 1)\n",
                "                     print(f\"既存データの最終日時: {last_date.date()} (欠落確認のため {START_DATE} からスキャンします)\")\n",
                "        except Exception as e:\n",
                "            print(f\"既存データ確認エラー: {e}\")\n",
                "\n",
                "print(f\"Scraping Target: {TARGET_YEAR}, Start: {START_DATE}, End: {END_DATE}\")\n",
                "nar_scrape_execution(TARGET_YEAR, start_date=START_DATE, end_date=END_DATE)\n"
            ],
            "metadata": {
                "id": "run_cell"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.2 欠損データの補完 (HorseID & 過去走 & 効率化) [NAR用]\n",
                "import pandas as pd\n",
                "import sys\n",
                "import os\n",
                "import re\n",
                "from datetime import datetime\n",
                "import time\n",
                "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
                "import threading\n",
                "\n",
                "# Ensure scraper path\n",
                "sys.path.append(os.path.join(PROJECT_PATH, 'scraper'))\n",
                "from scraper.race_scraper import RaceScraper\n",
                "\n",
                "def fetch_race_horse_ids(rid):\n",
                "    scraper = RaceScraper()\n",
                "    try:\n",
                "        url = f'https://race.netkeiba.com/race/result.html?race_id={rid}'\n",
                "        soup = scraper._get_soup(url)\n",
                "        if not soup: return None\n",
                "        \n",
                "        table = soup.find('table', id='All_Result_Table')\n",
                "        if not table: return None\n",
                "        \n",
                "        horse_map = {}\n",
                "        rows = table.find_all('tr', class_='HorseList')\n",
                "        for row in rows:\n",
                "            name_tag = row.select_one('.Horse_Name a')\n",
                "            if name_tag:\n",
                "                h_name = name_tag.text.strip()\n",
                "                href = name_tag.get('href', '')\n",
                "                match = re.search(r'/horse/(\\\\d+)', href)\n",
                "                if match:\n",
                "                    horse_map[h_name] = match.group(1)\n",
                "        return (rid, horse_map)\n",
                "    except Exception as e:\n",
                "        print(f'Error fetching race {rid}: {e}')\n",
                "        return None\n",
                "\n",
                "def fetch_horse_history(horse_id):\n",
                "    scraper = RaceScraper()\n",
                "    try:\n",
                "        df = scraper.get_past_races(str(horse_id), n_samples=None)\n",
                "        return (horse_id, df)\n",
                "    except Exception as e:\n",
                "        # print(f'Error fetching horse {horse_id}: {e}')\n",
                "        return (horse_id, pd.DataFrame())\n",
                "\n",
                "def fill_missing_past_data_nar_notebook():\n",
                "    csv_path = os.path.join(PROJECT_PATH, 'database_nar.csv')\n",
                "    if not os.path.exists(csv_path):\n",
                "        print(f'Error: {csv_path} not found.')\n",
                "        return\n",
                "\n",
                "    print(f'Reading {csv_path}...')\n",
                "    df = pd.read_csv(csv_path)\n",
                "\n",
                "    if '日付' in df.columns:\n",
                "        df['date_dt'] = pd.to_datetime(df['日付'], format='%Y年%m月%d日', errors='coerce')\n",
                "    else:\n",
                "        print('Error: 日付 column not found.')\n",
                "        return\n",
                "\n",
                "    if 'horse_id' not in df.columns:\n",
                "        df['horse_id'] = None\n",
                "\n",
                "    # 1. Fill Missing Horse IDs (Optimized: Only missing)\n",
                "    if 'race_id' in df.columns:\n",
                "        missing_mask = df['horse_id'].isna() | (df['horse_id'] == '')\n",
                "        if missing_mask.any():\n",
                "            races_to_update = df.loc[missing_mask, 'race_id'].unique()\n",
                "            print(f'Need to fetch IDs for {len(races_to_update)} races...')\n",
                "            \n",
                "            with ThreadPoolExecutor(max_workers=5) as executor:\n",
                "                futures = {executor.submit(fetch_race_horse_ids, rid): rid for rid in races_to_update}\n",
                "                completed = 0\n",
                "                for future in as_completed(futures):\n",
                "                    completed += 1\n",
                "                    if completed % 10 == 0: print(f'  [IDs] {completed}/{len(races_to_update)}')\n",
                "                    result = future.result()\n",
                "                    if result:\n",
                "                        rid, horse_map = result\n",
                "                        if horse_map:\n",
                "                            indices = df[df['race_id'] == rid].index\n",
                "                            for idx in indices:\n",
                "                                h_name = df.at[idx, '馬名']\n",
                "                                if h_name in horse_map:\n",
                "                                    df.at[idx, 'horse_id'] = horse_map[h_name]\n",
                "            \n",
                "            df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
                "            print('Saved updated IDs.')\n",
                "        else:\n",
                "            print('All Horse IDs present.')\n",
                "\n",
                "    # 2. Fill Past History\n",
                "    fields_map = {\n",
                "        'date': 'date', 'rank': 'rank', 'time': 'time', 'run_style': 'run_style',\n",
                "        'race_name': 'race_name', 'last_3f': 'last_3f', 'horse_weight': 'horse_weight',\n",
                "        'jockey': 'jockey', 'condition': 'condition', 'odds': 'odds',\n",
                "        'weather': 'weather', 'distance': 'distance', 'course_type': 'course_type'\n",
                "    }\n",
                "\n",
                "    # Dtype Fix: Batch create missing columns with object dtype\n",
                "    new_cols = []\n",
                "    for k in fields_map.keys():\n",
                "        for i in range(1, 6):\n",
                "            col = f'past_{i}_{k}'\n",
                "            if col not in df.columns:\n",
                "                new_cols.append(col)\n",
                "    \n",
                "    if new_cols:\n",
                "        # Initialize with object type\n",
                "        df_new = pd.DataFrame(None, index=df.index, columns=new_cols, dtype='object')\n",
                "        df = pd.concat([df, df_new], axis=1)\n",
                "\n",
                "    # --- Optimization: Only fetch horses that have missing past data ---\n",
                "    # We check if 'past_1_date' is null. If so, that horse needs update.\n",
                "    # Check all target columns? No, checking past_1_date is usually enough.\n",
                "    if 'past_1_date' in df.columns:\n",
                "        target_horses = df[df['past_1_date'].isna()]['horse_id'].dropna().unique()\n",
                "    else:\n",
                "        target_horses = df['horse_id'].dropna().unique()\n",
                "\n",
                "    print(f'Found {len(target_horses)} horses needing update (out of {df[\"horse_id\"].nunique()} total). Fetching history...')\n",
                "    \n",
                "    history_store = {}\n",
                "\n",
                "    if len(target_horses) > 0:\n",
                "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
                "            futures = {executor.submit(fetch_horse_history, hid): hid for hid in target_horses}\n",
                "            completed = 0\n",
                "            for future in as_completed(futures):\n",
                "                completed += 1\n",
                "                if completed % 50 == 0: print(f'  [History] {completed}/{len(target_horses)}')\n",
                "                try:\n",
                "                   hid, hist_df = future.result()\n",
                "                   history_store[hid] = hist_df\n",
                "                except:\n",
                "                   pass\n",
                "\n",
                "        print('Applying history data...')\n",
                "        # Optimization: Only iterate rows that might need update would be faster but for simplicity iterate all\n",
                "        # Check `hid in history_store` to quickly skip\n",
                "        for idx, row in df.iterrows():\n",
                "            hid = row.get('horse_id')\n",
                "            if pd.isna(hid) or hid not in history_store:\n",
                "                continue\n",
                "\n",
                "            current_date = row.get('date_dt')\n",
                "            hist_df = history_store[hid]\n",
                "            if hist_df.empty: continue\n",
                "            \n",
                "            if 'date' in hist_df.columns:\n",
                "                hist_df['date_obj'] = pd.to_datetime(hist_df['date'], errors='coerce')\n",
                "            \n",
                "            if 'date_obj' not in hist_df.columns: continue\n",
                "            if pd.isna(current_date): continue\n",
                "            \n",
                "            # Filter past races\n",
                "            past_races = hist_df[hist_df['date_obj'] < current_date].sort_values('date_obj', ascending=False).head(5)\n",
                "            \n",
                "            for i, (p_idx, p_row) in enumerate(past_races.iterrows()):\n",
                "                n = i + 1\n",
                "                if n > 5: break\n",
                "                for k, v in fields_map.items():\n",
                "                    df.at[idx, f'past_{n}_{k}'] = p_row.get(v)\n",
                "\n",
                "        if 'date_dt' in df.columns: df.drop(columns=['date_dt'], inplace=True)\n",
                "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
                "        print('Done filling past data for NAR.')\n",
                "    else:\n",
                "        print('No missing past data found. Skipping.')\n",
                "\n",
                "fill_missing_past_data_nar_notebook()\n"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# 5. データ加工 (Feature Engineering) の実行\n",
                "# ----------------------------------------\n",
                "# スクレイピングした database_nar.csv から学習用データを生成します\n",
                "from ml.feature_engineering import calculate_features\n",
                "\n",
                "INPUT_CSV_NAR = os.path.join(PROJECT_PATH, \"database_nar.csv\")\n",
                "OUTPUT_CSV_NAR = os.path.join(PROJECT_PATH, \"processed_data_nar.csv\")\n",
                "\n",
                "if os.path.exists(INPUT_CSV_NAR):\n",
                "    print(\"Starting Feature Engineering (NAR)...\")\n",
                "    calculate_features(INPUT_CSV_NAR, OUTPUT_CSV_NAR)\n",
                "    print(\"Done!\")\n",
                "else:\n",
                "    print(\"Error: database_nar.csv not found.\")\n"
            ],
            "metadata": {
                "id": "feature_eng_nar"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}