{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# NAR Scraping Notebook\n","\n","地方競馬（NAR）のデータをスクレイピングし、Google Drive上のデータセットに追加します。"],"metadata":{"id":"intro_md"}},{"cell_type":"code","source":["# 1. Google Driveのマウント\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import sys\n","\n","# ★★★ 設定項目 ★★★\n","# scraperフォルダが存在するパス (Google Drive上のパス)\n","# 例: '/content/drive/MyDrive/dai-keiba'\n","PROJECT_PATH = '/content/drive/MyDrive/dai-keiba'\n","\n","if not os.path.exists(PROJECT_PATH):\n","    print(f\"Error: Path {PROJECT_PATH} does not exist. Please check your Drive structure.\")\n","else:\n","    print(f\"Project path found: {PROJECT_PATH}\")\n","    os.chdir(PROJECT_PATH)\n","    sys.path.append(PROJECT_PATH)\n"],"metadata":{"id":"mount_drive","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767614788108,"user_tz":-540,"elapsed":23958,"user":{"displayName":"伊藤大貴","userId":"16595662099933633711"}},"outputId":"d26ce124-3b6e-4816-8f41-34bea50fb7f4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Project path found: /content/drive/MyDrive/dai-keiba\n"]}]},{"cell_type":"code","source":["# 2. 必要なライブラリのインポート\n","try:\n","    import pandas as pd\n","    import requests\n","    import bs4\n","except ImportError:\n","    !pip install pandas requests beautifulsoup4\n","    import pandas as pd\n","    import requests\n","    import bs4\n","\n","from datetime import datetime, date\n","from scraper.auto_scraper import scrape_nar_year\n","import time\n"],"metadata":{"id":"imports","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767614790737,"user_tz":-540,"elapsed":2624,"user":{"displayName":"伊藤大貴","userId":"16595662099933633711"}},"outputId":"c2e03261-1552-4b77-85ce-ba8db4fa94b0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["DEBUG: auto_scraper module loaded (Version: Fix-Name-Cache)\n"]}]},{"cell_type":"code","source":["# 3. スクレイピング実行関数の定義\n","\n","def nar_scrape_execution(year_str, start_date=None, end_date=None):\n","    CSV_FILE_PATH_NAR = os.path.join(PROJECT_PATH, \"data\", \"raw\", \"database_nar.csv\")\n","    print(f\"Using CSV Path: {CSV_FILE_PATH_NAR}\")\n","\n","    def save_callback(df_new):\n","        if df_new is None or df_new.empty: return\n","\n","        if os.path.exists(CSV_FILE_PATH_NAR):\n","            try:\n","                existing = pd.read_csv(CSV_FILE_PATH_NAR, dtype={'race_id': str, 'horse_id': str}, low_memory=False)\n","                combined = pd.concat([existing, df_new], ignore_index=True)\n","                # Deduplicate\n","                if 'race_id' in combined.columns and '馬 番' in combined.columns:\n","                    combined = combined.drop_duplicates(subset=['race_id', '馬 番'], keep='last')\n","                combined.to_csv(CSV_FILE_PATH_NAR, index=False)\n","                print(f\"  [Saved] {len(df_new)} rows added. Total: {len(combined)}\")\n","            except Exception as e:\n","                # ★ CRITICAL FIX\n","                print(f\"❌ Read Error: {e}. Aborting to PREVENT OVERWRITE.\")\n","                raise e\n","        else:\n","            df_new.to_csv(CSV_FILE_PATH_NAR, index=False)\n","            print(f\"  [Created] {CSV_FILE_PATH_NAR} with {len(df_new)} rows.\")\n","\n","    print(f\"Starting NAR Scraping for {year_str} ({start_date} ~ {end_date})\")\n","\n","    # Load existing IDs to skip\n","    existing_ids = set()\n","    if os.path.exists(CSV_FILE_PATH_NAR):\n","        try:\n","             df_e = pd.read_csv(CSV_FILE_PATH_NAR, usecols=['race_id'], dtype={'race_id': str}, low_memory=False)\n","             existing_ids = set(df_e['race_id'].astype(str))\n","             print(f\"  Loaded {len(existing_ids)} existing race IDs to skip.\")\n","        except:\n","             pass\n","\n","    scrape_nar_year(year_str, start_date=start_date, end_date=end_date, save_callback=save_callback, existing_race_ids=existing_ids)\n"],"metadata":{"id":"def_exec","executionInfo":{"status":"ok","timestamp":1767614790740,"user_tz":-540,"elapsed":1,"user":{"displayName":"伊藤大貴","userId":"16595662099933633711"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# 4. 実行パラメータの設定と開始\n","# -----------------------------\n","TARGET_YEAR = \"2026\"\n","TARGET_MONTH = None  # ★何月を取得するか指定 (Noneの場合は全期間、1〜12を指定)\n","\n","import calendar\n","from datetime import date\n","\n","START_DATE = None\n","END_DATE = None\n","\n","if TARGET_MONTH:\n","    # 指定した月の1日〜末日を設定\n","    _, last_day = calendar.monthrange(int(TARGET_YEAR), int(TARGET_MONTH))\n","    START_DATE = date(int(TARGET_YEAR), int(TARGET_MONTH), 1)\n","    END_DATE = date(int(TARGET_YEAR), int(TARGET_MONTH), last_day)\n","    print(f\"Targeting specific month: {START_DATE} to {END_DATE}\")\n","else:\n","    # 自動判定ロジック (既存データの翌日から)\n","    CSV_FILE_PATH_NAR = os.path.join(PROJECT_PATH, \"data\", \"raw\", \"database_nar.csv\")\n","    if os.path.exists(CSV_FILE_PATH_NAR):\n","        try:\n","            df_exist = pd.read_csv(CSV_FILE_PATH_NAR)\n","            if '日付' in df_exist.columns and not df_exist.empty:\n","                 df_exist['date_obj'] = pd.to_datetime(df_exist['日付'], format='%Y年%m月%d日', errors='coerce')\n","                 last_date = df_exist['date_obj'].max()\n","                 if pd.notna(last_date):\n","                     # START_DATE = last_date.date() # 旧: 続きから\n","                     # 新: 欠落補完のために、強制的にその年の1月1日からスキャンする (existing_idsでスキップされる)\n","                     START_DATE = date(int(TARGET_YEAR), 1, 1)\n","                     print(f\"既存データの最終日時: {last_date.date()} (欠落確認のため {START_DATE} からスキャンします)\")\n","        except Exception as e:\n","            print(f\"既存データ確認エラー: {e}\")\n","\n","print(f\"Scraping Target: {TARGET_YEAR}, Start: {START_DATE}, End: {END_DATE}\")\n","nar_scrape_execution(TARGET_YEAR, start_date=START_DATE, end_date=END_DATE)\n"],"metadata":{"id":"run_cell","colab":{"base_uri":"https://localhost:8080/"},"outputId":"97e83ede-9e28-4e8b-d947-9ec6932bfda6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1325571933.py:23: DtypeWarning: Columns (4,5,7,9,10,18,26,28,30,32,33,34,36,38,39,41,43,45,46,47,49,51,52,54,56,58,59,60,62,64,65,67,69,71,72,73,75,77,78,80,82,84,85,86,88,90,92,93,94) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df_exist = pd.read_csv(CSV_FILE_PATH_NAR)\n"]},{"output_type":"stream","name":"stdout","text":["既存データの最終日時: 2025-12-31 (欠落確認のため 2026-01-01 からスキャンします)\n","Scraping Target: 2026, Start: 2026-01-01, End: None\n","Using CSV Path: /content/drive/MyDrive/dai-keiba/data/raw/database_nar.csv\n","Starting NAR Scraping for 2026 (2026-01-01 ~ None)\n","  Loaded 19324 existing race IDs to skip.\n","=== Starting NAR Bulk Scraping for 2026 ===\n","Checking 2026-01-01...\n","  Found 35 races.\n","  Enriching 11 horses with past data...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-780074656.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  combined = pd.concat([existing, df_new], ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["  [Saved] 11 rows added. Total: 196135\n","  Enriching 11 horses with past data...\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QX5XVvAyaXBR"},"outputs":[],"source":["# 4.2 欠損データの補完 (HorseID & 過去走 & 効率化) [NAR用]\n","import pandas as pd\n","import sys\n","import os\n","import re\n","from datetime import datetime\n","import time\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","import threading\n","\n","# Ensure scraper path\n","sys.path.append(os.path.join(PROJECT_PATH, 'scraper'))\n","from scraper.race_scraper import RaceScraper\n","\n","def fetch_race_horse_ids(rid):\n","    scraper = RaceScraper()\n","    try:\n","        url = f'https://race.netkeiba.com/race/result.html?race_id={rid}'\n","        soup = scraper._get_soup(url)\n","        if not soup: return None\n","\n","        table = soup.find('table', id='All_Result_Table')\n","        if not table: return None\n","\n","        horse_map = {}\n","        rows = table.find_all('tr', class_='HorseList')\n","        for row in rows:\n","            name_tag = row.select_one('.Horse_Name a')\n","            if name_tag:\n","                h_name = name_tag.text.strip()\n","                href = name_tag.get('href', '')\n","                match = re.search(r'/horse/(\\\\d+)', href)\n","                if match:\n","                    horse_map[h_name] = match.group(1)\n","        return (rid, horse_map)\n","    except Exception as e:\n","        print(f'Error fetching race {rid}: {e}')\n","        return None\n","\n","def fetch_horse_history(horse_id):\n","    scraper = RaceScraper()\n","    try:\n","        df = scraper.get_past_races(str(horse_id), n_samples=None)\n","        return (horse_id, df)\n","    except Exception as e:\n","        # print(f'Error fetching horse {horse_id}: {e}')\n","        return (horse_id, pd.DataFrame())\n","\n","def fill_missing_past_data_nar_notebook():\n","    csv_path = os.path.join(PROJECT_PATH, 'data', 'raw', 'database_nar.csv')\n","    if not os.path.exists(csv_path):\n","        print(f'Error: {csv_path} not found.')\n","        return\n","\n","    print(f'Reading {csv_path}...')\n","    df = pd.read_csv(csv_path)\n","\n","    if '日付' in df.columns:\n","        df['date_dt'] = pd.to_datetime(df['日付'], format='%Y年%m月%d日', errors='coerce')\n","    else:\n","        print('Error: 日付 column not found.')\n","        return\n","\n","    if 'horse_id' not in df.columns:\n","        df['horse_id'] = None\n","\n","    # 1. Fill Missing Horse IDs (Optimized: Only missing)\n","    if 'race_id' in df.columns:\n","        missing_mask = df['horse_id'].isna() | (df['horse_id'] == '')\n","        if missing_mask.any():\n","            races_to_update = df.loc[missing_mask, 'race_id'].unique()\n","            print(f'Need to fetch IDs for {len(races_to_update)} races...')\n","\n","            with ThreadPoolExecutor(max_workers=5) as executor:\n","                futures = {executor.submit(fetch_race_horse_ids, rid): rid for rid in races_to_update}\n","                completed = 0\n","                for future in as_completed(futures):\n","                    completed += 1\n","                    if completed % 10 == 0: print(f'  [IDs] {completed}/{len(races_to_update)}')\n","                    result = future.result()\n","                    if result:\n","                        rid, horse_map = result\n","                        if horse_map:\n","                            indices = df[df['race_id'] == rid].index\n","                            for idx in indices:\n","                                h_name = df.at[idx, '馬名']\n","                                if h_name in horse_map:\n","                                    df.at[idx, 'horse_id'] = horse_map[h_name]\n","\n","            df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n","            print('Saved updated IDs.')\n","        else:\n","            print('All Horse IDs present.')\n","\n","    # 2. Fill Past History\n","    fields_map = {\n","        'date': 'date', 'rank': 'rank', 'time': 'time', 'run_style': 'run_style',\n","        'race_name': 'race_name', 'last_3f': 'last_3f', 'horse_weight': 'horse_weight',\n","        'jockey': 'jockey', 'condition': 'condition', 'odds': 'odds',\n","        'weather': 'weather', 'distance': 'distance', 'course_type': 'course_type'\n","    }\n","\n","    # Dtype Fix: Batch create missing columns with object dtype\n","    new_cols = []\n","    for k in fields_map.keys():\n","        for i in range(1, 6):\n","            col = f'past_{i}_{k}'\n","            if col not in df.columns:\n","                new_cols.append(col)\n","\n","    if new_cols:\n","        # Initialize with object type\n","        df_new = pd.DataFrame(None, index=df.index, columns=new_cols, dtype='object')\n","        df = pd.concat([df, df_new], axis=1)\n","\n","    # --- Optimization: Only fetch horses that have missing past data ---\n","    # We check if 'past_1_date' is null. If so, that horse needs update.\n","    # Check all target columns? No, checking past_1_date is usually enough.\n","    if 'past_1_date' in df.columns:\n","        target_horses = df[df['past_1_date'].isna()]['horse_id'].dropna().unique()\n","    else:\n","        target_horses = df['horse_id'].dropna().unique()\n","\n","    print(f'Found {len(target_horses)} horses needing update (out of {df[\"horse_id\"].nunique()} total). Fetching history...')\n","\n","    history_store = {}\n","\n","    if len(target_horses) > 0:\n","        with ThreadPoolExecutor(max_workers=5) as executor:\n","            futures = {executor.submit(fetch_horse_history, hid): hid for hid in target_horses}\n","            completed = 0\n","            for future in as_completed(futures):\n","                completed += 1\n","                if completed % 50 == 0: print(f'  [History] {completed}/{len(target_horses)}')\n","                try:\n","                   hid, hist_df = future.result()\n","                   history_store[hid] = hist_df\n","                except:\n","                   pass\n","\n","        print('Applying history data...')\n","        # Optimization: Only iterate rows that might need update would be faster but for simplicity iterate all\n","        # Check `hid in history_store` to quickly skip\n","        for idx, row in df.iterrows():\n","            hid = row.get('horse_id')\n","            if pd.isna(hid) or hid not in history_store:\n","                continue\n","\n","            current_date = row.get('date_dt')\n","            hist_df = history_store[hid]\n","            if hist_df.empty: continue\n","\n","            if 'date' in hist_df.columns:\n","                hist_df['date_obj'] = pd.to_datetime(hist_df['date'], errors='coerce')\n","\n","            if 'date_obj' not in hist_df.columns: continue\n","            if pd.isna(current_date): continue\n","\n","            # Filter past races\n","            past_races = hist_df[hist_df['date_obj'] < current_date].sort_values('date_obj', ascending=False).head(5)\n","\n","            for i, (p_idx, p_row) in enumerate(past_races.iterrows()):\n","                n = i + 1\n","                if n > 5: break\n","                for k, v in fields_map.items():\n","                    df.at[idx, f'past_{n}_{k}'] = p_row.get(v)\n","\n","        if 'date_dt' in df.columns: df.drop(columns=['date_dt'], inplace=True)\n","        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n","        print('Done filling past data for NAR.')\n","    else:\n","        print('No missing past data found. Skipping.')\n","\n","fill_missing_past_data_nar_notebook()\n"]},{"cell_type":"code","source":["# 上書き保存の関数定義 (Robust Journaling Mode) - NAR版\n","# 確実性を最優先し、1頭ずつキャッシュファイルに記録しながら進めます。\n","\n","def fill_nar_bloodline_robust():\n","    from tqdm.auto import tqdm\n","    import os\n","    import csv\n","    import time\n","\n","    csv_path = os.path.join(PROJECT_PATH, 'data', 'raw', 'database_nar.csv')\n","    cache_path = os.path.join(PROJECT_PATH, 'data', 'nar_pedigree_cache.csv')\n","\n","    if not os.path.exists(os.path.dirname(cache_path)):\n","        os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n","\n","    # 1. Load Main Database\n","    print(f'Reading {csv_path}...')\n","    df = pd.read_csv(csv_path, low_memory=False, dtype={'race_id': str, 'horse_id': str})\n","\n","    if 'horse_id' in df.columns:\n","        df['horse_id'] = df['horse_id'].astype(str).str.replace(r'\\.0$', '', regex=True)\n","\n","    # 2. Load/Create Cache\n","    cached_ids = set()\n","    if os.path.exists(cache_path):\n","        try:\n","            cache_df = pd.read_csv(cache_path, dtype=str)\n","            cached_ids = set(cache_df['horse_id'].unique())\n","            print(f\"Loaded {len(cached_ids)} cached records from {os.path.basename(cache_path)}\")\n","        except:\n","            print(\"Cache file corrupted or empty. Starting fresh.\")\n","    else:\n","        with open(cache_path, 'w', encoding='utf-8') as f:\n","            f.write('horse_id,father,mother,bms\\n')\n","\n","    # 3. Identify Missing Horses\n","    for col in ['father', 'mother', 'bms']:\n","        if col not in df.columns: df[col] = None\n","\n","    mask_missing_db = (df['father'].isna()) | (df['father'] == '') | (df['father'] == 'nan')\n","    missing_ids = df.loc[mask_missing_db, 'horse_id'].dropna().unique()\n","    missing_ids = [h for h in missing_ids if h.isdigit()]\n","\n","    target_horses = [h for h in missing_ids if h not in cached_ids]\n","\n","    print(f\"Total Missing in DB: {len(missing_ids)}\")\n","    print(f\"Already Cached: {len(cached_ids)}\")\n","    print(f\"Target to Scrape: {len(target_horses)}\")\n","\n","    if len(target_horses) > 0:\n","        from scraper.race_scraper import RaceScraper\n","        scraper = RaceScraper()\n","\n","        print(\"Starting Row-by-Row Scraping...\")\n","\n","        with open(cache_path, 'a', encoding='utf-8', newline='') as f:\n","            writer = csv.writer(f)\n","\n","            for i, hid in enumerate(tqdm(target_horses)):\n","                try:\n","                    profile = scraper.get_horse_profile(hid)\n","                    if profile:\n","                        father = profile.get('father', '')\n","                        mother = profile.get('mother', '')\n","                        bms = profile.get('bms', '')\n","\n","                        writer.writerow([hid, father, mother, bms])\n","                        f.flush()\n","                except Exception as e:\n","                    print(f\"Error scraping {hid}: {e}\")\n","\n","    else:\n","        print(\"No new horses to scrape.\")\n","\n","    # 4. Merge Cache into Database\n","    print(\"Merging cache into database...\")\n","    if os.path.exists(cache_path):\n","        cache_df = pd.read_csv(cache_path, dtype=str)\n","        f_map = dict(zip(cache_df['horse_id'], cache_df['father']))\n","        m_map = dict(zip(cache_df['horse_id'], cache_df['mother']))\n","        b_map = dict(zip(cache_df['horse_id'], cache_df['bms']))\n","\n","        mask = df['horse_id'].isin(f_map.keys())\n","        df.loc[mask, 'father'] = df.loc[mask, 'horse_id'].map(f_map).fillna(df.loc[mask, 'father'])\n","        df.loc[mask, 'mother'] = df.loc[mask, 'horse_id'].map(m_map).fillna(df.loc[mask, 'mother'])\n","        df.loc[mask, 'bms'] = df.loc[mask, 'horse_id'].map(b_map).fillna(df.loc[mask, 'bms'])\n","\n","        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n","        print(\"✅ NAR Database updated successfully.\")\n","\n","fill_nar_bloodline_robust()"],"metadata":{"id":"ychyQ0yqfXr-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZaHOyKXjDEuK"},"source":["# 5. 血統データの補完 (Bloodline Backfill)\n","\n","取得漏れの血統データ（父、母、母父）をバックグラウンドで補完します。\n","既存のデータを確認し、欠損がある馬のみ対象とします。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wfPvytPvDEuK"},"outputs":[],"source":["# 血統データのバックフィル実行\n","from scraper.auto_scraper import fill_bloodline_data\n","\n","print(\"Starting Bloodline Backfill process...\")\n","fill_bloodline_data(mode=\"NAR\")\n","print(\"Bloodline Backfill completed.\")"]}]}