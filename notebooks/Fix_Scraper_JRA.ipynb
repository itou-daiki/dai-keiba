{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ› ï¸ JRA Scraper Fix Notebook\n",
        "\n",
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€JRAãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®ã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã®ä¿®æ­£ç‰ˆã§ã™ã€‚\n",
        "ã¾ãšã¯ãƒ‰ãƒ©ã‚¤ãƒ–ã‚’ãƒã‚¦ãƒ³ãƒˆã—ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from datetime import datetime\n",
        "\n",
        "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã®è¨­å®š (ç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„)\n",
        "PROJECT_PATH = '/content/drive/MyDrive/dai-keiba'\n",
        "sys.path.append(PROJECT_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. RaceScraper Class definition\n",
        "\n",
        "ã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹ã‚’ç›´æ¥å®šç¾©ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import io\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "class RaceScraper:\n",
        "    def __init__(self):\n",
        "        self.headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "        }\n",
        "\n",
        "    def _get_soup(self, url):\n",
        "        try:\n",
        "            time.sleep(1) # Be polite\n",
        "            response = requests.get(url, headers=self.headers, timeout=10)\n",
        "            response.encoding = response.apparent_encoding\n",
        "            if response.status_code == 200:\n",
        "                return BeautifulSoup(response.text, 'html.parser')\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "    def get_past_races(self, horse_id, n_samples=5):\n",
        "        \"\"\"\n",
        "        Fetches past n_samples race results for a given horse_id from netkeiba db.\n",
        "        Returns a DataFrame of past races.\n",
        "        \"\"\"\n",
        "        url = f\"https://db.netkeiba.com/horse/result/{horse_id}/\"\n",
        "        soup = self._get_soup(url)\n",
        "        if not soup:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # The results are usually in a table with class \"db_h_race_results\"\n",
        "        table = soup.select_one(\"table.db_h_race_results\")\n",
        "        if not table:\n",
        "            # Try to find any table with \"ç€é †\"\n",
        "            tables = soup.find_all(\"table\")\n",
        "            for t in tables:\n",
        "                if \"ç€é †\" in t.text:\n",
        "                    table = t\n",
        "                    break\n",
        "        \n",
        "        if not table:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Parse table\n",
        "        # We need to manually parse to get clean data and handle links if needed (though for past data, text is mostly fine)\n",
        "        # pd.read_html is easier for the table\n",
        "        try:\n",
        "            df = pd.read_html(io.StringIO(str(table)))[0]\n",
        "            \n",
        "            # Basic cleaning\n",
        "            df = df.dropna(how='all')\n",
        "            \n",
        "            # The columns in db.netkeiba are roughly:\n",
        "            # æ—¥ä»˜, é–‹å‚¬, å¤©æ°—, R, ãƒ¬ãƒ¼ã‚¹å, æ˜ åƒ, é ­æ•°, æ ç•ª, ... ç€é †, ... é€šé, ...\n",
        "            \n",
        "            # We want to keep: Date, Race Name, Course info, Rank, Time, Passing (Style)\n",
        "            \n",
        "            # Normalize column names (remove spaces/newlines)\n",
        "            df.columns = df.columns.astype(str).str.replace(r'\\s+', '', regex=True)\n",
        "\n",
        "            # Filter rows that look like actual races (Date column exists)\n",
        "            if 'æ—¥ä»˜' in df.columns:\n",
        "                df['date_obj'] = pd.to_datetime(df['æ—¥ä»˜'], format='%Y/%m/%d', errors='coerce')\n",
        "                df = df.dropna(subset=['date_obj'])\n",
        "                df = df.sort_values('date_obj', ascending=False)\n",
        "                \n",
        "            # Take top N\n",
        "            if n_samples:\n",
        "                df = df.head(n_samples)\n",
        "            \n",
        "            # Process Run Style (Leg Type)\n",
        "            if 'é€šé' in df.columns:\n",
        "                df['run_style_val'] = df['é€šé'].apply(self.extract_run_style)\n",
        "            else:\n",
        "                df['run_style_val'] = 3 # Unknown\n",
        "\n",
        "            # Extract/Rename Columns\n",
        "            # We want: æ—¥ä»˜, é–‹å‚¬, å¤©æ°—, R, ãƒ¬ãƒ¼ã‚¹å, æ˜ åƒ, é ­æ•°, æ ç•ª, ... ç€é †, ... é€šé, ...\n",
        "            # Important: 'ä¸Šã‚Š' (3F), 'é¦¬ä½“é‡', 'é¨æ‰‹'\n",
        "            \n",
        "            # Map standard columns if they exist\n",
        "            column_map = {\n",
        "                'æ—¥ä»˜': 'date',\n",
        "                'é–‹å‚¬': 'venue',\n",
        "                'å¤©æ°—': 'weather',\n",
        "                'ãƒ¬ãƒ¼ã‚¹å': 'race_name',\n",
        "                'ç€é †': 'rank',\n",
        "                'æ ç•ª': 'waku',\n",
        "                'é¦¬ç•ª': 'umaban',\n",
        "                'é¨æ‰‹': 'jockey',\n",
        "                'æ–¤é‡': 'weight_carried',\n",
        "                'é¦¬å ´': 'condition', # è‰¯/é‡/ç¨é‡ etc.\n",
        "                'ã‚¿ã‚¤ãƒ ': 'time',\n",
        "                'ç€å·®': 'margin',\n",
        "                'ä¸Šã‚Š': 'last_3f',\n",
        "                'é€šé': 'passing',\n",
        "                'é¦¬ä½“é‡': 'horse_weight',\n",
        "                'run_style_val': 'run_style',\n",
        "                'å˜å‹': 'odds',\n",
        "                'ã‚ªãƒƒã‚º': 'odds',\n",
        "                'è·é›¢': 'raw_distance' # e.g. \"èŠ1600\"\n",
        "            }\n",
        "            \n",
        "            # Rename available columns\n",
        "            df.rename(columns=column_map, inplace=True)\n",
        "            \n",
        "            # Extract Surface and Distance from 'raw_distance'\n",
        "            if 'raw_distance' in df.columns:\n",
        "                def parse_dist(x):\n",
        "                    if not isinstance(x, str): return None, None\n",
        "                    # \"èŠ1600\", \"ãƒ€1200\", \"éšœ3000\"\n",
        "                    # Sometimes \"èŠ1600\" or just \"1600\"\n",
        "                    surf = None\n",
        "                    dist = None\n",
        "                    if 'èŠ' in x: surf = 'èŠ'\n",
        "                    elif 'ãƒ€' in x: surf = 'ãƒ€'\n",
        "                    elif 'éšœ' in x: surf = 'éšœ'\n",
        "                    \n",
        "                    # Extract number\n",
        "                    match = re.search(r'(\\d+)', x)\n",
        "                    if match:\n",
        "                        dist = int(match.group(1))\n",
        "                    return surf, dist\n",
        "\n",
        "                parsed = df['raw_distance'].apply(parse_dist)\n",
        "                df['course_type'] = parsed.apply(lambda x: x[0])\n",
        "                df['distance'] = parsed.apply(lambda x: x[1])\n",
        "            else:\n",
        "                df['course_type'] = None\n",
        "                df['distance'] = None\n",
        "\n",
        "            # Coerce numeric\n",
        "            if 'rank' in df.columns:\n",
        "                df['rank'] = pd.to_numeric(df['rank'], errors='coerce')\n",
        "            \n",
        "            if 'odds' in df.columns:\n",
        "                 df['odds'] = pd.to_numeric(df['odds'], errors='coerce')\n",
        "            \n",
        "            # Fill missing\n",
        "            for target_col in list(column_map.values()) + ['course_type', 'distance']:\n",
        "                if target_col not in df.columns:\n",
        "                    df[target_col] = None\n",
        "                \n",
        "            return df\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing past races for {horse_id}: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def extract_run_style(self, passing_str):\n",
        "        \"\"\"\n",
        "        Converts passing order string (e.g., \"1-1-1\", \"10-10-12\") to run style (1,2,3,4).\n",
        "        1: Nige (Escape) - Lead at 1st corner\n",
        "        2: Senkou (Leader) - Within first ~4 or so\n",
        "        3: Sashi (Mid) - Midpack\n",
        "        4: Oikomi (Chaser) - Back\n",
        "        Returns integer code.\n",
        "        \"\"\"\n",
        "        if not isinstance(passing_str, str):\n",
        "            return 3 # Default to Mid\n",
        "            \n",
        "        # Clean string \"1-1-1\" -> [1, 1, 1]\n",
        "        try:\n",
        "            cleaned = re.sub(r'[^0-9-]', '', passing_str)\n",
        "            parts = [int(p) for p in cleaned.split('-') if p]\n",
        "            \n",
        "            if not parts:\n",
        "                return 3\n",
        "                \n",
        "            first_corner = parts[0]\n",
        "            \n",
        "            # Heuristics\n",
        "            if first_corner == 1:\n",
        "                return 1 # Nige\n",
        "            elif first_corner <= 4:\n",
        "                return 2 # Senkou\n",
        "            elif first_corner <= 9: # Assuming standard field size of 10-16, 9 is mid-ish limit? \n",
        "                # Actually \"Sashi\" is usually mid-rear. \n",
        "                # Let's say: 1=Lead, 2-4=Front, 5-10=Mid, >10=Back\n",
        "                return 3 # Sashi\n",
        "            else:\n",
        "                return 4 # Oikomi\n",
        "                \n",
        "        except:\n",
        "            return 3\n",
        "\n",
        "    def scrape_race_with_history(self, race_id):\n",
        "        \"\"\"\n",
        "        Detailed scraper that enters a race_result page, finding horse IDs, \n",
        "        then fetches history for each horse.\n",
        "        Returns a dictionary or structured object with the race result + history.\n",
        "        \"\"\"\n",
        "        url = f\"https://race.netkeiba.com/race/result.html?race_id={race_id}\"\n",
        "        soup = self._get_soup(url)\n",
        "        if not soup:\n",
        "            return None\n",
        "            \n",
        "        # 1. Parse Main Result Table to get Horse IDs and basic result\n",
        "        # Note: auto_scraper already does some of this, but we need Horse IDs specifically.\n",
        "        # \"All_Result_Table\"\n",
        "        \n",
        "        result_data = []\n",
        "        \n",
        "        table = soup.find(\"table\", id=\"All_Result_Table\")\n",
        "        if not table:\n",
        "            return None\n",
        "            \n",
        "        rows = table.find_all(\"tr\", class_=\"HorseList\")\n",
        "        \n",
        "        print(f\"Found {len(rows)} horses in race {race_id}. Fetching histories...\")\n",
        "        \n",
        "        for row in rows:\n",
        "            # Extract basic info\n",
        "            rank_elem = row.select_one(\".Rank\")\n",
        "            rank = rank_elem.text.strip() if rank_elem else \"\"\n",
        "            \n",
        "            horse_name_elem = row.select_one(\".Horse_Name a\")\n",
        "            horse_name = horse_name_elem.text.strip() if horse_name_elem else \"\"\n",
        "            horse_url = horse_name_elem.get(\"href\") if horse_name_elem else \"\"\n",
        "            \n",
        "            # Extract ID from URL\n",
        "            # https://db.netkeiba.com/horse/2018105027\n",
        "            horse_id = None\n",
        "            if horse_url:\n",
        "                match = re.search(r'/horse/(\\d+)', horse_url)\n",
        "                if match:\n",
        "                    horse_id = match.group(1)\n",
        "            \n",
        "            if not horse_id:\n",
        "                print(f\"  Skipping {horse_name} (No ID)\")\n",
        "                continue\n",
        "\n",
        "            print(f\"  Fetching history for {horse_name} ({horse_id})...\")\n",
        "            \n",
        "            # 2. Get Past History\n",
        "            df_past = self.get_past_races(horse_id, n_samples=5)\n",
        "            \n",
        "            # 3. Structure Data\n",
        "            # converting df_past to a list of dicts or flattened fields\n",
        "            history = []\n",
        "            if not df_past.empty:\n",
        "                for idx, r in df_past.iterrows():\n",
        "                    # Extract relevant columns\n",
        "                    # We need at least: Rank, RunStyle, Time(Seconds?), Pace?\n",
        "                    # For now just dump raw-ish data\n",
        "                    hist_item = {\n",
        "                        \"date\": r.get('æ—¥ä»˜'),\n",
        "                        \"race_name\": r.get('ãƒ¬ãƒ¼ã‚¹å'),\n",
        "                        \"rank\": r.get('ç€é †'),\n",
        "                        \"passing\": r.get('é€šé'),\n",
        "                        \"run_style\": r.get('run_style_val'),\n",
        "                        \"time\": r.get('ã‚¿ã‚¤ãƒ '),\n",
        "                        # Add more as needed for Feature Engineering\n",
        "                    }\n",
        "                    history.append(hist_item)\n",
        "            \n",
        "            entry = {\n",
        "                \"race_id\": race_id,\n",
        "                \"horse_id\": horse_id,\n",
        "                \"horse_name\": horse_name,\n",
        "                \"rank\": rank,\n",
        "                \"history\": history\n",
        "            }\n",
        "            result_data.append(entry)\n",
        "            \n",
        "        return result_data\n",
        "\n",
        "    def get_horse_profile(self, horse_id):\n",
        "        \"\"\"\n",
        "        Fetches horse profile to get pedigree (Father, Mother, Grandfather(BMS)).\n",
        "        Returns a dictionary or None.\n",
        "        \"\"\"\n",
        "        # Use pedigree page for reliable bloodline data\n",
        "        url = f\"https://db.netkeiba.com/horse/ped/{horse_id}/\"\n",
        "        soup = self._get_soup(url)\n",
        "        if not soup:\n",
        "            return None\n",
        "        \n",
        "        # Parse Blood Table\n",
        "        # table class=\"blood_table\"\n",
        "        \n",
        "        data = {\n",
        "            \"father\": \"\",\n",
        "            \"mother\": \"\",\n",
        "            \"bms\": \"\"\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            table = soup.select_one(\"table.blood_table\")\n",
        "            if table:\n",
        "                rows = table.find_all(\"tr\")\n",
        "                # 5-generation table has 32 rows usually\n",
        "                # Father at Row 0 (rowspan 16)\n",
        "                # Mother at Row 16 (rowspan 16)\n",
        "                \n",
        "                if len(rows) >= 17:\n",
        "                    # Father: Row 0, Col 0\n",
        "                    r0 = rows[0].find_all(\"td\")\n",
        "                    if r0:\n",
        "                        txt = r0[0].text.strip()\n",
        "                        # Clean: \"ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ãƒ’ãƒ¼ãƒ­ãƒ¼\\n2004 æ —æ¯›...\" -> \"ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ãƒ’ãƒ¼ãƒ­ãƒ¼\"\n",
        "                        # Take first line\n",
        "                        data[\"father\"] = txt.split('\\n')[0].strip()\n",
        "                        \n",
        "                    # Mother & BMS: Row 16\n",
        "                    r16 = rows[16].find_all(\"td\")\n",
        "                    if len(r16) >= 2:\n",
        "                        # Mother\n",
        "                        m_txt = r16[0].text.strip()\n",
        "                        data[\"mother\"] = m_txt.split('\\n')[0].strip()\n",
        "                        \n",
        "                        # BMS (Mother's Father)\n",
        "                        bms_txt = r16[1].text.strip()\n",
        "                        data[\"bms\"] = bms_txt.split('\\n')[0].strip()\n",
        "                        \n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing profile for {horse_id}: {e}\")\n",
        "            \n",
        "        return data\n",
        "\n",
        "    def get_race_metadata(self, race_id):\n",
        "        \"\"\"\n",
        "        Fetches metadata for a specific race ID from Netkeiba.\n",
        "        Returns dict with: race_name, date, venue, course_type, distance, weather, condition, turn\n",
        "        \"\"\"\n",
        "        url = f\"https://race.netkeiba.com/race/result.html?race_id={race_id}\"\n",
        "        soup = self._get_soup(url)\n",
        "        if not soup:\n",
        "            return None\n",
        "            \n",
        "        data = {\n",
        "            \"race_name\": \"\",\n",
        "            \"date\": \"\",\n",
        "            \"venue\": \"\",\n",
        "            \"course_type\": \"\",\n",
        "            \"distance\": \"\",\n",
        "            \"weather\": \"\",\n",
        "            \"condition\": \"\",\n",
        "            \"turn\": \"\", # New: Dictionary key for turn direction\n",
        "            \"race_id\": race_id\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            # Race Name\n",
        "            title_elem = soup.select_one(\".RaceName\")\n",
        "            if title_elem:\n",
        "                data[\"race_name\"] = title_elem.text.strip()\n",
        "                \n",
        "            # Date & Venue & Conditions\n",
        "            # <div class=\"RaceData01\">... 2023å¹´1æœˆ5æ—¥ ... 1å›ä¸­å±±1æ—¥ ...</div>\n",
        "            # Content: \"15:35ç™ºèµ° / èŠ1600m (å³ å¤–) / å¤©å€™:æ™´ / é¦¬å ´:è‰¯\"\n",
        "            \n",
        "            rd1 = soup.select_one(\".RaceData01\")\n",
        "            \n",
        "            if rd1:\n",
        "                txt = rd1.text.strip()\n",
        "                \n",
        "                # Weather\n",
        "                if \"å¤©å€™:æ™´\" in txt: data[\"weather\"] = \"æ™´\"\n",
        "                elif \"å¤©å€™:æ›‡\" in txt: data[\"weather\"] = \"æ›‡\"\n",
        "                elif \"å¤©å€™:å°é›¨\" in txt: data[\"weather\"] = \"å°é›¨\"\n",
        "                elif \"å¤©å€™:é›¨\" in txt: data[\"weather\"] = \"é›¨\"\n",
        "                elif \"å¤©å€™:é›ª\" in txt: data[\"weather\"] = \"é›ª\"\n",
        "                \n",
        "                # Condition\n",
        "                if \"é¦¬å ´:è‰¯\" in txt: data[\"condition\"] = \"è‰¯\"\n",
        "                elif \"é¦¬å ´:ç¨\" in txt: data[\"condition\"] = \"ç¨é‡\" # Covers ç¨é‡\n",
        "                elif \"é¦¬å ´:é‡\" in txt: data[\"condition\"] = \"é‡\"\n",
        "                elif \"é¦¬å ´:ä¸è‰¯\" in txt: data[\"condition\"] = \"ä¸è‰¯\"\n",
        "                \n",
        "                # Course & Distance (\"èŠ1600m\")\n",
        "                # Regex for \"èŠ\", \"ãƒ€\", \"éšœ\" followed by digits\n",
        "                match = re.search(r'(èŠ|ãƒ€|éšœ)(\\d+)m', txt)\n",
        "                if match:\n",
        "                    ctype_raw = match.group(1)\n",
        "                    if ctype_raw == \"èŠ\": data[\"course_type\"] = \"èŠ\"\n",
        "                    elif ctype_raw == \"ãƒ€\": data[\"course_type\"] = \"ãƒ€ãƒ¼ãƒˆ\"\n",
        "                    elif ctype_raw == \"éšœ\": data[\"course_type\"] = \"éšœå®³\"\n",
        "                    \n",
        "                    data[\"distance\"] = match.group(2)\n",
        "                \n",
        "                # Turn Direction (\"å³\", \"å·¦\", \"ç›´ç·š\")\n",
        "                # Usually in parentheses like \"(å³)\" or \"(å·¦)\" or \"(èŠ å·¦)\"\n",
        "                if \"å³\" in txt: data[\"turn\"] = \"å³\"\n",
        "                elif \"å·¦\" in txt: data[\"turn\"] = \"å·¦\"\n",
        "                elif \"ç›´ç·š\" in txt: data[\"turn\"] = \"ç›´\"\n",
        "\n",
        "            # Date\n",
        "            # Try finding date in Title or dedicated element\n",
        "            date_elem = soup.select_one(\"dl#RaceList_DateList dd.Active\") \n",
        "            if date_elem:\n",
        "                 # Usually \"1æœˆ5æ—¥(é‡‘)\" - needs Year\n",
        "                 # We can rely on the fact that race_id contains year (2025...)\n",
        "                 # But let's look for YYYYå¹´ in the whole text or title\n",
        "                 pass\n",
        "            \n",
        "            # Fallback Date from Title Tag or Meta\n",
        "            if not data[\"date\"]:\n",
        "                 meta_title = soup.title.text if soup.title else \"\"\n",
        "                 match_date = re.search(r'(\\d{4}å¹´\\d{1,2}æœˆ\\d{1,2}æ—¥)', meta_title)\n",
        "                 if match_date:\n",
        "                     data[\"date\"] = match_date.group(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing metadata for {race_id}: {e}\")\n",
        "            \n",
        "        return data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test\n",
        "    scraper = RaceScraper()\n",
        "    print(\"Running test...\")\n",
        "    # Example: Do Deuce (2019105219)\n",
        "    # url = \"https://db.netkeiba.com/horse/2019105219/\"\n",
        "    # print(f\"Fetching {url}\")\n",
        "    df = scraper.get_past_races(\"2019105219\")\n",
        "    if df.empty:\n",
        "        print(\"DF is empty. Checking raw soup for 'db_h_race_results'...\")\n",
        "        soup = scraper._get_soup(f\"https://db.netkeiba.com/horse/result/2019105219/\")\n",
        "        if soup:\n",
        "             t = soup.select_one(\"table.db_h_race_results\")\n",
        "             print(f\"Selector 'table.db_h_race_results' found: {t is not None}\")\n",
        "             if not t:\n",
        "                 print(\"Trying fallback 'table' with 'ç€é †'...\")\n",
        "                 tables = soup.find_all(\"table\")\n",
        "                 found = False\n",
        "                 for i, tbl in enumerate(tables):\n",
        "                     print(f\"Table {i} classes: {tbl.get('class')}\")\n",
        "                     if \"ç€é †\" in tbl.text or \"ç€ é †\" in tbl.text or \"æ—¥ä»˜\" in tbl.text:\n",
        "                         print(\"Found a table with 'ç€é †/æ—¥ä»˜'.\")\n",
        "                         # print(str(tbl)[:200])\n",
        "                         t = tbl\n",
        "                         found = True\n",
        "                         break\n",
        "                 if not found:\n",
        "                     print(\"No table with 'ç€é †' found in soup.\")\n",
        "                     print(\"Soup snippet:\", soup.text[:500])\n",
        "                 else:\n",
        "                    # Retry parsing with found table\n",
        "                     try:\n",
        "                        df = pd.read_html(str(t))[0]\n",
        "                        print(\"Retry DF Head:\")\n",
        "                        print(df.head())\n",
        "                     except Exception as e:\n",
        "                        print(f\"Retry parsing failed: {e}\")\n",
        "        else:\n",
        "            print(\"Soup is None.\")\n",
        "    else:\n",
        "        print(df.head())\n",
        "        print(\"Columns:\", df.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Wrapper Functions\n",
        "\n",
        "æ¬ è½ã—ã¦ã„ãŸé–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_race_horse_ids(race_id):\n",
        "    \"\"\"\n",
        "    RaceScraperã‚’ä½¿ã£ã¦ãƒ¬ãƒ¼ã‚¹çµæœãƒšãƒ¼ã‚¸ã‹ã‚‰é¦¬IDã‚’å–å¾—ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°\n",
        "    Returns: (race_id, {horse_name: horse_id})\n",
        "    \"\"\"\n",
        "    scraper = RaceScraper()\n",
        "    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ\n",
        "    data = scraper.scrape_race_with_history(race_id)\n",
        "    if not data:\n",
        "        return None\n",
        "    \n",
        "    # {é¦¬å: ID} ã®è¾æ›¸ã‚’ä½œæˆ\n",
        "    id_map = {}\n",
        "    for entry in data:\n",
        "        if entry.get('horse_name') and entry.get('horse_id'):\n",
        "            id_map[entry['horse_name']] = entry['horse_id']\n",
        "            \n",
        "    return race_id, id_map\n",
        "\n",
        "def fetch_horse_history(horse_id):\n",
        "    \"\"\"\n",
        "    RaceScraperã‚’ä½¿ã£ã¦é¦¬ã®éå»æˆç¸¾ã‚’å–å¾—ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°\n",
        "    Returns: (horse_id, history_dataframe)\n",
        "    \"\"\"\n",
        "    scraper = RaceScraper()\n",
        "    df = scraper.get_past_races(horse_id, n_samples=5) # ç›´è¿‘5èµ°ã®ã¿å–å¾—\n",
        "    return horse_id, df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. JRA Data Fix Script\n",
        "\n",
        "ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦æ¬ æãƒ‡ãƒ¼ã‚¿ã‚’è£œå®Œã—ã¦ãã ã•ã„ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# JRAã‚³ãƒ¼ãƒ‰\n",
        "# 4.2 æ¬ æãƒ‡ãƒ¼ã‚¿ã®è£œå®Œ (JRA Version)\n",
        "\n",
        "try:\n",
        "    from tqdm.auto import tqdm\n",
        "except ImportError:\n",
        "    !pip install tqdm\n",
        "    from tqdm.auto import tqdm\n",
        "\n",
        "def fill_missing_past_data_jra_debug():\n",
        "    csv_path = os.path.join(PROJECT_PATH, 'data', 'raw', 'database.csv')\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f'Error: {csv_path} not found.')\n",
        "        return\n",
        "\n",
        "    print(f'Reading {csv_path}...')\n",
        "    df = pd.read_csv(csv_path, low_memory=False, dtype={'race_id': str, 'horse_id': str})\n",
        "    \n",
        "    # ID Cleaning\n",
        "    if 'horse_id' in df.columns:\n",
        "        df['horse_id'] = df['horse_id'].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
        "\n",
        "    if 'æ—¥ä»˜' in df.columns:\n",
        "        df['date_dt'] = pd.to_datetime(df['æ—¥ä»˜'], format='%Yå¹´%mæœˆ%dæ—¥', errors='coerce')\n",
        "    else:\n",
        "        print('Error: æ—¥ä»˜ column not found.')\n",
        "        return\n",
        "\n",
        "    def save_df_safe(dataframe, msg=\"\"):\n",
        "        try:\n",
        "             out_df = dataframe.drop(columns=['date_dt', 'date_obj'], errors='ignore')\n",
        "             out_df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
        "             print(f\"  [Saved] {msg}\")\n",
        "        except Exception as e:\n",
        "             print(f\"  [Save Failed] {e}\")\n",
        "\n",
        "    # === Fill Past History Only ===\n",
        "    unique_horses = df['horse_id'].dropna().unique()\n",
        "    \n",
        "    # Resume Logic\n",
        "    if 'past_1_date' in df.columns:\n",
        "        done_horses = df.loc[df['past_1_date'].notna(), 'horse_id'].unique()\n",
        "        unique_horses = [h for h in unique_horses if h not in done_horses]\n",
        "        \n",
        "    print(f'Debugging history for {len(unique_horses)} horses...')\n",
        "\n",
        "    horse_batch_size = 50 \n",
        "    fields_map = {\n",
        "        'date': 'date', 'rank': 'rank', 'time': 'time', 'race_name': 'race_name', \n",
        "        'last_3f': 'last_3f', 'horse_weight': 'horse_weight', 'jockey': 'jockey', \n",
        "        'condition': 'condition', 'odds': 'odds', 'weather': 'weather', \n",
        "        'distance': 'distance', 'course_type': 'course_type'\n",
        "    }\n",
        "    \n",
        "    for k in fields_map.keys():\n",
        "        for i in range(1, 6):\n",
        "            col = f'past_{i}_{k}'\n",
        "            if col not in df.columns: df[col] = None\n",
        "\n",
        "    for i in range(0, len(unique_horses), horse_batch_size):\n",
        "        batch_horses = unique_horses[i:i+horse_batch_size]\n",
        "        print(f\"Processing Batch {i//horse_batch_size + 1} ({len(batch_horses)} horses)...\")\n",
        "        \n",
        "        history_store = {}\n",
        "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "            futures = {executor.submit(fetch_horse_history, hid): hid for hid in batch_horses}\n",
        "            for future in tqdm(as_completed(futures), total=len(batch_horses), leave=False):\n",
        "                try:\n",
        "                    hid, hist_df = future.result()\n",
        "                    if not hist_df.empty:\n",
        "                        if 'date' in hist_df.columns:\n",
        "                            hist_df['date_obj'] = pd.to_datetime(hist_df['date'], errors='coerce')\n",
        "                            history_store[str(hid)] = hist_df\n",
        "                except: pass\n",
        "        \n",
        "        modified_batch = False\n",
        "        updates_count = 0\n",
        "        \n",
        "        if history_store:\n",
        "            mask_batch = df['horse_id'].isin(history_store.keys())\n",
        "            target_indices = df[mask_batch].index\n",
        "            \n",
        "            for idx in target_indices:\n",
        "                hid = str(df.at[idx, 'horse_id'])\n",
        "                current_date = df.at[idx, 'date_dt']\n",
        "                \n",
        "                if hid in history_store:\n",
        "                    hist_df = history_store[hid]\n",
        "                    if 'date_obj' not in hist_df.columns: continue\n",
        "                    \n",
        "                    past_races = hist_df[hist_df['date_obj'] < current_date].sort_values('date_obj', ascending=False).head(5)\n",
        "                    \n",
        "                    if not past_races.empty:\n",
        "                        for j, (p_idx, p_row) in enumerate(past_races.iterrows()):\n",
        "                            n = j + 1\n",
        "                            for k, v in fields_map.items():\n",
        "                                df.at[idx, f'past_{n}_{k}'] = p_row.get(v)\n",
        "                                modified_batch = True\n",
        "                        updates_count += 1\n",
        "            \n",
        "            if modified_batch:\n",
        "                save_df_safe(df, f\"Batch {i//horse_batch_size + 1}\")\n",
        "                import gc\n",
        "                gc.collect()\n",
        "            else:\n",
        "                print(\"  âš ï¸ No data updated in this batch.\")\n",
        "        else:\n",
        "            print(\"  âš ï¸ No history data fetched for this batch.\")\n",
        "\n",
        "    if 'date_dt' in df.columns: df.drop(columns=['date_dt'], inplace=True, errors='ignore')\n",
        "    print('Done filling past data for JRA.')\n",
        "\n",
        "fill_missing_past_data_jra_debug()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}