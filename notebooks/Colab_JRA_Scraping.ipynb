{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèá JRA Â∑ÆÂàÜ„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ (Master ID Based)\n",
    "„Åì„ÅÆ„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„ÅØ `race_ids.csv` („Éû„Çπ„Çø„Éº„É™„Çπ„Éà) „Å® `database.csv` (Êó¢Â≠ò„Éá„Éº„Çø) „ÇíÊØîËºÉ„Åó„ÄÅ\n",
    "**„Åæ„Å†ÂèñÂæó„Åó„Å¶„ÅÑ„Å™„ÅÑ„É¨„Éº„Çπ„ÅÆ„Åø** „Çí„Éî„É≥„Éù„Ç§„É≥„Éà„ÅßÈ´òÈÄü„Å´ÂèñÂæó„Åó„Åæ„Åô„ÄÇ\n",
    "‰∫ãÂâç„Å´ `Colab_ID_Fetcher.ipynb` „ÇíÂÆüË°å„Åó„Å¶„Éû„Çπ„Çø„Éº„É™„Çπ„Éà„ÇíÊõ¥Êñ∞„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive Mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ°Ô∏è Keep-Alive („Ç¢„Ç§„Éâ„É´„Çø„Ç§„É†„Ç¢„Ç¶„ÉàÂõûÈÅø)\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "display(Javascript('''\n",
    "function ClickConnect(){\n",
    "    console.log(\"Keep-alive: Working\");\n",
    "    var buttons = document.querySelectorAll(\"colab-connect-button\");\n",
    "    buttons.forEach(function(btn){\n",
    "        btn.click();\n",
    "    });\n",
    "}\n",
    "setInterval(ClickConnect, 60000);\n",
    "console.log(\"Keep-alive script started - clicks every 60 seconds\");\n",
    "'''))\n",
    "\n",
    "print(\"‚úÖ Keep-alive activated (auto-clicks every 60 seconds)\")\n",
    "print(\"üí° This prevents idle timeout during long scraping sessions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "SAVE_DIR = '/content/drive/MyDrive/dai-keiba/data/raw'\n",
    "TARGET_CSV = 'database.csv'     # Existing Data\n",
    "MASTER_ID_CSV = 'race_ids.csv'  # Master List from ID Fetcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ ÊúÄÈÅ©Âåñ„Éò„É´„Éë„ÉºÈñ¢Êï∞\\n",
    "import pandas as pd\\n",
    "import gc\\n",
    "import os\\n",
    "import shutil\\n",
    "from typing import Set, Optional\\n",
    "\\n",
    "# psutil„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´\\n",
    "try:\\n",
    "    import psutil\\n",
    "except ImportError:\\n",
    "    !pip install -q psutil\\n",
    "    import psutil\\n",
    "\\n",
    "def check_memory_usage() -> float:\\n",
    "    \\\"\\\"\\\"ÁèæÂú®„ÅÆ„É°„É¢„É™‰ΩøÁî®Èáè„ÇíÁ¢∫Ë™ç(MBÂçò‰Ωç)\\\"\\\"\\\"\\n",
    "    process = psutil.Process(os.getpid())\\n",
    "    return process.memory_info().rss / 1024 / 1024\\n",
    "\\n",
    "def log_memory(label: str = \\\"\\\", threshold_mb: float = 10000) -> None:\\n",
    "    \\\"\\\"\\\"„É°„É¢„É™‰ΩøÁî®Èáè„Çí„É≠„Ç∞Âá∫Âäõ\\\"\\\"\\\"\\n",
    "    mem_mb = check_memory_usage()\\n",
    "    status = \\\"‚ö†Ô∏è\\\" if mem_mb > threshold_mb else \\\"üíæ\\\"\\n",
    "    print(f\\\"  {status} Memory: {mem_mb:.1f} MB {label}\\\")\\n",
    "    if mem_mb > threshold_mb:\\n",
    "        print(f\\\"  ‚ö†Ô∏è  „É°„É¢„É™‰ΩøÁî®Èáè„ÅåÈ´ò„ÅÑ! GCÂÆüË°å...\\\")\\n",
    "        gc.collect()\\n",
    "\\n",
    "class HorseHistoryCache:\\n",
    "    \\\"\\\"\\\"È¶¨„ÅÆÂ±•Ê≠¥„Éá„Éº„Çø„Çí„Ç≠„É£„ÉÉ„Ç∑„É•\\\"\\\"\\\"\\n",
    "    def __init__(self, max_size: int = 1000):\\n",
    "        self.cache = {}\\n",
    "        self.max_size = max_size\\n",
    "        self.hits = 0\\n",
    "        self.misses = 0\\n",
    "    \\n",
    "    def get(self, horse_id: str, race_date: str) -> Optional[pd.DataFrame]:\\n",
    "        cache_key = f\\\"{horse_id}_{race_date}\\\"\\n",
    "        if cache_key in self.cache:\\n",
    "            self.hits += 1\\n",
    "            return self.cache[cache_key].copy()\\n",
    "        self.misses += 1\\n",
    "        return None\\n",
    "    \\n",
    "    def put(self, horse_id: str, race_date: str, df: pd.DataFrame) -> None:\\n",
    "        cache_key = f\\\"{horse_id}_{race_date}\\\"\\n",
    "        if len(self.cache) >= self.max_size:\\n",
    "            oldest_key = next(iter(self.cache))\\n",
    "            del self.cache[oldest_key]\\n",
    "        self.cache[cache_key] = df.copy()\\n",
    "    \\n",
    "    def stats(self) -> str:\\n",
    "        total = self.hits + self.misses\\n",
    "        hit_rate = (self.hits / total * 100) if total > 0 else 0\\n",
    "        return f\\\"Cache: {len(self.cache)} entries, Hit rate: {hit_rate:.1f}% ({self.hits}/{total})\\\"\\n",
    "\\n",
    "def fetch_with_retry(url: str, headers: dict, max_retries: int = 3) -> Optional[object]:\\n",
    "    \\\"\\\"\\\"„É™„Éà„É©„Ç§„É≠„Ç∏„ÉÉ„ÇØ‰ªò„ÅçHTTP„É™„ÇØ„Ç®„Çπ„Éà\\\"\\\"\\\"\\n",
    "    import requests\\n",
    "    import time\\n",
    "    for attempt in range(max_retries):\\n",
    "        try:\\n",
    "            resp = requests.get(url, headers=headers, timeout=15)\\n",
    "            if resp.status_code in [403, 429]:\\n",
    "                wait_time = (2 ** attempt) * 5\\n",
    "                print(f\\\"  ‚ö†Ô∏è  „É¨„Éº„ÉàÂà∂Èôê (Status {resp.status_code}). {wait_time}ÁßíÂæÖÊ©ü...\\\")\\n",
    "                time.sleep(wait_time)\\n",
    "                continue\\n",
    "            if resp.status_code == 200:\\n",
    "                return resp\\n",
    "        except requests.exceptions.Timeout:\\n",
    "            print(f\\\"  ‚ö†Ô∏è  Timeout (attempt {attempt + 1}/{max_retries})\\\")\\n",
    "        except requests.exceptions.RequestException as e:\\n",
    "            print(f\\\"  ‚ö†Ô∏è  Request Error: {e}\\\")\\n",
    "        if attempt < max_retries - 1:\\n",
    "            time.sleep(2)\\n",
    "    return None\\n",
    "\\n",
    "def deduplicate_in_chunks(csv_path: str, chunk_size: int = 10000) -> None:\\n",
    "    \\\"\\\"\\\"„ÉÅ„É£„É≥„ÇØÂçò‰Ωç„ÅßÈáçË§áÂâäÈô§(„É°„É¢„É™ÂäπÁéáÁöÑ)\\\"\\\"\\\"\\n",
    "    if not os.path.exists(csv_path):\\n",
    "        return\\n",
    "    print(f\\\"  üîÑ „ÉÅ„É£„É≥„ÇØÂçò‰Ωç„ÅßÈáçË§áÂâäÈô§‰∏≠... (chunk_size={chunk_size})\\\")\\n",
    "    seen: Set[str] = set()\\n",
    "    temp_path = csv_path + '.tmp'\\n",
    "    try:\\n",
    "        headers = pd.read_csv(csv_path, nrows=0).columns.tolist()\\n",
    "        if 'race_id' not in headers or 'horse_id' not in headers:\\n",
    "            print(\\\"  ‚ö†Ô∏è  race_id „Åæ„Åü„ÅØ horse_id „Ç´„É©„É†„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì\\\")\\n",
    "            return\\n",
    "        first_chunk = True\\n",
    "        total_rows = 0\\n",
    "        unique_rows = 0\\n",
    "        for chunk in pd.read_csv(csv_path, dtype=str, chunksize=chunk_size, low_memory=False):\\n",
    "            total_rows += len(chunk)\\n",
    "            chunk['_key'] = chunk['race_id'].fillna('') + '_' + chunk['horse_id'].fillna('')\\n",
    "            chunk_dedup = chunk[~chunk['_key'].isin(seen)]\\n",
    "            seen.update(chunk_dedup['_key'].tolist())\\n",
    "            unique_rows += len(chunk_dedup)\\n",
    "            chunk_dedup = chunk_dedup.drop('_key', axis=1)\\n",
    "            chunk_dedup.to_csv(temp_path, mode='a', header=first_chunk, index=False)\\n",
    "            first_chunk = False\\n",
    "            del chunk, chunk_dedup\\n",
    "            gc.collect()\\n",
    "        shutil.move(temp_path, csv_path)\\n",
    "        duplicates = total_rows - unique_rows\\n",
    "        print(f\\\"  ‚úÖ ÈáçË§áÂâäÈô§ÂÆå‰∫Ü: {total_rows} ‚Üí {unique_rows} rows ({duplicates} duplicates removed)\\\")\\n",
    "    except Exception as e:\\n",
    "        print(f\\\"  ‚ùå ÈáçË§áÂâäÈô§„Ç®„É©„Éº: {e}\\\")\\n",
    "        if os.path.exists(temp_path):\\n",
    "            os.remove(temp_path)\\n",
    "\\n",
    "# „Ç∞„É≠„Éº„Éê„É´„Ç≠„É£„ÉÉ„Ç∑„É•„ÇíÂàùÊúüÂåñ\\n",
    "horse_cache = HorseHistoryCache()\\n",
    "\\n",
    "print(\\\"‚úÖ ÊúÄÈÅ©Âåñ„Éò„É´„Éë„ÉºÈñ¢Êï∞„Åå„É≠„Éº„Éâ„Åï„Çå„Åæ„Åó„Åü\\\")\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2026-01-08: Rich Scraping Logic (JRA/NAR Universal)\n",
    "# This code handles scraping of Race Result + Horse History + Pedigree in one pass.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "from datetime import datetime\n",
    "import urllib.parse\n",
    "import time\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- RaceScraper Helper Class (Embedded) ---\n",
    "class RaceScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "    def _get_soup(self, url):\n",
    "        try:\n",
    "            time.sleep(1) # Be polite\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.encoding = response.apparent_encoding\n",
    "            if response.status_code == 200:\n",
    "                return BeautifulSoup(response.text, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def get_horse_profile(self, horse_id):\n",
    "        \"\"\"\n",
    "        Fetches horse profile to get pedigree (Father, Mother, Grandfather(BMS)).\n",
    "        Returns a dictionary or None.\n",
    "        \"\"\"\n",
    "        url = f\"https://db.netkeiba.com/horse/ped/{horse_id}/\"\n",
    "        soup = self._get_soup(url)\n",
    "        data = {\"father\": \"\", \"mother\": \"\", \"bms\": \"\"}\n",
    "        if not soup: return data\n",
    "\n",
    "        try:\n",
    "            table = soup.select_one(\"table.blood_table\")\n",
    "            if table:\n",
    "                rows = table.find_all(\"tr\")\n",
    "                if len(rows) >= 17:\n",
    "                    # Father: Row 0\n",
    "                    r0 = rows[0].find_all(\"td\")\n",
    "                    if r0:\n",
    "                        txt = r0[0].text.strip()\n",
    "                        data[\"father\"] = txt.split('\\n')[0].strip()\n",
    "\n",
    "                    # Mother & BMS: Row 16\n",
    "                    r16 = rows[16].find_all(\"td\")\n",
    "                    if len(r16) >= 2:\n",
    "                        m_txt = r16[0].text.strip()\n",
    "                        data[\"mother\"] = m_txt.split('\\n')[0].strip()\n",
    "                        bms_txt = r16[1].text.strip()\n",
    "                        data[\"bms\"] = bms_txt.split('\\n')[0].strip()\n",
    "        except Exception as e:\n",
    "            pass # Silent fail for profile\n",
    "        return data\n",
    "\n",
    "    def extract_run_style(self, passing_str):\n",
    "        if not isinstance(passing_str, str): return 3\n",
    "        try:\n",
    "            cleaned = re.sub(r'[^0-9-]', '', passing_str)\n",
    "            parts = [int(p) for p in cleaned.split('-') if p]\n",
    "            if not parts: return 3\n",
    "            first_corner = parts[0]\n",
    "            if first_corner == 1: return 1 # Nige\n",
    "            elif first_corner <= 4: return 2 # Senkou\n",
    "            elif first_corner <= 9: return 3 # Sashi\n",
    "            else: return 4 # Oikomi\n",
    "        except: return 3\n",
    "\n",
    "    def get_past_races(self, horse_id, current_race_date, n_samples=5):\n",
    "        \"\"\"\n",
    "        Fetches past n_samples race results.\n",
    "        Filters out races AFTER current_race_date.\n",
    "        Returns a DataFrame.\n",
    "        \"\"\"\n",
    "        url = f\"https://db.netkeiba.com/horse/result/{horse_id}/\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup: return pd.DataFrame()\n",
    "\n",
    "        table = soup.select_one(\"table.db_h_race_results\")\n",
    "        if not table:\n",
    "             tables = soup.find_all(\"table\")\n",
    "             for t in tables:\n",
    "                 if \"ÁùÄÈ†Ü\" in t.text:\n",
    "                     table = t\n",
    "                     break\n",
    "        if not table: return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            df = pd.read_html(io.StringIO(str(table)))[0]\n",
    "            df = df.dropna(how='all')\n",
    "            # Normalize columns\n",
    "            df.columns = df.columns.astype(str).str.replace(r'\\s+', '', regex=True)\n",
    "            \n",
    "            # Date Parsing\n",
    "            if 'Êó•‰ªò' in df.columns:\n",
    "                df['date_obj'] = pd.to_datetime(df['Êó•‰ªò'], format='%Y/%m/%d', errors='coerce')\n",
    "                df = df.dropna(subset=['date_obj'])\n",
    "                \n",
    "                # Filter past races only\n",
    "                if isinstance(current_race_date, str):\n",
    "                    current_date = pd.to_datetime(current_race_date)\n",
    "                else:\n",
    "                    current_date = pd.to_datetime(current_race_date)\n",
    "                    \n",
    "                df = df[df['date_obj'] < current_date]\n",
    "                df = df.sort_values('date_obj', ascending=False)\n",
    "            \n",
    "            if df.empty: return df\n",
    "\n",
    "            # Limit to N\n",
    "            df = df.head(n_samples)\n",
    "\n",
    "            # Extract Run Style\n",
    "            if 'ÈÄöÈÅé' in df.columns:\n",
    "                df['run_style_val'] = df['ÈÄöÈÅé'].apply(self.extract_run_style)\n",
    "            else:\n",
    "                df['run_style_val'] = 3\n",
    "\n",
    "            # Column Mapping\n",
    "            column_map = {\n",
    "                'Êó•‰ªò': 'date', 'ÈñãÂÇ¨': 'venue', 'Â§©Ê∞ó': 'weather', '„É¨„Éº„ÇπÂêç': 'race_name',\n",
    "                'ÁùÄÈ†Ü': 'rank', 'Êû†Áï™': 'waku', 'È¶¨Áï™': 'umaban', 'È®éÊâã': 'jockey',\n",
    "                'Êñ§Èáè': 'weight_carried', 'È¶¨Â†¥': 'condition', '„Çø„Ç§„É†': 'time',\n",
    "                'ÁùÄÂ∑Æ': 'margin', '‰∏ä„Çä': 'last_3f', 'ÈÄöÈÅé': 'passing', 'È¶¨‰ΩìÈáç': 'horse_weight',\n",
    "                'run_style_val': 'run_style', 'ÂçòÂãù': 'odds', '„Ç™„ÉÉ„Ç∫': 'odds', 'Ë∑ùÈõ¢': 'raw_distance'\n",
    "            }\n",
    "            df.rename(columns=column_map, inplace=True)\n",
    "            \n",
    "            # Parse Distance/Course\n",
    "            if 'raw_distance' in df.columns:\n",
    "                def parse_dist(x):\n",
    "                    if not isinstance(x, str): return None, None\n",
    "                    surf = None; dist = None\n",
    "                    if 'Ëäù' in x: surf = 'Ëäù'\n",
    "                    elif '„ÉÄ' in x: surf = '„ÉÄ'\n",
    "                    elif 'Èöú' in x: surf = 'Èöú'\n",
    "                    match = re.search(r'(\\d+)', x)\n",
    "                    if match: dist = int(match.group(1))\n",
    "                    return surf, dist\n",
    "                \n",
    "                parsed = df['raw_distance'].apply(parse_dist)\n",
    "                df['course_type'] = parsed.apply(lambda x: x[0])\n",
    "                df['distance'] = parsed.apply(lambda x: x[1])\n",
    "            else:\n",
    "                df['course_type'] = None; df['distance'] = None\n",
    "\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# --- Main Rich Scraper Function ---\n",
    "def scrape_race_rich(url, existing_race_ids=None, max_retries=3, force_race_id=None):\n",
    "    \"\"\"\n",
    "    Scrapes Race + History + Pedigree in one go.\n",
    "    \"\"\"\n",
    "    print(f\"  „É¨„Éº„ÇπËß£Êûê‰∏≠: {url}\")\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    \n",
    "    # 1. Fetch Race Page\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=15)\n",
    "        resp.encoding = 'EUC-JP'\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        \n",
    "        # --- Metadata (Date, Venue, Race Name, etc) ---\n",
    "        # (This part reuses logic from original scrape_jra_race)\n",
    "        h1_elem = soup.select_one(\"div.header_line h1 .txt\")\n",
    "        full_text = h1_elem.text.strip() if h1_elem else (soup.h1.text.strip() if soup.h1 else \"\")\n",
    "        \n",
    "        date_text = \"\"; venue_text = \"\"; kai = \"01\"; day = \"01\"; r_num = \"10\"\n",
    "        match_date = re.search(r'(\\d{4}Âπ¥\\d{1,2}Êúà\\d{1,2}Êó•)', full_text)\n",
    "        if match_date: date_text = match_date.group(1)\n",
    "        \n",
    "        # Parse logic is mainly for JRA. For NAR (or simple forced ID), we skip if force_race_id is present.\n",
    "        if force_race_id:\n",
    "             race_id = str(force_race_id)\n",
    "        else:\n",
    "            venues_str = \"Êú≠Âπå|ÂáΩÈ§®|Á¶èÂ≥∂|Êñ∞ÊΩü|Êù±‰∫¨|‰∏≠Â±±|‰∏≠‰∫¨|‰∫¨ÈÉΩ|Èò™Á•û|Â∞èÂÄâ\"\n",
    "            match_meta = re.search(rf'(\\d+)Âõû({venues_str})(\\\\d+)Êó•', full_text)\n",
    "            if match_meta:\n",
    "                kai = f\"{int(match_meta.group(1)):02}\"\n",
    "                venue_text = match_meta.group(2)\n",
    "                day = f\"{int(match_meta.group(3)):02}\"\n",
    "                \n",
    "            match_race = re.search(r'(\\d+)„É¨„Éº„Çπ', full_text)\n",
    "            if match_race: r_num = f\"{int(match_race.group(1)):02}\"\n",
    "            \n",
    "            place_map = {\n",
    "                \"Êú≠Âπå\": \"01\", \"ÂáΩÈ§®\": \"02\", \"Á¶èÂ≥∂\": \"03\", \"Êñ∞ÊΩü\": \"04\", \"Êù±‰∫¨\": \"05\",\n",
    "                \"‰∏≠Â±±\": \"06\", \"‰∏≠‰∫¨\": \"07\", \"‰∫¨ÈÉΩ\": \"08\", \"Èò™Á•û\": \"09\", \"Â∞èÂÄâ\": \"10\"\n",
    "            }\n",
    "            p_code = place_map.get(venue_text, \"00\")\n",
    "            year = date_text[:4] if date_text else \"2025\"\n",
    "            \n",
    "            race_id = f\"{year}{p_code}{kai}{day}{r_num}\"\n",
    "        \n",
    "        # SKIP Check\n",
    "        if existing_race_ids and race_id in existing_race_ids:\n",
    "            return None\n",
    "\n",
    "        # Basic Race Info\n",
    "        race_name = soup.select_one(\".race_name\").text.strip() if soup.select_one(\".race_name\") else \"\"\n",
    "        header_text = soup.select_one(\"div.header_line\").text if soup.select_one(\"div.header_line\") else full_text\n",
    "        \n",
    "        # Course/Dist\n",
    "        course_type = \"\"; distance = \"\"\n",
    "        dt_match = re.search(r'(Ëäù|„ÉÄ|„ÉÄ„Éº„Éà|ÈöúÂÆ≥)[^0-9]*(\\d+)', header_text)\n",
    "        if dt_match:\n",
    "            c = dt_match.group(1)\n",
    "            course_type = 'Ëäù' if 'Ëäù' in c else ('„ÉÄ„Éº„Éà' if '„ÉÄ' in c else 'ÈöúÂÆ≥')\n",
    "            distance = dt_match.group(2)\n",
    "            \n",
    "        # Weather/Condition\n",
    "        weather = \"\"; condition = \"\"; rotation = \"\"\n",
    "        w_match = re.search(r'Â§©ÂÄô\\s*[:Ôºö]\\s*(\\S+)', soup.text)\n",
    "        if w_match: weather = w_match.group(1)\n",
    "        \n",
    "        c_match = re.search(r'(?:Ëäù|„ÉÄ„Éº„Éà)\\s*[:Ôºö]\\s*(\\S+)', soup.text)\n",
    "        if c_match: condition = c_match.group(1)\n",
    "        \n",
    "        if \"Âè≥\" in header_text: rotation = \"Âè≥\"\n",
    "        elif \"Â∑¶\" in header_text: rotation = \"Â∑¶\"\n",
    "        elif \"Áõ¥\" in header_text: rotation = \"Áõ¥Á∑ö\"\n",
    "\n",
    "        # --- Parse Result Table ---\n",
    "        tables = soup.find_all('table')\n",
    "        target_table = None\n",
    "        for t in tables:\n",
    "            if \"ÁùÄÈ†Ü\" in t.text and \"È¶¨Âêç\" in t.text:\n",
    "                target_table = t\n",
    "                break\n",
    "        \n",
    "        if not target_table: return None\n",
    "        \n",
    "        rows = target_table.find_all('tr')\n",
    "        race_scraper = RaceScraper()\n",
    "        \n",
    "        data_list = []\n",
    "        \n",
    "        # Pre-convert date for filtering\n",
    "        d_obj = pd.to_datetime(date_text, format='%YÂπ¥%mÊúà%dÊó•') if date_text else datetime.now()\n",
    "        \n",
    "        print(f\"    Fetching details for {len(rows)-1} horses...\")\n",
    "        \n",
    "        for row in rows:\n",
    "            if row.find('th'): continue # Skip header\n",
    "            cells = row.find_all('td')\n",
    "            if not cells: continue\n",
    "            \n",
    "            # Basic info\n",
    "            rank = cells[0].text.strip()\n",
    "            waku = \"\"\n",
    "            if cells[1].find('img'): \n",
    "                alt = cells[1].find('img').get('alt', '')\n",
    "                m = re.search(r'Êû†(\\d+)', alt)\n",
    "                waku = m.group(1) if m else alt\n",
    "            umaban = cells[2].text.strip()\n",
    "            horse_name_elem = cells[3].find('a')\n",
    "            horse_name = cells[3].text.strip()\n",
    "            horse_id = \"\"\n",
    "            if horse_name_elem and 'href' in horse_name_elem.attrs:\n",
    "                hm = re.search(r'/horse/(\\d+)', horse_name_elem['href'])\n",
    "                if hm: horse_id = hm.group(1)\n",
    "            \n",
    "            jockey = cells[6].text.strip()\n",
    "            time_val = cells[7].text.strip()\n",
    "            # ... other basic fields\n",
    "            \n",
    "            # --- Rich Fetching (History & Pedigree) ---\n",
    "            blood_data = {\"father\": \"\", \"mother\": \"\", \"bms\": \"\"}\n",
    "            past_data = {}\n",
    "            \n",
    "            if horse_id:\n",
    "                # 1. Pedigree\n",
    "                blood_data = race_scraper.get_horse_profile(horse_id)\n",
    "                \n",
    "                # 2. History\n",
    "                df_past = race_scraper.get_past_races(horse_id, d_obj, n_samples=5)\n",
    "                \n",
    "                # Flatten History\n",
    "                for i in range(5):\n",
    "                    prefix = f\"past_{i+1}\"\n",
    "                    if i < len(df_past):\n",
    "                        r = df_past.iloc[i]\n",
    "                        past_data[f\"{prefix}_date\"] = r.get('date', '')\n",
    "                        past_data[f\"{prefix}_rank\"] = r.get('rank', '')\n",
    "                        past_data[f\"{prefix}_time\"] = r.get('time', '')\n",
    "                        past_data[f\"{prefix}_run_style\"] = r.get('run_style', '')\n",
    "                        past_data[f\"{prefix}_race_name\"] = r.get('race_name', '')\n",
    "                        past_data[f\"{prefix}_last_3f\"] = r.get('last_3f', '')\n",
    "                        past_data[f\"{prefix}_horse_weight\"] = r.get('horse_weight', '')\n",
    "                        past_data[f\"{prefix}_jockey\"] = r.get('jockey', '')\n",
    "                        past_data[f\"{prefix}_condition\"] = r.get('condition', '')\n",
    "                        past_data[f\"{prefix}_odds\"] = r.get('odds', '')\n",
    "                        past_data[f\"{prefix}_weather\"] = r.get('weather', '')\n",
    "                        past_data[f\"{prefix}_distance\"] = r.get('distance', '')\n",
    "                        past_data[f\"{prefix}_course_type\"] = r.get('course_type', '')\n",
    "                    else:\n",
    "                        # Fill Empty\n",
    "                        for col in ['date','rank','time','run_style','race_name','last_3f','horse_weight','jockey','condition','odds','weather','distance','course_type']:\n",
    "                            past_data[f\"{prefix}_{col}\"] = \"\"\n",
    "\n",
    "                # Be polite between horses\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            # Combine\n",
    "            row_dict = {\n",
    "                \"Êó•‰ªò\": date_text, \"‰ºöÂ†¥\": venue_text, \"„É¨„Éº„ÇπÁï™Âè∑\": f\"{r_num}R\", \"„É¨„Éº„ÇπÂêç\": race_name, \"ÈáçË≥û\": \"\",\n",
    "                \"„Ç≥„Éº„Çπ„Çø„Ç§„Éó\": course_type, \"Ë∑ùÈõ¢\": distance, \"Âõû„Çä\": rotation, \"Â§©ÂÄô\": weather, \"È¶¨Â†¥Áä∂ÊÖã\": condition,\n",
    "                \"ÁùÄÈ†Ü\": rank, \"Êû†\": waku, \"È¶¨Áï™\": umaban, \"È¶¨Âêç\": horse_name, \n",
    "                \"ÊÄßÈΩ¢\": cells[4].text.strip(), \"Êñ§Èáè\": cells[5].text.strip(), \"È®éÊâã\": jockey, \n",
    "                \"„Çø„Ç§„É†\": time_val, \"ÁùÄÂ∑Æ\": cells[8].text.strip(), \"‰∫∫Ê∞ó\": cells[13].text.strip(), \n",
    "                \"ÂçòÂãù„Ç™„ÉÉ„Ç∫\": cells[14].text.strip() if len(cells)>14 else \"0.0\", \n",
    "                \"Âæå3F\": cells[10].text.strip(), \"Âé©Ëàé\": cells[12].text.strip(), \n",
    "                \"È¶¨‰ΩìÈáç(Â¢óÊ∏õ)\": cells[11].text.strip(),\n",
    "                \"race_id\": race_id, \"horse_id\": horse_id,\n",
    "                **blood_data,\n",
    "                **past_data\n",
    "            }\n",
    "            data_list.append(row_dict)\n",
    "            \n",
    "        df = pd.DataFrame(data_list)\n",
    "        \n",
    "        # Enforce User-Specified Column Order\n",
    "        ordered_columns = [\n",
    "            \"Êó•‰ªò\", \"‰ºöÂ†¥\", \"„É¨„Éº„ÇπÁï™Âè∑\", \"„É¨„Éº„ÇπÂêç\", \"ÈáçË≥û\", \"„Ç≥„Éº„Çπ„Çø„Ç§„Éó\", \"Ë∑ùÈõ¢\", \"Âõû„Çä\", \"Â§©ÂÄô\", \"È¶¨Â†¥Áä∂ÊÖã\",\n",
    "            \"ÁùÄÈ†Ü\", \"Êû†\", \"È¶¨Áï™\", \"È¶¨Âêç\", \"ÊÄßÈΩ¢\", \"Êñ§Èáè\", \"È®éÊâã\", \"„Çø„Ç§„É†\", \"ÁùÄÂ∑Æ\", \"‰∫∫Ê∞ó\", \"ÂçòÂãù„Ç™„ÉÉ„Ç∫\",\n",
    "            \"Âæå3F\", \"Âé©Ëàé\", \"È¶¨‰ΩìÈáç(Â¢óÊ∏õ)\", \"race_id\", \"horse_id\"\n",
    "        ]\n",
    "        # rich data columns\n",
    "        for i in range(1, 6):\n",
    "            p = f\"past_{i}\"\n",
    "            ordered_columns.extend([\n",
    "                f\"{p}_date\", f\"{p}_rank\", f\"{p}_time\", f\"{p}_run_style\", f\"{p}_race_name\",\n",
    "                f\"{p}_last_3f\", f\"{p}_horse_weight\", f\"{p}_jockey\", f\"{p}_condition\",\n",
    "                f\"{p}_odds\", f\"{p}_weather\", f\"{p}_distance\", f\"{p}_course_type\"\n",
    "            ])\n",
    "        ordered_columns.extend([\"father\", \"mother\", \"bms\"])\n",
    "        \n",
    "        # Add missing cols with empty string, remove extras (if any/optional)\n",
    "        # reindex handles this safely\n",
    "        df_ordered = df.reindex(columns=ordered_columns, fill_value=\"\")\n",
    "        \n",
    "        return df_ordered\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping race {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Year/Month Iteration Logic (Scrape Year Rich) ---\n",
    "def scrape_jra_year_rich(year_str, start_date=None, end_date=None, save_callback=None, existing_race_ids=None):\n",
    "    # Parameter Map for Monthly Results\n",
    "    JRA_MONTH_PARAMS = {\n",
    "        \"2026\": { \"01\": \"E4\", \"02\": \"B2\", \"03\": \"80\", \"04\": \"4E\", \"05\": \"1C\", \"06\": \"EA\", \"07\": \"B8\", \"08\": \"86\", \"09\": \"54\", \"10\": \"22\", \"11\": \"F0\", \"12\": \"BE\" },\n",
    "        \"2025\": { \"01\": \"3F\", \"02\": \"0D\", \"03\": \"DB\", \"04\": \"A9\", \"05\": \"77\", \"06\": \"45\", \"07\": \"13\", \"08\": \"E1\", \"09\": \"AF\", \"10\": \"1E\", \"11\": \"EC\", \"12\": \"D3\" },\n",
    "        \"2024\": { \"01\": \"B3\", \"02\": \"81\", \"03\": \"4F\", \"04\": \"1D\", \"05\": \"EB\", \"06\": \"B9\", \"07\": \"87\", \"08\": \"55\", \"09\": \"23\", \"10\": \"92\", \"11\": \"60\", \"12\": \"2E\" },\n",
    "        \"2023\": { \"01\": \"27\", \"02\": \"F5\", \"03\": \"C3\", \"04\": \"91\", \"05\": \"5F\", \"06\": \"2D\", \"07\": \"FB\", \"08\": \"C9\", \"09\": \"97\", \"10\": \"06\", \"11\": \"D4\", \"12\": \"A2\" },\n",
    "        \"2022\": { \"01\": \"9B\", \"02\": \"69\", \"03\": \"37\", \"04\": \"05\", \"05\": \"D3\", \"06\": \"A1\", \"07\": \"6F\", \"08\": \"3D\", \"09\": \"0B\", \"10\": \"7A\", \"11\": \"48\", \"12\": \"16\" },\n",
    "        \"2021\": { \"01\": \"0F\", \"02\": \"DD\", \"03\": \"AB\", \"04\": \"79\", \"05\": \"47\", \"06\": \"15\", \"07\": \"E3\", \"08\": \"B1\", \"09\": \"7F\", \"10\": \"EE\", \"11\": \"BC\", \"12\": \"8A\" },\n",
    "        \"2020\": { \"01\": \"83\", \"02\": \"51\", \"03\": \"1F\", \"04\": \"ED\", \"05\": \"BB\", \"06\": \"89\", \"07\": \"57\", \"08\": \"25\", \"09\": \"F3\", \"10\": \"62\", \"11\": \"30\", \"12\": \"FE\" }\n",
    "    }\n",
    "    \n",
    "    if year_str not in JRA_MONTH_PARAMS:\n",
    "        print(f\"Âπ¥Â∫¶ {year_str} „ÅØ„Çµ„Éù„Éº„Éà„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ\")\n",
    "        return\n",
    "\n",
    "    params = JRA_MONTH_PARAMS[year_str]\n",
    "    base_url = \"https://www.jra.go.jp/JRADB/accessS.html\"\n",
    "\n",
    "    # Determine months to iterate\n",
    "    start_m = 1\n",
    "    end_m = 12\n",
    "\n",
    "    if start_date:\n",
    "        start_m = start_date.month\n",
    "    if end_date:\n",
    "        end_m = end_date.month\n",
    "\n",
    "    # Cap at Today\n",
    "    from datetime import date\n",
    "    today = date.today()\n",
    "\n",
    "    if end_date:\n",
    "        actual_end_date = min(end_date, today)\n",
    "    else:\n",
    "        actual_end_date = today\n",
    "\n",
    "    print(f\"=== JRA ‰∏ÄÊã¨„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÈñãÂßã {year_str} („É™„ÉÉ„ÉÅ„É¢„Éº„Éâ) ===\")\n",
    "    print(f\"ÊúüÈñì: {start_date or 'Start'} - {actual_end_date}\")\n",
    "    \n",
    "    if int(year_str) == today.year:\n",
    "        end_m = min(end_m, today.month)\n",
    "\n",
    "    total_processed = 0\n",
    "\n",
    "    for m in range(start_m, end_m + 1):\n",
    "        month = f\"{m:02}\"\n",
    "        if month not in params:\n",
    "            continue\n",
    "\n",
    "        suffix = params[month]\n",
    "        try:\n",
    "            ym = int(year_str + month)\n",
    "            prefix = \"pw01skl00\" if ym >= 202512 else \"pw01skl10\"\n",
    "        except:\n",
    "            prefix = \"pw01skl10\"\n",
    "\n",
    "        cname = f\"{prefix}{year_str}{month}/{suffix}\"\n",
    "\n",
    "        print(f\"\\nüìÖ {year_str}/{month} „ÇíÂèñÂæó‰∏≠...\")\n",
    "\n",
    "        try:\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "            }\n",
    "            response = requests.post(base_url, data={\"cname\": cname}, headers=headers, timeout=15)\n",
    "            response.encoding = 'cp932'\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå {cname} „ÅÆÂèñÂæó„Å´Â§±Êïó„Åó„Åæ„Åó„Åü (Status {response.status_code})\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            race_cnames = []\n",
    "            links = soup.find_all('a')\n",
    "            for link in links:\n",
    "                onclick = link.get('onclick', '')\n",
    "                match = re.search(r\"doAction\\('[^']+',\\s*'([^']+)'\\)\", onclick)\n",
    "                if match:\n",
    "                    c = match.group(1)\n",
    "                    if c.startswith('pw01srl'):\n",
    "                        race_cnames.append(c)\n",
    "\n",
    "            race_cnames = sorted(list(set(race_cnames)))\n",
    "            print(f\"  {len(race_cnames)} ÈñãÂÇ¨Êó•„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü\")\n",
    "\n",
    "            for day_cname in tqdm(race_cnames, desc=f\"  {year_str}/{month}\", leave=False):\n",
    "                resp_day = requests.post(base_url, data={\"cname\": day_cname}, headers=headers, timeout=15)\n",
    "                resp_day.encoding = 'cp932'\n",
    "                soup_day = BeautifulSoup(resp_day.text, 'html.parser')\n",
    "\n",
    "                race_list_items = []\n",
    "                all_anchors = soup_day.find_all('a')\n",
    "                for a in all_anchors:\n",
    "                    onclick = a.get('onclick', '')\n",
    "                    match_sde = re.search(r\"doAction\\s*\\(\\s*['\\\"][^'\\\"]+['\\\"]\\s*,\\s*['\\\"](pw01sde[^'\\\"]+)['\\\"]\\s*\\)\", onclick)\n",
    "                    href = a.get('href', '')\n",
    "\n",
    "                    final_url = \"\"\n",
    "                    if match_sde:\n",
    "                        final_url = f\"{base_url}?CNAME={match_sde.group(1)}\"\n",
    "                    elif 'pw01sde' in href:\n",
    "                         if 'CNAME=' in href:\n",
    "                             final_url = urllib.parse.urljoin(base_url, href)\n",
    "                         else:\n",
    "                             final_url = urllib.parse.urljoin(base_url, href)\n",
    "\n",
    "                    if final_url:\n",
    "                        race_list_items.append(final_url)\n",
    "\n",
    "                unique_races = sorted(list(set(race_list_items)))\n",
    "\n",
    "                daily_data = []\n",
    "\n",
    "                for r_link in unique_races:\n",
    "                    # Fetch with Rich Scraper\n",
    "                    df = scrape_race_rich(r_link, existing_race_ids=existing_race_ids)\n",
    "\n",
    "                    if df is not None and not df.empty:\n",
    "                        daily_data.append(df)\n",
    "                        total_processed += 1\n",
    "                    \n",
    "                    # Rate limiting\n",
    "                    time.sleep(1.0) \n",
    "                \n",
    "                # Save daily batch\n",
    "                if daily_data and save_callback:\n",
    "                    df_day = pd.concat(daily_data, ignore_index=True)\n",
    "                    save_callback(df_day) \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {month}Êúà„ÅÆÂá¶ÁêÜ‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {e}\")\n",
    "            \n",
    "    print(\"ÂÆå‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differential Scraping Execution\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import gc\n",
    "\n",
    "def run_differential_scraping():\n",
    "    # 1. Load Paths\n",
    "    csv_path = os.path.join(SAVE_DIR, TARGET_CSV)\n",
    "    id_path = os.path.join(SAVE_DIR, MASTER_ID_CSV)\n",
    "    \n",
    "    # 2. Check Master List\n",
    "    if not os.path.exists(id_path):\n",
    "        print(f\"‚ùå „Éû„Çπ„Çø„Éº„É™„Çπ„Éà„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {id_path}\\nÂÖà„Å´ Colab_ID_Fetcher.ipynb „ÇíÂÆüË°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        master_df = pd.read_csv(id_path, dtype=str)\n",
    "        if 'race_id' not in master_df.columns:\n",
    "            print(\"‚ùå „Éû„Çπ„Çø„Éº„É™„Çπ„Éà„Å´ race_id „Ç´„É©„É†„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ\")\n",
    "            return\n",
    "        master_ids = set(master_df['race_id'].dropna().unique())\n",
    "        print(f\"üìã „Éû„Çπ„Çø„Éº„É™„Çπ„ÉàÂÖ®‰ª∂: {len(master_ids)} „É¨„Éº„Çπ\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå „Éû„Çπ„Çø„Éº„É™„Çπ„ÉàË™≠„ÅøËæº„Åø„Ç®„É©„Éº: {e}\")\n",
    "        return\n",
    "\n",
    "    # 3. Check Existing DB\n",
    "    existing_ids = set()\n",
    "    if os.path.exists(csv_path):\n",
    "        try:\n",
    "            existing_df = pd.read_csv(csv_path, dtype=str, usecols=['race_id'])\n",
    "            if 'race_id' in existing_df.columns:\n",
    "                existing_ids = set(existing_df['race_id'].dropna().unique())\n",
    "            print(f\"üíæ Êó¢Â≠òÂèñÂæóÊ∏à„Åø: {len(existing_ids)} „É¨„Éº„Çπ\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Êó¢Â≠òCSVË™≠„ÅøËæº„Åø„Ç®„É©„Éº (ÂàùÂõû„Å®„Åø„Å™„Åó„Åæ„Åô): {e}\")\n",
    "    \n",
    "    # 4. Calculate Diff\n",
    "    # target = master - existing\n",
    "    target_ids = sorted(list(master_ids - existing_ids))\n",
    "    print(f\"üöÄ ‰ªäÂõû„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÂØæË±°: {len(target_ids)} „É¨„Éº„Çπ\")\n",
    "    \n",
    "    if not target_ids:\n",
    "        print(\"‚úÖ ÂÖ®„Å¶ÂèñÂæóÊ∏à„Åø„Åß„Åô„ÄÇÁµÇ‰∫Ü„Åó„Åæ„Åô„ÄÇ\")\n",
    "        return\n",
    "\n",
    "    # 5. Scrape Loop\n",
    "    # scraping_logic_v2.py defines RaceScraper class and scrape_race_rich function\n",
    "    \n",
    "    chunk_size = 50\n",
    "    total = len(target_ids)\n",
    "    \n",
    "    # Helper to append\n",
    "    def safe_append_csv(df_chunk, path):\n",
    "        if not os.path.exists(path):\n",
    "            df_chunk.to_csv(path, index=False)\n",
    "        else:\n",
    "            try:\n",
    "                # Align columns\n",
    "                headers = pd.read_csv(path, nrows=0).columns.tolist()\n",
    "                df_aligned = df_chunk.reindex(columns=headers)\n",
    "                df_aligned.to_csv(path, mode='a', header=False, index=False)\n",
    "            except:\n",
    "                # Fallback if alignment fails\n",
    "                df_chunk.to_csv(path, mode='a', header=False, index=False)\n",
    "\n",
    "    print(\"\\nÈñãÂßã„Åó„Åæ„Åô...\")\n",
    "    \n",
    "    buffer = []\n",
    "    \n",
    "    for i, rid in enumerate(tqdm(target_ids)):\n",
    "        # Construct URL (JRA)\n",
    "        # https://race.netkeiba.com/race/result.html?race_id=202506010101\n",
    "        url = f\"https://race.netkeiba.com/race/result.html?race_id={rid}\"\n",
    "        \n",
    "        try:\n",
    "            time.sleep(1) # Gentle scraping\n",
    "            # scrape_race_rich auto-fetches history too\n",
    "            df = scrape_race_rich(url, force_race_id=rid)\n",
    "            \n",
    "            if df is not None and not df.empty:\n",
    "                buffer.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error scraping {rid}: {e}\")\n",
    "            \n",
    "        # Save Chunk\n",
    "        if len(buffer) >= chunk_size or (i == total - 1 and buffer):\n",
    "            try:\n",
    "                df_chunk = pd.concat(buffer, ignore_index=True)\n",
    "                safe_append_csv(df_chunk, csv_path)\n",
    "                print(f\"  Saved {len(buffer)} races.\")\n",
    "                buffer = []\n",
    "                gc.collect()\n",
    "            except Exception as e:\n",
    "                print(f\"  Save Error: {e}\")\n",
    "    \n",
    "    print(\"‚úÖ „Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÂÆå‰∫Ü\")\n",
    "    \n",
    "    # Final Deduplication\n",
    "    if os.path.exists(csv_path):\n",
    "        print('ÊúÄÁµÇÊï¥ÁêÜ‰∏≠...')\n",
    "        try:\n",
    "            df_final = pd.read_csv(csv_path, dtype=str, low_memory=False)\n",
    "            if 'race_id' in df_final.columns and 'horse_id' in df_final.columns:\n",
    "                before = len(df_final)\n",
    "                df_final.drop_duplicates(subset=['race_id', 'horse_id'], keep='last', inplace=True)\n",
    "                after = len(df_final)\n",
    "                if before != after:\n",
    "                    df_final.to_csv(csv_path, index=False)\n",
    "                    print(f'  ÈáçË§áÂâäÈô§: {before} -> {after} rows')\n",
    "        except Exception: pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "run_differential_scraping()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}