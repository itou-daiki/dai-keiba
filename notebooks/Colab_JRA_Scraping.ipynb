{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ‡ JRA å…¨ãƒ¬ãƒ¼ã‚¹å–å¾— (2020-2026)\n",
    "ä»¥ä¸‹ã®è¨­å®šå¤‰æ•°ã‚’å¤‰æ›´ã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚æŒ‡å®šã—ãŸæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€`SAVE_DIR` ã«ä¿å­˜ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆã™ã‚‹å ´åˆã®ã¿å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "from datetime import datetime\n",
    "import urllib.parse\n",
    "import time\n",
    "\n",
    "def scrape_jra_race(url, existing_race_ids=None):\n",
    "    \"\"\"\n",
    "    Scrapes a single race page from JRA website.\n",
    "    Returns a pandas DataFrame matching the schema of database.csv.\n",
    "    If existing_race_ids is provided and the race ID is found, returns None (skip).\n",
    "    \"\"\"\n",
    "    print(f\"Accessing URL: {url}...\")\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" \n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        # Netkeiba usually uses EUC-JP, JRA uses Shift_JIS. \n",
    "        # Since this function is mostly used for Netkeiba (NAR/JRA-Backfill), default to EUC-JP.\n",
    "        # response.encoding = 'cp932' \n",
    "        response.encoding = 'EUC-JP'\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: Status code {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # --- Metadata Extraction ---\n",
    "        h1_elem = soup.select_one(\"div.header_line h1 .txt\")\n",
    "        full_text = h1_elem.text.strip() if h1_elem else \"\"\n",
    "        if not full_text and soup.h1:\n",
    "            full_text = soup.h1.text.strip()\n",
    "\n",
    "        date_text = \"\"\n",
    "        venue_text = \"\"\n",
    "        race_num_text = \"\"\n",
    "        kai = \"01\"\n",
    "        day = \"01\"\n",
    "        \n",
    "        # Extract Date\n",
    "        match_date = re.search(r'(\\d{4}å¹´\\d{1,2}æœˆ\\d{1,2}æ—¥)', full_text)\n",
    "        if match_date:\n",
    "            date_text = match_date.group(1)\n",
    "            \n",
    "        # Extract Venue, Kai, Day\n",
    "        venues_str = \"æœ­å¹Œ|å‡½é¤¨|ç¦å³¶|æ–°æ½Ÿ|æ±äº¬|ä¸­å±±|ä¸­äº¬|äº¬éƒ½|é˜ªç¥|å°å€‰\"\n",
    "        match_meta = re.search(rf'(\\d+)å›({venues_str})(\\d+)æ—¥', full_text)\n",
    "        if match_meta:\n",
    "            kai = f\"{int(match_meta.group(1)):02}\"\n",
    "            venue_text = match_meta.group(2)\n",
    "            day = f\"{int(match_meta.group(3)):02}\"\n",
    "            \n",
    "        # Extract Race Num\n",
    "        match_race = re.search(r'(\\d+)ãƒ¬ãƒ¼ã‚¹', full_text)\n",
    "        if match_race:\n",
    "            r_val = int(match_race.group(1))\n",
    "            race_num_text = f\"{r_val}R\"\n",
    "            r_num = f\"{r_val:02}\"\n",
    "        else:\n",
    "            race_num_text = \"10R\" # Fallback\n",
    "            r_num = \"10\"\n",
    "            \n",
    "        # Race Name\n",
    "        race_name_text = \"\"\n",
    "        name_elem = soup.select_one(\".race_name\")\n",
    "        if name_elem:\n",
    "            race_name_text = name_elem.text.strip()\n",
    "\n",
    "        # Grade\n",
    "        grade_text = \"\"\n",
    "        if \"G1\" in str(soup) or \"ï¼§â… \" in str(soup): grade_text = \"G1\"\n",
    "        elif \"G2\" in str(soup) or \"ï¼§â…¡\" in str(soup): grade_text = \"G2\"\n",
    "        elif \"G3\" in str(soup) or \"ï¼§â…¢\" in str(soup): grade_text = \"G3\"\n",
    "\n",
    "        # --- Added: Course, Distance, Weather, Condition ---\n",
    "        # JRA HTML structure varies, but often contained in specific divs or text lines.\n",
    "        # We will scan the entire header text or specific class for these patterns.\n",
    "        \n",
    "        # 1. Course & Distance (e.g., \"èŠ2000ãƒ¡ãƒ¼ãƒˆãƒ«\", \"ãƒ€ãƒ¼ãƒˆ1800ãƒ¡ãƒ¼ãƒˆãƒ«\", \"èŠãƒ»1600m\")\n",
    "        # Usually in the same block as race name or just below.\n",
    "        # We search the whole header area text.\n",
    "        header_text = soup.select_one(\"div.header_line\").text if soup.select_one(\"div.header_line\") else soup.text\n",
    "        \n",
    "        # Regex: Allow spaces, dots, etc. between Type and Dist\n",
    "        dist_type_match = re.search(r'(èŠ|ãƒ€|ãƒ€ãƒ¼ãƒˆ|éšœå®³)[^0-9]*(\\d+)', header_text)\n",
    "        course_type = \"\"\n",
    "        distance = \"\"\n",
    "        \n",
    "        if dist_type_match:\n",
    "            c_val = dist_type_match.group(1)\n",
    "            d_val = dist_type_match.group(2)\n",
    "            \n",
    "            if \"èŠ\" in c_val: course_type = \"èŠ\"\n",
    "            elif \"ãƒ€\" in c_val: course_type = \"ãƒ€ãƒ¼ãƒˆ\"\n",
    "            elif \"éšœ\" in c_val: course_type = \"éšœå®³\"\n",
    "            \n",
    "            distance = int(d_val)\n",
    "        \n",
    "        # 1.5 Rotation (Right/Left/Straight)\n",
    "        rotation = \"\"\n",
    "        # Often formatted as \"(å³)\" or \"ï¼ˆå·¦ï¼‰\" \n",
    "        rot_match = re.search(r'[ï¼ˆ\\(](å³|å·¦|ç›´ç·š)[ï¼‰\\)]', header_text)\n",
    "        if rot_match:\n",
    "            rotation = rot_match.group(1)\n",
    "        else:\n",
    "             # Fallback: Inference based on Venue\n",
    "             # Tokyo, Chukyo, Niigata -> Left (Default), others Right\n",
    "             # Niigata 1000m -> Straight\n",
    "             if \"å·¦\" in header_text: rotation = \"å·¦\"\n",
    "             elif \"å³\" in header_text: rotation = \"å³\"\n",
    "             elif \"ç›´ç·š\" in header_text: rotation = \"ç›´ç·š\"\n",
    "\n",
    "        \n",
    "        # 2. Weather (e.g., \"å¤©å€™ï¼šæ™´\")\n",
    "        weather = \"\"\n",
    "        w_match = re.search(r'å¤©å€™\\s*[:ï¼š]\\s*(\\S+)', soup.text)\n",
    "        if w_match:\n",
    "            weather = w_match.group(1).strip()\n",
    "            \n",
    "        # 3. Condition (e.g., \"èŠï¼šè‰¯\", \"ãƒ€ãƒ¼ãƒˆï¼šç¨é‡\")\n",
    "        # Note: A race can have both if it's mixed, but usually we care about the main one or the one matching course_type.\n",
    "        condition = \"\"\n",
    "        \n",
    "        # Try specific pattern based on course type\n",
    "        if course_type == \"èŠ\":\n",
    "             c_match = re.search(r'èŠ\\s*[:ï¼š]\\s*(\\S+)', soup.text)\n",
    "             if c_match: condition = c_match.group(1).strip()\n",
    "        elif course_type == \"ãƒ€ãƒ¼ãƒˆ\":\n",
    "             c_match = re.search(r'ãƒ€ãƒ¼ãƒˆ\\s*[:ï¼š]\\s*(\\S+)', soup.text)\n",
    "             if c_match: condition = c_match.group(1).strip()\n",
    "        \n",
    "        # Fallback if generic or course type unknown, grab first one found\n",
    "        if not condition:\n",
    "             c_match_gen = re.search(r'(?:èŠ|ãƒ€ãƒ¼ãƒˆ)\\s*[:ï¼š]\\s*(\\S+)', soup.text)\n",
    "             if c_match_gen: condition = c_match_gen.group(1).strip()\n",
    "\n",
    "        # --- Table Extraction (Custom BS4 Parsing) ---\n",
    "        # Find table with \"ç€é †\"\n",
    "        tables = soup.find_all('table')\n",
    "        target_table = None\n",
    "        for tbl in tables:\n",
    "            if \"ç€é †\" in tbl.text and \"é¦¬å\" in tbl.text:\n",
    "                target_table = tbl\n",
    "                break\n",
    "        \n",
    "        if not target_table:\n",
    "            print(f\"Warning: Result table not found in {url} (Encoding: {response.encoding})\")\n",
    "            return None\n",
    "\n",
    "        # Parse Rows\n",
    "        rows = target_table.find_all('tr')\n",
    "        data = []\n",
    "        \n",
    "        for row in rows:\n",
    "            # Skip header (usually th) or invalid rows\n",
    "            if row.find('th'):\n",
    "                continue\n",
    "                \n",
    "            cells = row.find_all('td')\n",
    "            if not cells:\n",
    "                continue\n",
    "\n",
    "            # Need to robustly map cells. \n",
    "            # We can use class names if available, or index.\n",
    "            # Based on debug:\n",
    "            # 0: place (ç€é †)\n",
    "            # 1: waku (æ ) -> img alt\n",
    "            # 2: num (é¦¬ç•ª)\n",
    "            # 3: horse (é¦¬å)\n",
    "            # 4: age (æ€§é½¢)\n",
    "            # 5: weight (æ–¤é‡)\n",
    "            # 6: jockey (é¨æ‰‹)\n",
    "            # 7: time (ã‚¿ã‚¤ãƒ )\n",
    "            # 8: margin (ç€å·®)\n",
    "            # 9: corner (é€šé)\n",
    "            # 10: f_time (ä¸Šã‚Š)\n",
    "            # 11: h_weight (é¦¬ä½“é‡)\n",
    "            # 12: trainer (èª¿æ•™å¸«)\n",
    "            # 13: pop (äººæ°—)\n",
    "            # * Odds is missing *\n",
    "\n",
    "            def get_text(idx):\n",
    "                if idx < len(cells):\n",
    "                    return cells[idx].get_text(strip=True)\n",
    "                return \"\"\n",
    "\n",
    "            # Extract Frame (Waku) from Image\n",
    "            waku_text = \"\"\n",
    "            if len(cells) > 1:\n",
    "                img = cells[1].find('img')\n",
    "                if img and 'alt' in img.attrs:\n",
    "                    # Example: \"æ 6ç·‘\" -> Extract number\n",
    "                    alt = img['alt']\n",
    "                    m = re.search(r'æ (\\d+)', alt)\n",
    "                    if m:\n",
    "                        waku_text = m.group(1)\n",
    "                    else:\n",
    "                        waku_text = alt # Fallback\n",
    "            \n",
    "            # Extract Horse ID\n",
    "            horse_id = \"\"\n",
    "            if len(cells) > 3:\n",
    "                a_tag = cells[3].find('a')\n",
    "                if a_tag and 'href' in a_tag.attrs:\n",
    "                    href = a_tag['href']\n",
    "                    # /horse/2018105247/\n",
    "                    m = re.search(r'/horse/(\\d+)', href)\n",
    "                    if m:\n",
    "                        horse_id = m.group(1)\n",
    "\n",
    "            row_data = {\n",
    "                'ç€ é †': get_text(0),\n",
    "                'æ ': waku_text,\n",
    "                'é¦¬ ç•ª': get_text(2),\n",
    "                'é¦¬å': get_text(3),\n",
    "                'horse_id': horse_id, # Added\n",
    "                'æ€§é½¢': get_text(4),\n",
    "                'æ–¤é‡': get_text(5),\n",
    "                'é¨æ‰‹': get_text(6),\n",
    "                'ã‚¿ã‚¤ãƒ ': get_text(7),\n",
    "                'ç€å·®': get_text(8),\n",
    "                'ã‚³ãƒ¼ãƒŠãƒ¼ é€šéé †': get_text(9),\n",
    "                'å¾Œ3F': get_text(10),\n",
    "                'é¦¬ä½“é‡ (å¢—æ¸›)': get_text(11),\n",
    "                'å©èˆ': get_text(12),\n",
    "                'äºº æ°—': get_text(13),\n",
    "                'å˜å‹ ã‚ªãƒƒã‚º': \"0.0\" # Missing in source\n",
    "            }\n",
    "            data.append(row_data)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Add Metadata Columns\n",
    "        df['æ—¥ä»˜'] = date_text\n",
    "        df['ä¼šå ´'] = venue_text\n",
    "        df['ãƒ¬ãƒ¼ã‚¹ç•ªå·'] = race_num_text\n",
    "        df['ãƒ¬ãƒ¼ã‚¹å'] = race_name_text\n",
    "        df['é‡è³'] = grade_text\n",
    "        df['è·é›¢'] = distance\n",
    "        df['ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—'] = course_type\n",
    "        df['å¤©å€™'] = weather\n",
    "        df['å¤©å€™'] = weather\n",
    "        df['é¦¬å ´çŠ¶æ…‹'] = condition\n",
    "        df['å›ã‚Š'] = rotation\n",
    "        \n",
    "        # ID Generation\n",
    "        place_map = {\n",
    "            \"æœ­å¹Œ\": \"01\", \"å‡½é¤¨\": \"02\", \"ç¦å³¶\": \"03\", \"æ–°æ½Ÿ\": \"04\", \"æ±äº¬\": \"05\",\n",
    "            \"ä¸­å±±\": \"06\", \"ä¸­äº¬\": \"07\", \"äº¬éƒ½\": \"08\", \"é˜ªç¥\": \"09\", \"å°å€‰\": \"10\"\n",
    "        }\n",
    "        p_code = place_map.get(venue_text, \"00\")\n",
    "        \n",
    "        year = \"2025\"\n",
    "        if date_text:\n",
    "            year = date_text[:4]\n",
    "            \n",
    "        generated_id = f\"{year}{p_code}{kai}{day}{r_num}\"\n",
    "        \n",
    "        # SKIP CHECK\n",
    "        if existing_race_ids and generated_id in existing_race_ids:\n",
    "            print(f\"Skipping {generated_id} (Already exists)\")\n",
    "            return None\n",
    "\n",
    "        df['race_id'] = generated_id\n",
    "\n",
    "        # Cleanups\n",
    "        if 'å˜å‹ ã‚ªãƒƒã‚º' in df.columns:\n",
    "            df['å˜å‹ ã‚ªãƒƒã‚º'] = pd.to_numeric(df['å˜å‹ ã‚ªãƒƒã‚º'], errors='coerce').fillna(0.0)\n",
    "\n",
    "        standard_columns = [\n",
    "            \"æ—¥ä»˜\",\"ä¼šå ´\",\"ãƒ¬ãƒ¼ã‚¹ç•ªå·\",\"ãƒ¬ãƒ¼ã‚¹å\",\"é‡è³\",\"ç€ é †\",\"æ \",\"é¦¬ ç•ª\",\"é¦¬å\",\"æ€§é½¢\",\"æ–¤é‡\",\"é¨æ‰‹\",\n",
    "            \"ã‚¿ã‚¤ãƒ \",\"ç€å·®\",\"äºº æ°—\",\"å˜å‹ ã‚ªãƒƒã‚º\",\"å¾Œ3F\",\"ã‚³ãƒ¼ãƒŠãƒ¼ é€šéé †\",\"å©èˆ\",\"é¦¬ä½“é‡ (å¢—æ¸›)\",\"race_id\",\n",
    "            \"è·é›¢\",\"ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—\",\"å¤©å€™\",\"é¦¬å ´çŠ¶æ…‹\",\"å›ã‚Š\"\n",
    "        ]\n",
    "        \n",
    "        for col in standard_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = \"\"\n",
    "                \n",
    "        df = df[standard_columns]\n",
    "        \n",
    "        print(f\"Scraped {len(df)} rows.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping JRA URL: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Parameter Map for Monthly Results (Reverse Engineered)\n",
    "JRA_MONTH_PARAMS = {\n",
    "    \"2026\": { \"01\": \"E4\", \"02\": \"B2\", \"03\": \"80\", \"04\": \"4E\", \"05\": \"1C\", \"06\": \"EA\", \"07\": \"B8\", \"08\": \"86\", \"09\": \"54\", \"10\": \"22\", \"11\": \"F0\", \"12\": \"BE\" },\n",
    "    \"2025\": { \"01\": \"3F\", \"02\": \"0D\", \"03\": \"DB\", \"04\": \"A9\", \"05\": \"77\", \"06\": \"45\", \"07\": \"13\", \"08\": \"E1\", \"09\": \"AF\", \"10\": \"1E\", \"11\": \"EC\", \"12\": \"D3\" },\n",
    "    \"2024\": { \"01\": \"B3\", \"02\": \"81\", \"03\": \"4F\", \"04\": \"1D\", \"05\": \"EB\", \"06\": \"B9\", \"07\": \"87\", \"08\": \"55\", \"09\": \"23\", \"10\": \"92\", \"11\": \"60\", \"12\": \"2E\" },\n",
    "    \"2023\": { \"01\": \"27\", \"02\": \"F5\", \"03\": \"C3\", \"04\": \"91\", \"05\": \"5F\", \"06\": \"2D\", \"07\": \"FB\", \"08\": \"C9\", \"09\": \"97\", \"10\": \"06\", \"11\": \"D4\", \"12\": \"A2\" },\n",
    "    \"2022\": { \"01\": \"9B\", \"02\": \"69\", \"03\": \"37\", \"04\": \"05\", \"05\": \"D3\", \"06\": \"A1\", \"07\": \"6F\", \"08\": \"3D\", \"09\": \"0B\", \"10\": \"7A\", \"11\": \"48\", \"12\": \"16\" },\n",
    "    \"2021\": { \"01\": \"0F\", \"02\": \"DD\", \"03\": \"AB\", \"04\": \"79\", \"05\": \"47\", \"06\": \"15\", \"07\": \"E3\", \"08\": \"B1\", \"09\": \"7F\", \"10\": \"EE\", \"11\": \"BC\", \"12\": \"8A\" },\n",
    "    \"2020\": { \"01\": \"83\", \"02\": \"51\", \"03\": \"1F\", \"04\": \"ED\", \"05\": \"BB\", \"06\": \"89\", \"07\": \"57\", \"08\": \"25\", \"09\": \"F3\", \"10\": \"62\", \"11\": \"30\", \"12\": \"FE\" }\n",
    "}\n",
    "\n",
    "def scrape_jra_year(year_str, start_date=None, end_date=None, save_callback=None, existing_race_ids=None):\n",
    "    \"\"\"\n",
    "    Scrapes races for a given year and date range.\n",
    "    year_str: \"2024\" or \"2025\"\n",
    "    start_date: datetime.date (optional)\n",
    "    end_date: datetime.date (optional)\n",
    "    save_callback: function(df) to save progress\n",
    "    \"\"\"\n",
    "    \n",
    "    if year_str not in JRA_MONTH_PARAMS:\n",
    "        print(f\"Year {year_str} not supported in parameter map.\")\n",
    "        return\n",
    "\n",
    "    params = JRA_MONTH_PARAMS[year_str]\n",
    "    base_url = \"https://www.jra.go.jp/JRADB/accessS.html\"\n",
    "    \n",
    "    # Determine months to iterate\n",
    "    start_m = 1\n",
    "    end_m = 12\n",
    "    \n",
    "    if start_date:\n",
    "        start_m = start_date.month\n",
    "    if end_date:\n",
    "        end_m = end_date.month\n",
    "    \n",
    "    # Cap at Today to prevent future scraping\n",
    "    from datetime import date\n",
    "    today = date.today()\n",
    "    \n",
    "    # If explicit end_date is used, respect it, but also respect today if it is earlier?\n",
    "    # Usually for results, we never want future.\n",
    "    if end_date:\n",
    "        actual_end_date = min(end_date, today)\n",
    "    else:\n",
    "        actual_end_date = today\n",
    "        \n",
    "    print(f\"=== Starting JRA Bulk Scraping for {year_str} (Period: {start_date or 'Start'} - {actual_end_date}) ===\")\n",
    "\n",
    "    # Adjust end_m based on today if we are in target year\n",
    "    if int(year_str) == today.year:\n",
    "        end_m = min(end_m, today.month)\n",
    "    elif int(year_str) > today.year:\n",
    "        print(f\"Year {year_str} is in the future. Stopping.\")\n",
    "        return\n",
    "    \n",
    "    for m in range(start_m, end_m + 1):\n",
    "        month = f\"{m:02}\"\n",
    "        if month not in params:\n",
    "            continue\n",
    "            \n",
    "        suffix = params[month]\n",
    "        # Logic for skl00 vs skl10\n",
    "        try:\n",
    "            ym = int(year_str + month)\n",
    "            prefix = \"pw01skl00\" if ym >= 202512 else \"pw01skl10\"\n",
    "        except:\n",
    "            prefix = \"pw01skl10\"\n",
    "\n",
    "        cname = f\"{prefix}{year_str}{month}/{suffix}\"\n",
    "        \n",
    "        print(f\"Fetching list for {year_str}/{month} (CNAME={cname})...\")\n",
    "        \n",
    "        try:\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" \n",
    "            }\n",
    "            response = requests.post(base_url, data={\"cname\": cname}, headers=headers, timeout=10)\n",
    "            response.encoding = 'cp932'\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch {cname} (Status {response.status_code})\")\n",
    "                continue\n",
    "                \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            race_cnames = []\n",
    "            links = soup.find_all('a')\n",
    "            for link in links:\n",
    "                onclick = link.get('onclick', '')\n",
    "                match = re.search(r\"doAction\\('[^']+',\\s*'([^']+)'\\)\", onclick)\n",
    "                if match:\n",
    "                    c = match.group(1)\n",
    "                    if c.startswith('pw01srl'):\n",
    "                        race_cnames.append(c)\n",
    "            \n",
    "            race_cnames = sorted(list(set(race_cnames)))\n",
    "            print(f\"  Found {len(race_cnames)} race days in month.\")\n",
    "            \n",
    "            for day_cname in race_cnames:\n",
    "                resp_day = requests.post(base_url, data={\"cname\": day_cname}, headers=headers, timeout=10)\n",
    "                resp_day.encoding = 'cp932'\n",
    "                soup_day = BeautifulSoup(resp_day.text, 'html.parser')\n",
    "                \n",
    "                # Check date of this day page\n",
    "                day_date_text = \"\"\n",
    "                d_h1 = soup_day.select_one(\"div.header_line h1 .txt\")\n",
    "                full_d_text = d_h1.text.strip() if d_h1 else (soup_day.h1.text.strip() if soup_day.h1 else \"\")\n",
    "                \n",
    "                # Parse date from \"2025å¹´1æœˆ5æ—¥ï¼ˆæ—¥æ›œï¼‰1å›ä¸­å±±1æ—¥\"\n",
    "                # Need to match Date AND Venue/Kai/Day info for ID generation\n",
    "                # Pattern: YYYYå¹´MæœˆDæ—¥ ... Kå›VenueDæ—¥\n",
    "                \n",
    "                current_day_date = None\n",
    "                kai_str = \"01\"\n",
    "                day_str = \"01\"\n",
    "                venue_str = \"\"\n",
    "                p_code = \"00\"\n",
    "                \n",
    "                match_day_date = re.search(r'(\\d{4})å¹´(\\d{1,2})æœˆ(\\d{1,2})æ—¥', full_d_text)\n",
    "                if match_day_date:\n",
    "                    y, mo, d_day = map(int, match_day_date.groups())\n",
    "                    current_day_date = datetime(y, mo, d_day).date()\n",
    "                    \n",
    "                    # Filtering\n",
    "                    if start_date and current_day_date < start_date:\n",
    "                        print(f\"    Skipping day {current_day_date} (Before start date)\")\n",
    "                        continue\n",
    "                    if end_date and current_day_date > end_date:\n",
    "                        print(f\"    Skipping day {current_day_date} (After end date)\")\n",
    "                        continue\n",
    "                    print(f\"    Processing Day: {current_day_date} ({full_d_text})\")\n",
    "                else:\n",
    "                    print(f\"    Processing Day (Date unknown): {full_d_text[:20]}...\")\n",
    "\n",
    "                # Parse Venue Info for ID Generation\n",
    "                venues_ptn = \"æœ­å¹Œ|å‡½é¤¨|ç¦å³¶|æ–°æ½Ÿ|æ±äº¬|ä¸­å±±|ä¸­äº¬|äº¬éƒ½|é˜ªç¥|å°å€‰\"\n",
    "                match_meta = re.search(rf'(\\d+)å›({venues_ptn})(\\d+)æ—¥', full_d_text)\n",
    "                if match_meta:\n",
    "                    kai_str = f\"{int(match_meta.group(1)):02}\"\n",
    "                    venue_str = match_meta.group(2)\n",
    "                    day_str = f\"{int(match_meta.group(3)):02}\"\n",
    "                    \n",
    "                    place_map = {\n",
    "                        \"æœ­å¹Œ\": \"01\", \"å‡½é¤¨\": \"02\", \"ç¦å³¶\": \"03\", \"æ–°æ½Ÿ\": \"04\", \"æ±äº¬\": \"05\",\n",
    "                        \"ä¸­å±±\": \"06\", \"ä¸­äº¬\": \"07\", \"äº¬éƒ½\": \"08\", \"é˜ªç¥\": \"09\", \"å°å€‰\": \"10\"\n",
    "                    }\n",
    "                    p_code = place_map.get(venue_str, \"00\")\n",
    "                \n",
    "                # Collect Race Links AND Race Numbers\n",
    "                # Need to pair Link with Race Number\n",
    "                race_list_items = []\n",
    "\n",
    "                all_anchors = soup_day.find_all('a')\n",
    "                for a in all_anchors:\n",
    "                    onclick = a.get('onclick', '')\n",
    "                    # Check for doAction with robust regex (handles single/double quotes, whitespace)\n",
    "                    # Pattern: doAction('FormName', 'CNAME')\n",
    "                    match_sde = re.search(r\"doAction\\s*\\(\\s*['\\\"][^'\\\"]+['\\\"]\\s*,\\s*['\\\"](pw01sde[^'\\\"]+)['\\\"]\\s*\\)\", onclick)\n",
    "                    href = a.get('href', '')\n",
    "                    \n",
    "                    final_url = \"\"\n",
    "                    if match_sde:\n",
    "                        final_url = f\"{base_url}?CNAME={match_sde.group(1)}\"\n",
    "                    elif 'pw01sde' in href:\n",
    "                        # Fallback for simple hrefs\n",
    "                        if 'CNAME=' in href:\n",
    "                             final_url = urllib.parse.urljoin(base_url, href)\n",
    "                        else:\n",
    "                             # If href=\"accessS.html?CNAME=...\"\n",
    "                             # or just \"?CNAME=...\"\n",
    "                             final_url = urllib.parse.urljoin(base_url, href)\n",
    "                    \n",
    "                    if final_url:\n",
    "                        # Extract Race Number from anchor text (e.g. \"1R\", \"11R\")\n",
    "                        # Or generic image alt?\n",
    "                        # Usually text is \"1R\" or img alt=\"1R\"\n",
    "                        txt = a.text.strip()\n",
    "                        img = a.find('img')\n",
    "                        if not txt and img and 'alt' in img.attrs:\n",
    "                            txt = img['alt']\n",
    "                        \n",
    "                        r_num = -1\n",
    "                        r_num_match = re.search(r'(\\d+)R', txt)\n",
    "                        if r_num_match:\n",
    "                             r_num = int(r_num_match.group(1))\n",
    "                             \n",
    "                        # Append even if Race Num is not found (fix for missing races)\n",
    "                        race_list_items.append((final_url, r_num))\n",
    "                \n",
    "                # Deduplicate by URL (keep first found usually fine)\n",
    "                # Sort by Race Number (unknowns (-1) first or last?)\n",
    "                seen_urls = set()\n",
    "                unique_races = []\n",
    "                for url, r_num in race_list_items:\n",
    "                    if url not in seen_urls:\n",
    "                        unique_races.append((url, r_num))\n",
    "                        seen_urls.add(url)\n",
    "                \n",
    "                unique_races.sort(key=lambda x: x[1]) # Sort by race num (-1 will be first)\n",
    "                \n",
    "                print(f\"      -> {len(unique_races)} races found.\")\n",
    "\n",
    "                for r_link, r_num in unique_races:\n",
    "                    # PRE-FETCH OPTIMIZATION\n",
    "                    # Construct ID\n",
    "                    # Only if we successfully extracted Race Num and Venue info\n",
    "                    if r_num != -1 and p_code != \"00\" and current_day_date:\n",
    "                         # ID: YYYY PP KK DD RR\n",
    "                         # y is from match_day_date loop var (int)\n",
    "                         # p_code, kai_str, day_str strings\n",
    "                         \n",
    "                         # Ensure year is from the day page date\n",
    "                         y_str = str(y)\n",
    "                         r_num_str = f\"{r_num:02}\"\n",
    "                         \n",
    "                         generated_id = f\"{y_str}{p_code}{kai_str}{day_str}{r_num_str}\"\n",
    "                         \n",
    "                         if existing_race_ids and generated_id in existing_race_ids:\n",
    "                             # print(f\"        [Skip] {generated_id} (Pre-check)\")\n",
    "                             continue\n",
    "                    \n",
    "                    # If not skipped, fetch\n",
    "                    df = scrape_jra_race(r_link, existing_race_ids=existing_race_ids)\n",
    "                    \n",
    "                    if df is not None and not df.empty:\n",
    "                        if save_callback:\n",
    "                            save_callback(df)\n",
    "                        time.sleep(1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing month {month}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®š (ã“ã“ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„)\n",
    "YEAR = 2024          # å¯¾è±¡å¹´åº¦ (ä¾‹: 2024)\n",
    "START_MONTH = 1      # é–‹å§‹æœˆ (1-12)\n",
    "END_MONTH = 12       # çµ‚äº†æœˆ (1-12)\n",
    "SAVE_DIR = '/content/drive/MyDrive/dai-keiba/data/raw' # ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€\n",
    "\n",
    "# å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯\n",
    "import os\n",
    "from datetime import date\n",
    "import calendar\n",
    "\n",
    "if YEAR:\n",
    "    # Saveãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    \n",
    "    s_date = date(int(YEAR), int(START_MONTH), 1)\n",
    "    last_day = calendar.monthrange(int(YEAR), int(END_MONTH))[1]\n",
    "    e_date = date(int(YEAR), int(END_MONTH), last_day)\n",
    "    \n",
    "    # æœªæ¥ã®æ—¥ä»˜ã¯æ¤œç´¢ã—ãªã„ã‚ˆã†ã«åˆ¶é™\n",
    "    today = date.today()\n",
    "    if e_date > today:\n",
    "        e_date = today\n",
    "    \n",
    "    save_path = os.path.join(SAVE_DIR, 'database.csv')\n",
    "    print(f'{YEAR}å¹´ã®ãƒ‡ãƒ¼ã‚¿ã‚’ {s_date} ã‹ã‚‰ {e_date} ã¾ã§å–å¾—ã—ã¾ã™...')\n",
    "    print(f'ä¿å­˜å…ˆ: {save_path}')\n",
    "    \n",
    "    # å®‰å…¨ãªè¿½è¨˜é–¢æ•° (ã‚«ãƒ©ãƒ ãšã‚Œé˜²æ­¢)\n",
    "    def safe_append_csv(df_chunk, path):\n",
    "        import pandas as pd\n",
    "        import os\n",
    "        if not os.path.exists(path):\n",
    "            df_chunk.to_csv(path, index=False)\n",
    "        else:\n",
    "            try:\n",
    "                # æ—¢å­˜ãƒ˜ãƒƒãƒ€ãƒ¼èª­ã¿è¾¼ã¿\n",
    "                existing_cols = pd.read_csv(path, nrows=0).columns.tolist()\n",
    "                # ã‚«ãƒ©ãƒ åˆã‚ã› (éä¸è¶³å¯¾å¿œ)\n",
    "                df_aligned = df_chunk.reindex(columns=existing_cols)\n",
    "                # è¿½è¨˜\n",
    "                df_aligned.to_csv(path, mode='a', header=False, index=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Save Error: {e}\")\n",
    "                # ä¸‡ãŒä¸€ã®å ´åˆã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã¦æ–°è¦ä½œæˆã™ã‚‹ãªã©ã®åˆ†å²ã‚‚å¯ã ãŒã€ã“ã“ã§ã¯ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºã®ã¿\n",
    "\n",
    "    scrape_jra_year(str(YEAR), start_date=s_date, end_date=e_date, save_callback=lambda df: safe_append_csv(df, save_path))\n",
    "    print('å®Œäº†ã—ã¾ã—ãŸã€‚')\n",
    "else:\n",
    "    print('å¹´åº¦ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}