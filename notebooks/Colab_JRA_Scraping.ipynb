{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ‡ JRA å…¨ãƒ¬ãƒ¼ã‚¹å–å¾— (2020-2026)\n",
    "ä»¥ä¸‹ã®è¨­å®šå¤‰æ•°ã‚’å¤‰æ›´ã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚æŒ‡å®šã—ãŸæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€`SAVE_DIR` ã«ä¿å­˜ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆã™ã‚‹å ´åˆã®ã¿å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2026-01-08: Rich Scraping Logic (JRA/NAR Universal)\n",
    "# This code handles scraping of Race Result + Horse History + Pedigree in one pass.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "from datetime import datetime\n",
    "import urllib.parse\n",
    "import time\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- RaceScraper Helper Class (Embedded) ---\n",
    "class RaceScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "    def _get_soup(self, url):\n",
    "        try:\n",
    "            time.sleep(1) # Be polite\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.encoding = response.apparent_encoding\n",
    "            if response.status_code == 200:\n",
    "                return BeautifulSoup(response.text, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def get_horse_profile(self, horse_id):\n",
    "        \"\"\"\n",
    "        Fetches horse profile to get pedigree (Father, Mother, Grandfather(BMS)).\n",
    "        Returns a dictionary or None.\n",
    "        \"\"\"\n",
    "        url = f\"https://db.netkeiba.com/horse/ped/{horse_id}/\"\n",
    "        soup = self._get_soup(url)\n",
    "        data = {\"father\": \"\", \"mother\": \"\", \"bms\": \"\"}\n",
    "        if not soup: return data\n",
    "\n",
    "        try:\n",
    "            table = soup.select_one(\"table.blood_table\")\n",
    "            if table:\n",
    "                rows = table.find_all(\"tr\")\n",
    "                if len(rows) >= 17:\n",
    "                    # Father: Row 0\n",
    "                    r0 = rows[0].find_all(\"td\")\n",
    "                    if r0:\n",
    "                        txt = r0[0].text.strip()\n",
    "                        data[\"father\"] = txt.split('\\n')[0].strip()\n",
    "\n",
    "                    # Mother & BMS: Row 16\n",
    "                    r16 = rows[16].find_all(\"td\")\n",
    "                    if len(r16) >= 2:\n",
    "                        m_txt = r16[0].text.strip()\n",
    "                        data[\"mother\"] = m_txt.split('\\n')[0].strip()\n",
    "                        bms_txt = r16[1].text.strip()\n",
    "                        data[\"bms\"] = bms_txt.split('\\n')[0].strip()\n",
    "        except Exception as e:\n",
    "            pass # Silent fail for profile\n",
    "        return data\n",
    "\n",
    "    def extract_run_style(self, passing_str):\n",
    "        if not isinstance(passing_str, str): return 3\n",
    "        try:\n",
    "            cleaned = re.sub(r'[^0-9-]', '', passing_str)\n",
    "            parts = [int(p) for p in cleaned.split('-') if p]\n",
    "            if not parts: return 3\n",
    "            first_corner = parts[0]\n",
    "            if first_corner == 1: return 1 # Nige\n",
    "            elif first_corner <= 4: return 2 # Senkou\n",
    "            elif first_corner <= 9: return 3 # Sashi\n",
    "            else: return 4 # Oikomi\n",
    "        except: return 3\n",
    "\n",
    "    def get_past_races(self, horse_id, current_race_date, n_samples=5):\n",
    "        \"\"\"\n",
    "        Fetches past n_samples race results.\n",
    "        Filters out races AFTER current_race_date.\n",
    "        Returns a DataFrame.\n",
    "        \"\"\"\n",
    "        url = f\"https://db.netkeiba.com/horse/result/{horse_id}/\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup: return pd.DataFrame()\n",
    "\n",
    "        table = soup.select_one(\"table.db_h_race_results\")\n",
    "        if not table:\n",
    "             tables = soup.find_all(\"table\")\n",
    "             for t in tables:\n",
    "                 if \"ç€é †\" in t.text:\n",
    "                     table = t\n",
    "                     break\n",
    "        if not table: return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            df = pd.read_html(io.StringIO(str(table)))[0]\n",
    "            df = df.dropna(how='all')\n",
    "            # Normalize columns\n",
    "            df.columns = df.columns.astype(str).str.replace(r'\\s+', '', regex=True)\n",
    "            \n",
    "            # Date Parsing\n",
    "            if 'æ—¥ä»˜' in df.columns:\n",
    "                df['date_obj'] = pd.to_datetime(df['æ—¥ä»˜'], format='%Y/%m/%d', errors='coerce')\n",
    "                df = df.dropna(subset=['date_obj'])\n",
    "                \n",
    "                # Filter past races only\n",
    "                if isinstance(current_race_date, str):\n",
    "                    current_date = pd.to_datetime(current_race_date)\n",
    "                else:\n",
    "                    current_date = pd.to_datetime(current_race_date)\n",
    "                    \n",
    "                df = df[df['date_obj'] < current_date]\n",
    "                df = df.sort_values('date_obj', ascending=False)\n",
    "            \n",
    "            if df.empty: return df\n",
    "\n",
    "            # Limit to N\n",
    "            df = df.head(n_samples)\n",
    "\n",
    "            # Extract Run Style\n",
    "            if 'é€šé' in df.columns:\n",
    "                df['run_style_val'] = df['é€šé'].apply(self.extract_run_style)\n",
    "            else:\n",
    "                df['run_style_val'] = 3\n",
    "\n",
    "            # Column Mapping\n",
    "            column_map = {\n",
    "                'æ—¥ä»˜': 'date', 'é–‹å‚¬': 'venue', 'å¤©æ°—': 'weather', 'ãƒ¬ãƒ¼ã‚¹å': 'race_name',\n",
    "                'ç€é †': 'rank', 'æ ç•ª': 'waku', 'é¦¬ç•ª': 'umaban', 'é¨æ‰‹': 'jockey',\n",
    "                'æ–¤é‡': 'weight_carried', 'é¦¬å ´': 'condition', 'ã‚¿ã‚¤ãƒ ': 'time',\n",
    "                'ç€å·®': 'margin', 'ä¸Šã‚Š': 'last_3f', 'é€šé': 'passing', 'é¦¬ä½“é‡': 'horse_weight',\n",
    "                'run_style_val': 'run_style', 'å˜å‹': 'odds', 'ã‚ªãƒƒã‚º': 'odds', 'è·é›¢': 'raw_distance'\n",
    "            }\n",
    "            df.rename(columns=column_map, inplace=True)\n",
    "            \n",
    "            # Parse Distance/Course\n",
    "            if 'raw_distance' in df.columns:\n",
    "                def parse_dist(x):\n",
    "                    if not isinstance(x, str): return None, None\n",
    "                    surf = None; dist = None\n",
    "                    if 'èŠ' in x: surf = 'èŠ'\n",
    "                    elif 'ãƒ€' in x: surf = 'ãƒ€'\n",
    "                    elif 'éšœ' in x: surf = 'éšœ'\n",
    "                    match = re.search(r'(\\d+)', x)\n",
    "                    if match: dist = int(match.group(1))\n",
    "                    return surf, dist\n",
    "                \n",
    "                parsed = df['raw_distance'].apply(parse_dist)\n",
    "                df['course_type'] = parsed.apply(lambda x: x[0])\n",
    "                df['distance'] = parsed.apply(lambda x: x[1])\n",
    "            else:\n",
    "                df['course_type'] = None; df['distance'] = None\n",
    "\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# --- Main Rich Scraper Function ---\n",
    "def scrape_race_rich(url, existing_race_ids=None, max_retries=3):\n",
    "    \"\"\"\n",
    "    Scrapes Race + History + Pedigree in one go.\n",
    "    \"\"\"\n",
    "    print(f\"  Analysing Race: {url}\")\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    \n",
    "    # 1. Fetch Race Page\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=15)\n",
    "        resp.encoding = 'EUC-JP'\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        \n",
    "        # --- Metadata (Date, Venue, Race Name, etc) ---\n",
    "        # (This part reuses logic from original scrape_jra_race)\n",
    "        h1_elem = soup.select_one(\"div.header_line h1 .txt\")\n",
    "        full_text = h1_elem.text.strip() if h1_elem else (soup.h1.text.strip() if soup.h1 else \"\")\n",
    "        \n",
    "        date_text = \"\"; venue_text = \"\"; kai = \"01\"; day = \"01\"; r_num = \"10\"\n",
    "        match_date = re.search(r'(\\d{4}å¹´\\d{1,2}æœˆ\\d{1,2}æ—¥)', full_text)\n",
    "        if match_date: date_text = match_date.group(1)\n",
    "        \n",
    "        venues_str = \"æœ­å¹Œ|å‡½é¤¨|ç¦å³¶|æ–°æ½Ÿ|æ±äº¬|ä¸­å±±|ä¸­äº¬|äº¬éƒ½|é˜ªç¥|å°å€‰\"\n",
    "        match_meta = re.search(rf'(\\d+)å›({venues_str})(\\\\d+)æ—¥', full_text)\n",
    "        if match_meta:\n",
    "            kai = f\"{int(match_meta.group(1)):02}\"\n",
    "            venue_text = match_meta.group(2)\n",
    "            day = f\"{int(match_meta.group(3)):02}\"\n",
    "            \n",
    "        match_race = re.search(r'(\\d+)ãƒ¬ãƒ¼ã‚¹', full_text)\n",
    "        if match_race: r_num = f\"{int(match_race.group(1)):02}\"\n",
    "        \n",
    "        place_map = {\n",
    "            \"æœ­å¹Œ\": \"01\", \"å‡½é¤¨\": \"02\", \"ç¦å³¶\": \"03\", \"æ–°æ½Ÿ\": \"04\", \"æ±äº¬\": \"05\",\n",
    "            \"ä¸­å±±\": \"06\", \"ä¸­äº¬\": \"07\", \"äº¬éƒ½\": \"08\", \"é˜ªç¥\": \"09\", \"å°å€‰\": \"10\"\n",
    "        }\n",
    "        p_code = place_map.get(venue_text, \"00\")\n",
    "        year = date_text[:4] if date_text else \"2025\"\n",
    "        \n",
    "        race_id = f\"{year}{p_code}{kai}{day}{r_num}\"\n",
    "        \n",
    "        # SKIP Check\n",
    "        if existing_race_ids and race_id in existing_race_ids:\n",
    "            return None\n",
    "\n",
    "        # Basic Race Info\n",
    "        race_name = soup.select_one(\".race_name\").text.strip() if soup.select_one(\".race_name\") else \"\"\n",
    "        header_text = soup.select_one(\"div.header_line\").text if soup.select_one(\"div.header_line\") else full_text\n",
    "        \n",
    "        # Course/Dist\n",
    "        course_type = \"\"; distance = \"\"\n",
    "        dt_match = re.search(r'(èŠ|ãƒ€|ãƒ€ãƒ¼ãƒˆ|éšœå®³)[^0-9]*(\\d+)', header_text)\n",
    "        if dt_match:\n",
    "            c = dt_match.group(1)\n",
    "            course_type = 'èŠ' if 'èŠ' in c else ('ãƒ€ãƒ¼ãƒˆ' if 'ãƒ€' in c else 'éšœå®³')\n",
    "            distance = dt_match.group(2)\n",
    "            \n",
    "        # Weather/Condition\n",
    "        weather = \"\"; condition = \"\"; rotation = \"\"\n",
    "        w_match = re.search(r'å¤©å€™\\s*[:ï¼š]\\s*(\\S+)', soup.text)\n",
    "        if w_match: weather = w_match.group(1)\n",
    "        \n",
    "        c_match = re.search(r'(?:èŠ|ãƒ€ãƒ¼ãƒˆ)\\s*[:ï¼š]\\s*(\\S+)', soup.text)\n",
    "        if c_match: condition = c_match.group(1)\n",
    "        \n",
    "        if \"å³\" in header_text: rotation = \"å³\"\n",
    "        elif \"å·¦\" in header_text: rotation = \"å·¦\"\n",
    "        elif \"ç›´\" in header_text: rotation = \"ç›´ç·š\"\n",
    "\n",
    "        # --- Parse Result Table ---\n",
    "        tables = soup.find_all('table')\n",
    "        target_table = None\n",
    "        for t in tables:\n",
    "            if \"ç€é †\" in t.text and \"é¦¬å\" in t.text:\n",
    "                target_table = t\n",
    "                break\n",
    "        \n",
    "        if not target_table: return None\n",
    "        \n",
    "        rows = target_table.find_all('tr')\n",
    "        race_scraper = RaceScraper()\n",
    "        \n",
    "        data_list = []\n",
    "        \n",
    "        # Pre-convert date for filtering\n",
    "        d_obj = pd.to_datetime(date_text, format='%Yå¹´%mæœˆ%dæ—¥') if date_text else datetime.now()\n",
    "        \n",
    "        print(f\"    Fetching details for {len(rows)-1} horses...\")\n",
    "        \n",
    "        for row in rows:\n",
    "            if row.find('th'): continue # Skip header\n",
    "            cells = row.find_all('td')\n",
    "            if not cells: continue\n",
    "            \n",
    "            # Basic info\n",
    "            rank = cells[0].text.strip()\n",
    "            waku = \"\"\n",
    "            if cells[1].find('img'): \n",
    "                alt = cells[1].find('img').get('alt', '')\n",
    "                m = re.search(r'æ (\\d+)', alt)\n",
    "                waku = m.group(1) if m else alt\n",
    "            umaban = cells[2].text.strip()\n",
    "            horse_name_elem = cells[3].find('a')\n",
    "            horse_name = cells[3].text.strip()\n",
    "            horse_id = \"\"\n",
    "            if horse_name_elem and 'href' in horse_name_elem.attrs:\n",
    "                hm = re.search(r'/horse/(\\d+)', horse_name_elem['href'])\n",
    "                if hm: horse_id = hm.group(1)\n",
    "            \n",
    "            jockey = cells[6].text.strip()\n",
    "            time_val = cells[7].text.strip()\n",
    "            # ... other basic fields\n",
    "            \n",
    "            # --- Rich Fetching (History & Pedigree) ---\n",
    "            blood_data = {\"father\": \"\", \"mother\": \"\", \"bms\": \"\"}\n",
    "            past_data = {}\n",
    "            \n",
    "            if horse_id:\n",
    "                # 1. Pedigree\n",
    "                blood_data = race_scraper.get_horse_profile(horse_id)\n",
    "                \n",
    "                # 2. History\n",
    "                df_past = race_scraper.get_past_races(horse_id, d_obj, n_samples=5)\n",
    "                \n",
    "                # Flatten History\n",
    "                for i in range(5):\n",
    "                    prefix = f\"past_{i+1}\"\n",
    "                    if i < len(df_past):\n",
    "                        r = df_past.iloc[i]\n",
    "                        past_data[f\"{prefix}_date\"] = r.get('date', '')\n",
    "                        past_data[f\"{prefix}_rank\"] = r.get('rank', '')\n",
    "                        past_data[f\"{prefix}_time\"] = r.get('time', '')\n",
    "                        past_data[f\"{prefix}_run_style\"] = r.get('run_style', '')\n",
    "                        past_data[f\"{prefix}_race_name\"] = r.get('race_name', '')\n",
    "                        past_data[f\"{prefix}_last_3f\"] = r.get('last_3f', '')\n",
    "                        past_data[f\"{prefix}_horse_weight\"] = r.get('horse_weight', '')\n",
    "                        past_data[f\"{prefix}_jockey\"] = r.get('jockey', '')\n",
    "                        past_data[f\"{prefix}_condition\"] = r.get('condition', '')\n",
    "                        past_data[f\"{prefix}_odds\"] = r.get('odds', '')\n",
    "                        past_data[f\"{prefix}_weather\"] = r.get('weather', '')\n",
    "                        past_data[f\"{prefix}_distance\"] = r.get('distance', '')\n",
    "                        past_data[f\"{prefix}_course_type\"] = r.get('course_type', '')\n",
    "                    else:\n",
    "                        # Fill Empty\n",
    "                        for col in ['date','rank','time','run_style','race_name','last_3f','horse_weight','jockey','condition','odds','weather','distance','course_type']:\n",
    "                            past_data[f\"{prefix}_{col}\"] = \"\"\n",
    "\n",
    "                # Be polite between horses\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            # Combine\n",
    "            row_dict = {\n",
    "                \"æ—¥ä»˜\": date_text, \"ä¼šå ´\": venue_text, \"ãƒ¬ãƒ¼ã‚¹ç•ªå·\": f\"{r_num}R\", \"ãƒ¬ãƒ¼ã‚¹å\": race_name, \"é‡è³\": \"\", # Grade logic omitted for brevity but should exist\n",
    "                \"ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—\": course_type, \"è·é›¢\": distance, \"å›ã‚Š\": rotation, \"å¤©å€™\": weather, \"é¦¬å ´çŠ¶æ…‹\": condition,\n",
    "                \"ç€é †\": rank, \"æ \": waku, \"é¦¬ç•ª\": umaban, \"é¦¬å\": horse_name, \n",
    "                \"æ€§é½¢\": cells[4].text.strip(), \"æ–¤é‡\": cells[5].text.strip(), \"é¨æ‰‹\": jockey, \n",
    "                \"ã‚¿ã‚¤ãƒ \": time_val, \"ç€å·®\": cells[8].text.strip(), \"äººæ°—\": cells[13].text.strip(), \n",
    "                \"å˜å‹ã‚ªãƒƒã‚º\": cells[14].text.strip() if len(cells)>14 else \"0.0\", \n",
    "                \"å¾Œ3F\": cells[10].text.strip(), \"å©èˆ\": cells[12].text.strip(), \n",
    "                \"é¦¬ä½“é‡(å¢—æ¸›)\": cells[11].text.strip(),\n",
    "                \"race_id\": race_id, \"horse_id\": horse_id,\n",
    "                **blood_data,\n",
    "                **past_data\n",
    "            }\n",
    "            data_list.append(row_dict)\n",
    "            \n",
    "        return pd.DataFrame(data_list)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping race {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Year/Month Iteration Logic (Scrape Year Rich) ---\n",
    "def scrape_jra_year_rich(year_str, start_date=None, end_date=None, save_callback=None, existing_race_ids=None):\n",
    "    # Parameter Map for Monthly Results\n",
    "    JRA_MONTH_PARAMS = {\n",
    "        \"2026\": { \"01\": \"E4\", \"02\": \"B2\", \"03\": \"80\", \"04\": \"4E\", \"05\": \"1C\", \"06\": \"EA\", \"07\": \"B8\", \"08\": \"86\", \"09\": \"54\", \"10\": \"22\", \"11\": \"F0\", \"12\": \"BE\" },\n",
    "        \"2025\": { \"01\": \"3F\", \"02\": \"0D\", \"03\": \"DB\", \"04\": \"A9\", \"05\": \"77\", \"06\": \"45\", \"07\": \"13\", \"08\": \"E1\", \"09\": \"AF\", \"10\": \"1E\", \"11\": \"EC\", \"12\": \"D3\" },\n",
    "        \"2024\": { \"01\": \"B3\", \"02\": \"81\", \"03\": \"4F\", \"04\": \"1D\", \"05\": \"EB\", \"06\": \"B9\", \"07\": \"87\", \"08\": \"55\", \"09\": \"23\", \"10\": \"92\", \"11\": \"60\", \"12\": \"2E\" },\n",
    "        \"2023\": { \"01\": \"27\", \"02\": \"F5\", \"03\": \"C3\", \"04\": \"91\", \"05\": \"5F\", \"06\": \"2D\", \"07\": \"FB\", \"08\": \"C9\", \"09\": \"97\", \"10\": \"06\", \"11\": \"D4\", \"12\": \"A2\" },\n",
    "        \"2022\": { \"01\": \"9B\", \"02\": \"69\", \"03\": \"37\", \"04\": \"05\", \"05\": \"D3\", \"06\": \"A1\", \"07\": \"6F\", \"08\": \"3D\", \"09\": \"0B\", \"10\": \"7A\", \"11\": \"48\", \"12\": \"16\" },\n",
    "        \"2021\": { \"01\": \"0F\", \"02\": \"DD\", \"03\": \"AB\", \"04\": \"79\", \"05\": \"47\", \"06\": \"15\", \"07\": \"E3\", \"08\": \"B1\", \"09\": \"7F\", \"10\": \"EE\", \"11\": \"BC\", \"12\": \"8A\" },\n",
    "        \"2020\": { \"01\": \"83\", \"02\": \"51\", \"03\": \"1F\", \"04\": \"ED\", \"05\": \"BB\", \"06\": \"89\", \"07\": \"57\", \"08\": \"25\", \"09\": \"F3\", \"10\": \"62\", \"11\": \"30\", \"12\": \"FE\" }\n",
    "    }\n",
    "    \n",
    "    if year_str not in JRA_MONTH_PARAMS:\n",
    "        print(f\"Year {year_str} not supported in parameter map.\")\n",
    "        return\n",
    "\n",
    "    params = JRA_MONTH_PARAMS[year_str]\n",
    "    base_url = \"https://www.jra.go.jp/JRADB/accessS.html\"\n",
    "\n",
    "    # Determine months to iterate\n",
    "    start_m = 1\n",
    "    end_m = 12\n",
    "\n",
    "    if start_date:\n",
    "        start_m = start_date.month\n",
    "    if end_date:\n",
    "        end_m = end_date.month\n",
    "\n",
    "    # Cap at Today\n",
    "    from datetime import date\n",
    "    today = date.today()\n",
    "\n",
    "    if end_date:\n",
    "        actual_end_date = min(end_date, today)\n",
    "    else:\n",
    "        actual_end_date = today\n",
    "\n",
    "    print(f\"=== Starting JRA Bulk Scraping for {year_str} (Rich Data) ===\")\n",
    "    print(f\"Period: {start_date or 'Start'} - {actual_end_date}\")\n",
    "    \n",
    "    if int(year_str) == today.year:\n",
    "        end_m = min(end_m, today.month)\n",
    "\n",
    "    total_processed = 0\n",
    "\n",
    "    for m in range(start_m, end_m + 1):\n",
    "        month = f\"{m:02}\"\n",
    "        if month not in params:\n",
    "            continue\n",
    "\n",
    "        suffix = params[month]\n",
    "        try:\n",
    "            ym = int(year_str + month)\n",
    "            prefix = \"pw01skl00\" if ym >= 202512 else \"pw01skl10\"\n",
    "        except:\n",
    "            prefix = \"pw01skl10\"\n",
    "\n",
    "        cname = f\"{prefix}{year_str}{month}/{suffix}\"\n",
    "\n",
    "        print(f\"\\\\nğŸ“… Fetching {year_str}/{month}...\")\n",
    "\n",
    "        try:\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "            }\n",
    "            response = requests.post(base_url, data={\"cname\": cname}, headers=headers, timeout=15)\n",
    "            response.encoding = 'cp932'\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(f\"âŒ Failed to fetch {cname} (Status {response.status_code})\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            race_cnames = []\n",
    "            links = soup.find_all('a')\n",
    "            for link in links:\n",
    "                onclick = link.get('onclick', '')\n",
    "                # FIX: simplified regex to avoid escaping hell. Using raw string with minimal escaping.\n",
    "                # Expected pattern: doAction('...', 'pw01srl...')\n",
    "                match = re.search(r\"doAction\\('[^']+',\\s*'([^']+)'\\)\", onclick)\n",
    "                if match:\n",
    "                    c = match.group(1)\n",
    "                    if c.startswith('pw01srl'):\n",
    "                        race_cnames.append(c)\n",
    "\n",
    "            race_cnames = sorted(list(set(race_cnames)))\n",
    "            print(f\"  Found {len(race_cnames)} race days\")\n",
    "\n",
    "            for day_cname in tqdm(race_cnames, desc=f\"  {year_str}/{month}\", leave=False):\n",
    "                resp_day = requests.post(base_url, data={\"cname\": day_cname}, headers=headers, timeout=15)\n",
    "                resp_day.encoding = 'cp932'\n",
    "                soup_day = BeautifulSoup(resp_day.text, 'html.parser')\n",
    "\n",
    "                race_list_items = []\n",
    "                all_anchors = soup_day.find_all('a')\n",
    "                for a in all_anchors:\n",
    "                    onclick = a.get('onclick', '')\n",
    "                    # FIX: simplified regex here too.\n",
    "                    # Expected pattern: doAction('...', 'pw01sde...')\n",
    "                    # OR: doAction('...', 'pw01sde...') with different spacing\n",
    "                    match_sde = re.search(r\"doAction\\s*\\(\\s*['\\\"][^'\\\"]+['\\\"]\\s*,\\s*['\\\"](pw01sde[^'\\\"]+)['\\\"]\\s*\\)\", onclick)\n",
    "                    \n",
    "                    href = a.get('href', '')\n",
    "\n",
    "                    final_url = \"\"\n",
    "                    if match_sde:\n",
    "                        final_url = f\"{base_url}?CNAME={match_sde.group(1)}\"\n",
    "                    elif 'pw01sde' in href:\n",
    "                         if 'CNAME=' in href:\n",
    "                             final_url = urllib.parse.urljoin(base_url, href)\n",
    "                         else:\n",
    "                             final_url = urllib.parse.urljoin(base_url, href)\n",
    "\n",
    "                    if final_url:\n",
    "                        race_list_items.append(final_url)\n",
    "\n",
    "                unique_races = sorted(list(set(race_list_items)))\n",
    "\n",
    "                for r_link in unique_races:\n",
    "                    # Fetch with Rich Scraper\n",
    "                    df = scrape_race_rich(r_link, existing_race_ids=existing_race_ids)\n",
    "\n",
    "                    if df is not None and not df.empty:\n",
    "                        if save_callback:\n",
    "                            save_callback(df)\n",
    "                        total_processed += 1\n",
    "                    \n",
    "                    # Rate limiting\n",
    "                    time.sleep(1.0) \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing month {month}: {e}\")\n",
    "            \n",
    "    print(\"Completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®š (ã“ã“ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„)\n",
    "YEAR = 2024          # å¯¾è±¡å¹´åº¦ (ä¾‹: 2024)\n",
    "START_MONTH = 1      # é–‹å§‹æœˆ (1-12)\n",
    "END_MONTH = 12       # çµ‚äº†æœˆ (1-12)\n",
    "SAVE_DIR = '/content/drive/MyDrive/dai-keiba/data/raw' # ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€\n",
    "\n",
    "# å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import calendar\n",
    "\n",
    "if YEAR:\n",
    "    # Saveãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    \n",
    "    s_date = date(int(YEAR), int(START_MONTH), 1)\n",
    "    last_day = calendar.monthrange(int(YEAR), int(END_MONTH))[1]\n",
    "    e_date = date(int(YEAR), int(END_MONTH), last_day)\n",
    "    \n",
    "    # æœªæ¥ã®æ—¥ä»˜ã¯æ¤œç´¢ã—ãªã„ã‚ˆã†ã«åˆ¶é™\n",
    "    today = date.today()\n",
    "    if e_date > today:\n",
    "        e_date = today\n",
    "    \n",
    "    save_path = os.path.join(SAVE_DIR, 'database.csv')\n",
    "    print(f'{YEAR}å¹´ã®ãƒ‡ãƒ¼ã‚¿ã‚’ {s_date} ã‹ã‚‰ {e_date} ã¾ã§å–å¾—ã—ã¾ã™...')\n",
    "    print(f'ä¿å­˜å…ˆ: {save_path}')\n",
    "    \n",
    "    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ (é‡è¤‡å–å¾—é˜²æ­¢)\n",
    "    existing_race_ids = None\n",
    "    if os.path.exists(save_path):\n",
    "        try:\n",
    "            df_exist = pd.read_csv(save_path, usecols=['race_id'])\n",
    "            existing_race_ids = set(df_exist['race_id'].astype(str))\n",
    "            print(f'æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(existing_race_ids)} ä»¶ã®ãƒ¬ãƒ¼ã‚¹IDã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚')\n",
    "        except Exception as e:\n",
    "            print(f'æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}')\n",
    "    \n",
    "    # å®‰å…¨ãªè¿½è¨˜é–¢æ•° (ã‚«ãƒ©ãƒ ãšã‚Œé˜²æ­¢)\n",
    "    def safe_append_csv(df_chunk, path):\n",
    "        import pandas as pd\n",
    "        import os\n",
    "        if not os.path.exists(path):\n",
    "            df_chunk.to_csv(path, index=False)\n",
    "        else:\n",
    "            try:\n",
    "                # æ—¢å­˜ãƒ˜ãƒƒãƒ€ãƒ¼èª­ã¿è¾¼ã¿\n",
    "                existing_cols = pd.read_csv(path, nrows=0).columns.tolist()\n",
    "                # ã‚«ãƒ©ãƒ åˆã‚ã› (éä¸è¶³å¯¾å¿œ)\n",
    "                df_aligned = df_chunk.reindex(columns=existing_cols)\n",
    "                # è¿½è¨˜\n",
    "                df_aligned.to_csv(path, mode='a', header=False, index=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Save Error: {e}\")\n",
    "                # ä¸‡ãŒä¸€ã®å ´åˆã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã¦æ–°è¦ä½œæˆã™ã‚‹ãªã©ã®åˆ†å²ã‚‚å¯ã ãŒã€ã“ã“ã§ã¯ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºã®ã¿\n",
    "\n",
    "    scrape_jra_year_rich(str(YEAR), start_date=s_date, end_date=e_date, save_callback=lambda df: safe_append_csv(df, save_path), existing_race_ids=existing_race_ids)\n",
    "    print('å®Œäº†ã—ã¾ã—ãŸã€‚')\n",
    "else:\n",
    "    print('å¹´åº¦ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}