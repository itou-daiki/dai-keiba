{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèá JRA All-Race Scraper (20202026)\n",
    "Run this notebook to scrape all JRA races for a specific year and month range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "from datetime import datetime\n",
    "import urllib.parse\n",
    "import time\n",
    "\n",
    "def scrape_jra_race(url, existing_race_ids=None):\n",
    "    \"\"\"\n",
    "    Scrapes a single race page from JRA website.\n",
    "    Returns a pandas DataFrame matching the schema of database.csv.\n",
    "    If existing_race_ids is provided and the race ID is found, returns None (skip).\n",
    "    \"\"\"\n",
    "    print(f\"Accessing JRA URL: {url}...\")\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" \n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.encoding = 'cp932' # JRA uses Shift_JIS\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: Status code {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # --- Metadata Extraction ---\n",
    "        h1_elem = soup.select_one(\"div.header_line h1 .txt\")\n",
    "        full_text = h1_elem.text.strip() if h1_elem else \"\"\n",
    "        if not full_text and soup.h1:\n",
    "            full_text = soup.h1.text.strip()\n",
    "\n",
    "        date_text = \"\"\n",
    "        venue_text = \"\"\n",
    "        race_num_text = \"\"\n",
    "        kai = \"01\"\n",
    "        day = \"01\"\n",
    "        \n",
    "        # Extract Date\n",
    "        match_date = re.search(r'(\\d{4}Âπ¥\\d{1,2}Êúà\\d{1,2}Êó•)', full_text)\n",
    "        if match_date:\n",
    "            date_text = match_date.group(1)\n",
    "            \n",
    "        # Extract Venue, Kai, Day\n",
    "        venues_str = \"Êú≠Âπå|ÂáΩÈ§®|Á¶èÂ≥∂|Êñ∞ÊΩü|Êù±‰∫¨|‰∏≠Â±±|‰∏≠‰∫¨|‰∫¨ÈÉΩ|Èò™Á•û|Â∞èÂÄâ\"\n",
    "        match_meta = re.search(rf'(\\d+)Âõû({venues_str})(\\d+)Êó•', full_text)\n",
    "        if match_meta:\n",
    "            kai = f\"{int(match_meta.group(1)):02}\"\n",
    "            venue_text = match_meta.group(2)\n",
    "            day = f\"{int(match_meta.group(3)):02}\"\n",
    "            \n",
    "        # Extract Race Num\n",
    "        match_race = re.search(r'(\\d+)„É¨„Éº„Çπ', full_text)\n",
    "        if match_race:\n",
    "            r_val = int(match_race.group(1))\n",
    "            race_num_text = f\"{r_val}R\"\n",
    "            r_num = f\"{r_val:02}\"\n",
    "        else:\n",
    "            race_num_text = \"10R\" # Fallback\n",
    "            r_num = \"10\"\n",
    "            \n",
    "        # Race Name\n",
    "        race_name_text = \"\"\n",
    "        name_elem = soup.select_one(\".race_name\")\n",
    "        if name_elem:\n",
    "            race_name_text = name_elem.text.strip()\n",
    "\n",
    "        # Grade\n",
    "        grade_text = \"\"\n",
    "        if \"G1\" in str(soup) or \"Ôºß‚Ö†\" in str(soup): grade_text = \"G1\"\n",
    "        elif \"G2\" in str(soup) or \"Ôºß‚Ö°\" in str(soup): grade_text = \"G2\"\n",
    "        elif \"G3\" in str(soup) or \"Ôºß‚Ö¢\" in str(soup): grade_text = \"G3\"\n",
    "\n",
    "        # --- Added: Course, Distance, Weather, Condition ---\n",
    "        # JRA HTML structure varies, but often contained in specific divs or text lines.\n",
    "        # We will scan the entire header text or specific class for these patterns.\n",
    "        \n",
    "        # 1. Course & Distance (e.g., \"Ëäù2000„É°„Éº„Éà„É´\", \"„ÉÄ„Éº„Éà1800„É°„Éº„Éà„É´\", \"Ëäù„Éª1600m\")\n",
    "        # Usually in the same block as race name or just below.\n",
    "        # We search the whole header area text.\n",
    "        header_text = soup.select_one(\"div.header_line\").text if soup.select_one(\"div.header_line\") else soup.text\n",
    "        \n",
    "        # Regex: Allow spaces, dots, etc. between Type and Dist\n",
    "        dist_type_match = re.search(r'(Ëäù|„ÉÄ|„ÉÄ„Éº„Éà|ÈöúÂÆ≥)[^0-9]*(\\d+)', header_text)\n",
    "        course_type = \"\"\n",
    "        distance = \"\"\n",
    "        \n",
    "        if dist_type_match:\n",
    "            c_val = dist_type_match.group(1)\n",
    "            d_val = dist_type_match.group(2)\n",
    "            \n",
    "            if \"Ëäù\" in c_val: course_type = \"Ëäù\"\n",
    "            elif \"„ÉÄ\" in c_val: course_type = \"„ÉÄ„Éº„Éà\"\n",
    "            elif \"Èöú\" in c_val: course_type = \"ÈöúÂÆ≥\"\n",
    "            \n",
    "            distance = int(d_val)\n",
    "        \n",
    "        # 1.5 Rotation (Right/Left/Straight)\n",
    "        rotation = \"\"\n",
    "        # Often formatted as \"(Âè≥)\" or \"ÔºàÂ∑¶Ôºâ\" \n",
    "        rot_match = re.search(r'[Ôºà\\(](Âè≥|Â∑¶|Áõ¥Á∑ö)[Ôºâ\\)]', header_text)\n",
    "        if rot_match:\n",
    "            rotation = rot_match.group(1)\n",
    "        else:\n",
    "             # Fallback: Inference based on Venue\n",
    "             # Tokyo, Chukyo, Niigata -> Left (Default), others Right\n",
    "             # Niigata 1000m -> Straight\n",
    "             if \"Â∑¶\" in header_text: rotation = \"Â∑¶\"\n",
    "             elif \"Âè≥\" in header_text: rotation = \"Âè≥\"\n",
    "             elif \"Áõ¥Á∑ö\" in header_text: rotation = \"Áõ¥Á∑ö\"\n",
    "\n",
    "        \n",
    "        # 2. Weather (e.g., \"Â§©ÂÄôÔºöÊô¥\")\n",
    "        weather = \"\"\n",
    "        w_match = re.search(r'Â§©ÂÄô\\s*[:Ôºö]\\s*(\\S+)', soup.text)\n",
    "        if w_match:\n",
    "            weather = w_match.group(1).strip()\n",
    "            \n",
    "        # 3. Condition (e.g., \"ËäùÔºöËâØ\", \"„ÉÄ„Éº„ÉàÔºöÁ®çÈáç\")\n",
    "        # Note: A race can have both if it's mixed, but usually we care about the main one or the one matching course_type.\n",
    "        condition = \"\"\n",
    "        \n",
    "        # Try specific pattern based on course type\n",
    "        if course_type == \"Ëäù\":\n",
    "             c_match = re.search(r'Ëäù\\s*[:Ôºö]\\s*(\\S+)', soup.text)\n",
    "             if c_match: condition = c_match.group(1).strip()\n",
    "        elif course_type == \"„ÉÄ„Éº„Éà\":\n",
    "             c_match = re.search(r'„ÉÄ„Éº„Éà\\s*[:Ôºö]\\s*(\\S+)', soup.text)\n",
    "             if c_match: condition = c_match.group(1).strip()\n",
    "        \n",
    "        # Fallback if generic or course type unknown, grab first one found\n",
    "        if not condition:\n",
    "             c_match_gen = re.search(r'(?:Ëäù|„ÉÄ„Éº„Éà)\\s*[:Ôºö]\\s*(\\S+)', soup.text)\n",
    "             if c_match_gen: condition = c_match_gen.group(1).strip()\n",
    "\n",
    "        # --- Table Extraction (Custom BS4 Parsing) ---\n",
    "        # Find table with \"ÁùÄÈ†Ü\"\n",
    "        tables = soup.find_all('table')\n",
    "        target_table = None\n",
    "        for tbl in tables:\n",
    "            if \"ÁùÄÈ†Ü\" in tbl.text and \"È¶¨Âêç\" in tbl.text:\n",
    "                target_table = tbl\n",
    "                break\n",
    "        \n",
    "        if not target_table:\n",
    "            return None\n",
    "\n",
    "        # Parse Rows\n",
    "        rows = target_table.find_all('tr')\n",
    "        data = []\n",
    "        \n",
    "        for row in rows:\n",
    "            # Skip header (usually th) or invalid rows\n",
    "            if row.find('th'):\n",
    "                continue\n",
    "                \n",
    "            cells = row.find_all('td')\n",
    "            if not cells:\n",
    "                continue\n",
    "\n",
    "            # Need to robustly map cells. \n",
    "            # We can use class names if available, or index.\n",
    "            # Based on debug:\n",
    "            # 0: place (ÁùÄÈ†Ü)\n",
    "            # 1: waku (Êû†) -> img alt\n",
    "            # 2: num (È¶¨Áï™)\n",
    "            # 3: horse (È¶¨Âêç)\n",
    "            # 4: age (ÊÄßÈΩ¢)\n",
    "            # 5: weight (Êñ§Èáè)\n",
    "            # 6: jockey (È®éÊâã)\n",
    "            # 7: time („Çø„Ç§„É†)\n",
    "            # 8: margin (ÁùÄÂ∑Æ)\n",
    "            # 9: corner (ÈÄöÈÅé)\n",
    "            # 10: f_time (‰∏ä„Çä)\n",
    "            # 11: h_weight (È¶¨‰ΩìÈáç)\n",
    "            # 12: trainer (Ë™øÊïôÂ∏´)\n",
    "            # 13: pop (‰∫∫Ê∞ó)\n",
    "            # * Odds is missing *\n",
    "\n",
    "            def get_text(idx):\n",
    "                if idx < len(cells):\n",
    "                    return cells[idx].get_text(strip=True)\n",
    "                return \"\"\n",
    "\n",
    "            # Extract Frame (Waku) from Image\n",
    "            waku_text = \"\"\n",
    "            if len(cells) > 1:\n",
    "                img = cells[1].find('img')\n",
    "                if img and 'alt' in img.attrs:\n",
    "                    # Example: \"Êû†6Á∑ë\" -> Extract number\n",
    "                    alt = img['alt']\n",
    "                    m = re.search(r'Êû†(\\d+)', alt)\n",
    "                    if m:\n",
    "                        waku_text = m.group(1)\n",
    "                    else:\n",
    "                        waku_text = alt # Fallback\n",
    "            \n",
    "            # Extract Horse ID\n",
    "            horse_id = \"\"\n",
    "            if len(cells) > 3:\n",
    "                a_tag = cells[3].find('a')\n",
    "                if a_tag and 'href' in a_tag.attrs:\n",
    "                    href = a_tag['href']\n",
    "                    # /horse/2018105247/\n",
    "                    m = re.search(r'/horse/(\\d+)', href)\n",
    "                    if m:\n",
    "                        horse_id = m.group(1)\n",
    "\n",
    "            row_data = {\n",
    "                'ÁùÄ È†Ü': get_text(0),\n",
    "                'Êû†': waku_text,\n",
    "                'È¶¨ Áï™': get_text(2),\n",
    "                'È¶¨Âêç': get_text(3),\n",
    "                'horse_id': horse_id, # Added\n",
    "                'ÊÄßÈΩ¢': get_text(4),\n",
    "                'Êñ§Èáè': get_text(5),\n",
    "                'È®éÊâã': get_text(6),\n",
    "                '„Çø„Ç§„É†': get_text(7),\n",
    "                'ÁùÄÂ∑Æ': get_text(8),\n",
    "                '„Ç≥„Éº„Éä„Éº ÈÄöÈÅéÈ†Ü': get_text(9),\n",
    "                'Âæå3F': get_text(10),\n",
    "                'È¶¨‰ΩìÈáç (Â¢óÊ∏õ)': get_text(11),\n",
    "                'Âé©Ëàé': get_text(12),\n",
    "                '‰∫∫ Ê∞ó': get_text(13),\n",
    "                'ÂçòÂãù „Ç™„ÉÉ„Ç∫': \"0.0\" # Missing in source\n",
    "            }\n",
    "            data.append(row_data)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Add Metadata Columns\n",
    "        df['Êó•‰ªò'] = date_text\n",
    "        df['‰ºöÂ†¥'] = venue_text\n",
    "        df['„É¨„Éº„ÇπÁï™Âè∑'] = race_num_text\n",
    "        df['„É¨„Éº„ÇπÂêç'] = race_name_text\n",
    "        df['ÈáçË≥û'] = grade_text\n",
    "        df['Ë∑ùÈõ¢'] = distance\n",
    "        df['„Ç≥„Éº„Çπ„Çø„Ç§„Éó'] = course_type\n",
    "        df['Â§©ÂÄô'] = weather\n",
    "        df['Â§©ÂÄô'] = weather\n",
    "        df['È¶¨Â†¥Áä∂ÊÖã'] = condition\n",
    "        df['Âõû„Çä'] = rotation\n",
    "        \n",
    "        # ID Generation\n",
    "        place_map = {\n",
    "            \"Êú≠Âπå\": \"01\", \"ÂáΩÈ§®\": \"02\", \"Á¶èÂ≥∂\": \"03\", \"Êñ∞ÊΩü\": \"04\", \"Êù±‰∫¨\": \"05\",\n",
    "            \"‰∏≠Â±±\": \"06\", \"‰∏≠‰∫¨\": \"07\", \"‰∫¨ÈÉΩ\": \"08\", \"Èò™Á•û\": \"09\", \"Â∞èÂÄâ\": \"10\"\n",
    "        }\n",
    "        p_code = place_map.get(venue_text, \"00\")\n",
    "        \n",
    "        year = \"2025\"\n",
    "        if date_text:\n",
    "            year = date_text[:4]\n",
    "            \n",
    "        generated_id = f\"{year}{p_code}{kai}{day}{r_num}\"\n",
    "        \n",
    "        # SKIP CHECK\n",
    "        if existing_race_ids and generated_id in existing_race_ids:\n",
    "            print(f\"Skipping {generated_id} (Already exists)\")\n",
    "            return None\n",
    "\n",
    "        df['race_id'] = generated_id\n",
    "\n",
    "        # Cleanups\n",
    "        if 'ÂçòÂãù „Ç™„ÉÉ„Ç∫' in df.columns:\n",
    "            df['ÂçòÂãù „Ç™„ÉÉ„Ç∫'] = pd.to_numeric(df['ÂçòÂãù „Ç™„ÉÉ„Ç∫'], errors='coerce').fillna(0.0)\n",
    "\n",
    "        standard_columns = [\n",
    "            \"Êó•‰ªò\",\"‰ºöÂ†¥\",\"„É¨„Éº„ÇπÁï™Âè∑\",\"„É¨„Éº„ÇπÂêç\",\"ÈáçË≥û\",\"ÁùÄ È†Ü\",\"Êû†\",\"È¶¨ Áï™\",\"È¶¨Âêç\",\"ÊÄßÈΩ¢\",\"Êñ§Èáè\",\"È®éÊâã\",\n",
    "            \"„Çø„Ç§„É†\",\"ÁùÄÂ∑Æ\",\"‰∫∫ Ê∞ó\",\"ÂçòÂãù „Ç™„ÉÉ„Ç∫\",\"Âæå3F\",\"„Ç≥„Éº„Éä„Éº ÈÄöÈÅéÈ†Ü\",\"Âé©Ëàé\",\"È¶¨‰ΩìÈáç (Â¢óÊ∏õ)\",\"race_id\",\n",
    "            \"Ë∑ùÈõ¢\",\"„Ç≥„Éº„Çπ„Çø„Ç§„Éó\",\"Â§©ÂÄô\",\"È¶¨Â†¥Áä∂ÊÖã\",\"Âõû„Çä\"\n",
    "        ]\n",
    "        \n",
    "        for col in standard_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = \"\"\n",
    "                \n",
    "        df = df[standard_columns]\n",
    "        \n",
    "        print(f\"Scraped {len(df)} rows.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping JRA URL: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Parameter Map for Monthly Results (Reverse Engineered)\n",
    "JRA_MONTH_PARAMS = {\n",
    "    \"2026\": { \"01\": \"E4\", \"02\": \"B2\", \"03\": \"80\", \"04\": \"4E\", \"05\": \"1C\", \"06\": \"EA\", \"07\": \"B8\", \"08\": \"86\", \"09\": \"54\", \"10\": \"22\", \"11\": \"F0\", \"12\": \"BE\" },\n",
    "    \"2025\": { \"01\": \"3F\", \"02\": \"0D\", \"03\": \"DB\", \"04\": \"A9\", \"05\": \"77\", \"06\": \"45\", \"07\": \"13\", \"08\": \"E1\", \"09\": \"AF\", \"10\": \"1E\", \"11\": \"EC\", \"12\": \"D3\" },\n",
    "    \"2024\": { \"01\": \"B3\", \"02\": \"81\", \"03\": \"4F\", \"04\": \"1D\", \"05\": \"EB\", \"06\": \"B9\", \"07\": \"87\", \"08\": \"55\", \"09\": \"23\", \"10\": \"92\", \"11\": \"60\", \"12\": \"2E\" },\n",
    "    \"2023\": { \"01\": \"27\", \"02\": \"F5\", \"03\": \"C3\", \"04\": \"91\", \"05\": \"5F\", \"06\": \"2D\", \"07\": \"FB\", \"08\": \"C9\", \"09\": \"97\", \"10\": \"06\", \"11\": \"D4\", \"12\": \"A2\" },\n",
    "    \"2022\": { \"01\": \"9B\", \"02\": \"69\", \"03\": \"37\", \"04\": \"05\", \"05\": \"D3\", \"06\": \"A1\", \"07\": \"6F\", \"08\": \"3D\", \"09\": \"0B\", \"10\": \"7A\", \"11\": \"48\", \"12\": \"16\" },\n",
    "    \"2021\": { \"01\": \"0F\", \"02\": \"DD\", \"03\": \"AB\", \"04\": \"79\", \"05\": \"47\", \"06\": \"15\", \"07\": \"E3\", \"08\": \"B1\", \"09\": \"7F\", \"10\": \"EE\", \"11\": \"BC\", \"12\": \"8A\" },\n",
    "    \"2020\": { \"01\": \"83\", \"02\": \"51\", \"03\": \"1F\", \"04\": \"ED\", \"05\": \"BB\", \"06\": \"89\", \"07\": \"57\", \"08\": \"25\", \"09\": \"F3\", \"10\": \"62\", \"11\": \"30\", \"12\": \"FE\" }\n",
    "}\n",
    "\n",
    "def scrape_jra_year(year_str, start_date=None, end_date=None, save_callback=None, existing_race_ids=None):\n",
    "    \"\"\"\n",
    "    Scrapes races for a given year and date range.\n",
    "    year_str: \"2024\" or \"2025\"\n",
    "    start_date: datetime.date (optional)\n",
    "    end_date: datetime.date (optional)\n",
    "    save_callback: function(df) to save progress\n",
    "    \"\"\"\n",
    "    \n",
    "    if year_str not in JRA_MONTH_PARAMS:\n",
    "        print(f\"Year {year_str} not supported in parameter map.\")\n",
    "        return\n",
    "\n",
    "    params = JRA_MONTH_PARAMS[year_str]\n",
    "    base_url = \"https://www.jra.go.jp/JRADB/accessS.html\"\n",
    "    \n",
    "    # Determine months to iterate\n",
    "    start_m = 1\n",
    "    end_m = 12\n",
    "    \n",
    "    if start_date:\n",
    "        start_m = start_date.month\n",
    "    if end_date:\n",
    "        end_m = end_date.month\n",
    "    \n",
    "    # Cap at Today to prevent future scraping\n",
    "    from datetime import date\n",
    "    today = date.today()\n",
    "    \n",
    "    # If explicit end_date is used, respect it, but also respect today if it is earlier?\n",
    "    # Usually for results, we never want future.\n",
    "    if end_date:\n",
    "        actual_end_date = min(end_date, today)\n",
    "    else:\n",
    "        actual_end_date = today\n",
    "        \n",
    "    print(f\"=== Starting JRA Bulk Scraping for {year_str} (Period: {start_date or 'Start'} - {actual_end_date}) ===\")\n",
    "\n",
    "    # Adjust end_m based on today if we are in target year\n",
    "    if int(year_str) == today.year:\n",
    "        end_m = min(end_m, today.month)\n",
    "    elif int(year_str) > today.year:\n",
    "        print(f\"Year {year_str} is in the future. Stopping.\")\n",
    "        return\n",
    "    \n",
    "    for m in range(start_m, end_m + 1):\n",
    "        month = f\"{m:02}\"\n",
    "        if month not in params:\n",
    "            continue\n",
    "            \n",
    "        suffix = params[month]\n",
    "        # Logic for skl00 vs skl10\n",
    "        try:\n",
    "            ym = int(year_str + month)\n",
    "            prefix = \"pw01skl00\" if ym >= 202512 else \"pw01skl10\"\n",
    "        except:\n",
    "            prefix = \"pw01skl10\"\n",
    "\n",
    "        cname = f\"{prefix}{year_str}{month}/{suffix}\"\n",
    "        \n",
    "        print(f\"Fetching list for {year_str}/{month} (CNAME={cname})...\")\n",
    "        \n",
    "        try:\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" \n",
    "            }\n",
    "            response = requests.post(base_url, data={\"cname\": cname}, headers=headers, timeout=10)\n",
    "            response.encoding = 'cp932'\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch {cname} (Status {response.status_code})\")\n",
    "                continue\n",
    "                \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            race_cnames = []\n",
    "            links = soup.find_all('a')\n",
    "            for link in links:\n",
    "                onclick = link.get('onclick', '')\n",
    "                match = re.search(r\"doAction\\('[^']+',\\s*'([^']+)'\\)\", onclick)\n",
    "                if match:\n",
    "                    c = match.group(1)\n",
    "                    if c.startswith('pw01srl'):\n",
    "                        race_cnames.append(c)\n",
    "            \n",
    "            race_cnames = sorted(list(set(race_cnames)))\n",
    "            print(f\"  Found {len(race_cnames)} race days in month.\")\n",
    "            \n",
    "            for day_cname in race_cnames:\n",
    "                resp_day = requests.post(base_url, data={\"cname\": day_cname}, headers=headers, timeout=10)\n",
    "                resp_day.encoding = 'cp932'\n",
    "                soup_day = BeautifulSoup(resp_day.text, 'html.parser')\n",
    "                \n",
    "                # Check date of this day page\n",
    "                day_date_text = \"\"\n",
    "                d_h1 = soup_day.select_one(\"div.header_line h1 .txt\")\n",
    "                full_d_text = d_h1.text.strip() if d_h1 else (soup_day.h1.text.strip() if soup_day.h1 else \"\")\n",
    "                \n",
    "                # Parse date from \"2025Âπ¥1Êúà5Êó•ÔºàÊó•ÊõúÔºâ1Âõû‰∏≠Â±±1Êó•\"\n",
    "                # Need to match Date AND Venue/Kai/Day info for ID generation\n",
    "                # Pattern: YYYYÂπ¥MÊúàDÊó• ... KÂõûVenueDÊó•\n",
    "                \n",
    "                current_day_date = None\n",
    "                kai_str = \"01\"\n",
    "                day_str = \"01\"\n",
    "                venue_str = \"\"\n",
    "                p_code = \"00\"\n",
    "                \n",
    "                match_day_date = re.search(r'(\\d{4})Âπ¥(\\d{1,2})Êúà(\\d{1,2})Êó•', full_d_text)\n",
    "                if match_day_date:\n",
    "                    y, mo, d_day = map(int, match_day_date.groups())\n",
    "                    current_day_date = datetime(y, mo, d_day).date()\n",
    "                    \n",
    "                    # Filtering\n",
    "                    if start_date and current_day_date < start_date:\n",
    "                        print(f\"    Skipping day {current_day_date} (Before start date)\")\n",
    "                        continue\n",
    "                    if end_date and current_day_date > end_date:\n",
    "                        print(f\"    Skipping day {current_day_date} (After end date)\")\n",
    "                        continue\n",
    "                    print(f\"    Processing Day: {current_day_date} ({full_d_text})\")\n",
    "                else:\n",
    "                    print(f\"    Processing Day (Date unknown): {full_d_text[:20]}...\")\n",
    "\n",
    "                # Parse Venue Info for ID Generation\n",
    "                venues_ptn = \"Êú≠Âπå|ÂáΩÈ§®|Á¶èÂ≥∂|Êñ∞ÊΩü|Êù±‰∫¨|‰∏≠Â±±|‰∏≠‰∫¨|‰∫¨ÈÉΩ|Èò™Á•û|Â∞èÂÄâ\"\n",
    "                match_meta = re.search(rf'(\\d+)Âõû({venues_ptn})(\\d+)Êó•', full_d_text)\n",
    "                if match_meta:\n",
    "                    kai_str = f\"{int(match_meta.group(1)):02}\"\n",
    "                    venue_str = match_meta.group(2)\n",
    "                    day_str = f\"{int(match_meta.group(3)):02}\"\n",
    "                    \n",
    "                    place_map = {\n",
    "                        \"Êú≠Âπå\": \"01\", \"ÂáΩÈ§®\": \"02\", \"Á¶èÂ≥∂\": \"03\", \"Êñ∞ÊΩü\": \"04\", \"Êù±‰∫¨\": \"05\",\n",
    "                        \"‰∏≠Â±±\": \"06\", \"‰∏≠‰∫¨\": \"07\", \"‰∫¨ÈÉΩ\": \"08\", \"Èò™Á•û\": \"09\", \"Â∞èÂÄâ\": \"10\"\n",
    "                    }\n",
    "                    p_code = place_map.get(venue_str, \"00\")\n",
    "                \n",
    "                # Collect Race Links AND Race Numbers\n",
    "                # Need to pair Link with Race Number\n",
    "                race_list_items = []\n",
    "\n",
    "                all_anchors = soup_day.find_all('a')\n",
    "                for a in all_anchors:\n",
    "                    onclick = a.get('onclick', '')\n",
    "                    # Check for doAction with robust regex (handles single/double quotes, whitespace)\n",
    "                    # Pattern: doAction('FormName', 'CNAME')\n",
    "                    match_sde = re.search(r\"doAction\\s*\\(\\s*['\\\"][^'\\\"]+['\\\"]\\s*,\\s*['\\\"](pw01sde[^'\\\"]+)['\\\"]\\s*\\)\", onclick)\n",
    "                    href = a.get('href', '')\n",
    "                    \n",
    "                    final_url = \"\"\n",
    "                    if match_sde:\n",
    "                        final_url = f\"{base_url}?CNAME={match_sde.group(1)}\"\n",
    "                    elif 'pw01sde' in href:\n",
    "                        # Fallback for simple hrefs\n",
    "                        if 'CNAME=' in href:\n",
    "                             final_url = urllib.parse.urljoin(base_url, href)\n",
    "                        else:\n",
    "                             # If href=\"accessS.html?CNAME=...\"\n",
    "                             # or just \"?CNAME=...\"\n",
    "                             final_url = urllib.parse.urljoin(base_url, href)\n",
    "                    \n",
    "                    if final_url:\n",
    "                        # Extract Race Number from anchor text (e.g. \"1R\", \"11R\")\n",
    "                        # Or generic image alt?\n",
    "                        # Usually text is \"1R\" or img alt=\"1R\"\n",
    "                        txt = a.text.strip()\n",
    "                        img = a.find('img')\n",
    "                        if not txt and img and 'alt' in img.attrs:\n",
    "                            txt = img['alt']\n",
    "                        \n",
    "                        r_num = -1\n",
    "                        r_num_match = re.search(r'(\\d+)R', txt)\n",
    "                        if r_num_match:\n",
    "                             r_num = int(r_num_match.group(1))\n",
    "                             \n",
    "                        # Append even if Race Num is not found (fix for missing races)\n",
    "                        race_list_items.append((final_url, r_num))\n",
    "                \n",
    "                # Deduplicate by URL (keep first found usually fine)\n",
    "                # Sort by Race Number (unknowns (-1) first or last?)\n",
    "                seen_urls = set()\n",
    "                unique_races = []\n",
    "                for url, r_num in race_list_items:\n",
    "                    if url not in seen_urls:\n",
    "                        unique_races.append((url, r_num))\n",
    "                        seen_urls.add(url)\n",
    "                \n",
    "                unique_races.sort(key=lambda x: x[1]) # Sort by race num (-1 will be first)\n",
    "                \n",
    "                print(f\"      -> {len(unique_races)} races found.\")\n",
    "\n",
    "                for r_link, r_num in unique_races:\n",
    "                    # PRE-FETCH OPTIMIZATION\n",
    "                    # Construct ID\n",
    "                    # Only if we successfully extracted Race Num and Venue info\n",
    "                    if r_num != -1 and p_code != \"00\" and current_day_date:\n",
    "                         # ID: YYYY PP KK DD RR\n",
    "                         # y is from match_day_date loop var (int)\n",
    "                         # p_code, kai_str, day_str strings\n",
    "                         \n",
    "                         # Ensure year is from the day page date\n",
    "                         y_str = str(y)\n",
    "                         r_num_str = f\"{r_num:02}\"\n",
    "                         \n",
    "                         generated_id = f\"{y_str}{p_code}{kai_str}{day_str}{r_num_str}\"\n",
    "                         \n",
    "                         if existing_race_ids and generated_id in existing_race_ids:\n",
    "                             # print(f\"        [Skip] {generated_id} (Pre-check)\")\n",
    "                             continue\n",
    "                    \n",
    "                    # If not skipped, fetch\n",
    "                    df = scrape_jra_race(r_link, existing_race_ids=existing_race_ids)\n",
    "                    \n",
    "                    if df is not None and not df.empty:\n",
    "                        if save_callback:\n",
    "                            save_callback(df)\n",
    "                        time.sleep(1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing month {month}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution Block\n",
    "import os\n",
    "from datetime import date\n",
    "import calendar\n",
    "\n",
    "year = input('Enter Year (e.g. 2024): ')\n",
    "start_month = input('Enter Start Month (1-12, default 1): ') or '1'\n",
    "end_month = input('Enter End Month (1-12, default 12): ') or '12'\n",
    "\n",
    "if year:\n",
    "    s_date = date(int(year), int(start_month), 1)\n",
    "    last_day = calendar.monthrange(int(year), int(end_month))[1]\n",
    "    e_date = date(int(year), int(end_month), last_day)\n",
    "    \n",
    "    print(f'Scraping {year} from {s_date} to {e_date}...')\n",
    "    scrape_jra_year(year, start_date=s_date, end_date=e_date, save_callback=lambda df: df.to_csv('data/raw/database.csv', mode='a', header=not os.path.exists('data/raw/database.csv'), index=False))\n",
    "    print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}