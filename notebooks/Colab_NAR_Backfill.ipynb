{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è NAR „Éá„Éº„ÇøË£úÂÆå„ÉÑ„Éº„É´\n",
    "Ê¨†Êêç„Åó„Å¶„ÅÑ„ÇãË°ÄÁµ±ÊÉÖÂ†±„Åä„Çà„Å≥ÈÅéÂéªËµ∞Â±•Ê≠¥„ÇíË£úÂÆå„Åó„Åæ„Åô„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive„Çí„Éû„Ç¶„É≥„Éà„Åô„ÇãÂ†¥Âêà„ÅÆ„ÅøÂÆüË°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class RaceScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "    def _get_soup(self, url):\n",
    "        try:\n",
    "            time.sleep(1) # Be polite\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.encoding = response.apparent_encoding\n",
    "            if response.status_code == 200:\n",
    "                return BeautifulSoup(response.text, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def get_past_races(self, horse_id, target_date=None, n_samples=5):\n",
    "        \"\"\"\n",
    "        Fetches past n_samples race results for a given horse_id from netkeiba db.\n",
    "        If target_date is provided, filters for races STRICTLY BEFORE that date.\n",
    "        Returns a DataFrame of past races.\n",
    "        \"\"\"\n",
    "        url = f\"https://db.netkeiba.com/horse/result/{horse_id}/\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # The results are usually in a table with class \"db_h_race_results\"\n",
    "        table = soup.select_one(\"table.db_h_race_results\")\n",
    "        if not table:\n",
    "            # Try to find any table with \"ÁùÄÈ†Ü\"\n",
    "            tables = soup.find_all(\"table\")\n",
    "            for t in tables:\n",
    "                if \"ÁùÄÈ†Ü\" in t.text:\n",
    "                    table = t\n",
    "                    break\n",
    "        \n",
    "        if not table:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Parse table\n",
    "        try:\n",
    "            df = pd.read_html(io.StringIO(str(table)))[0]\n",
    "            \n",
    "            # Basic cleaning\n",
    "            df = df.dropna(how='all')\n",
    "            \n",
    "            # Normalize column names (remove spaces/newlines)\n",
    "            df.columns = df.columns.astype(str).str.replace(r'\\s+', '', regex=True)\n",
    "\n",
    "            # Filter rows that look like actual races (Date column exists)\n",
    "            if 'Êó•‰ªò' in df.columns:\n",
    "                df['date_obj'] = pd.to_datetime(df['Êó•‰ªò'], format='%Y/%m/%d', errors='coerce')\n",
    "                df = df.dropna(subset=['date_obj'])\n",
    "                \n",
    "                # Leakage Prevention: Filter races strictly before target_date\n",
    "                if target_date:\n",
    "                    if isinstance(target_date, str):\n",
    "                        target_dt = pd.to_datetime(target_date, errors='coerce')\n",
    "                    else:\n",
    "                        target_dt = pd.to_datetime(target_date) # handle date/datetime\n",
    "                        \n",
    "                    if target_dt is not None:\n",
    "                         # Use strictly less than (<) to exclude future and current race (if in DB)\n",
    "                         df = df[df['date_obj'] < target_dt]\n",
    "\n",
    "                df = df.sort_values('date_obj', ascending=False)\n",
    "                \n",
    "            # Take top N\n",
    "            if n_samples:\n",
    "                df = df.head(n_samples)\n",
    "            \n",
    "            # Process Run Style (Leg Type)\n",
    "            if 'ÈÄöÈÅé' in df.columns:\n",
    "                df['run_style_val'] = df['ÈÄöÈÅé'].apply(self.extract_run_style)\n",
    "            else:\n",
    "                df['run_style_val'] = 3 # Unknown\n",
    "\n",
    "            # Extract/Rename Columns\n",
    "            # We want: Êó•‰ªò, ÈñãÂÇ¨, Â§©Ê∞ó, R, „É¨„Éº„ÇπÂêç, Êò†ÂÉè, È†≠Êï∞, Êû†Áï™, ... ÁùÄÈ†Ü, ... ÈÄöÈÅé, ...\n",
    "            # Important: '‰∏ä„Çä' (3F), 'È¶¨‰ΩìÈáç', 'È®éÊâã'\n",
    "            \n",
    "            # Map standard columns if they exist\n",
    "            column_map = {\n",
    "                'Êó•‰ªò': 'date',\n",
    "                'ÈñãÂÇ¨': 'venue',\n",
    "                'Â§©Ê∞ó': 'weather',\n",
    "                '„É¨„Éº„ÇπÂêç': 'race_name',\n",
    "                'ÁùÄÈ†Ü': 'rank',\n",
    "                'Êû†Áï™': 'waku',\n",
    "                'È¶¨Áï™': 'umaban',\n",
    "                'È®éÊâã': 'jockey',\n",
    "                'Êñ§Èáè': 'weight_carried',\n",
    "                'È¶¨Â†¥': 'condition', # ËâØ/Èáç/Á®çÈáç etc.\n",
    "                '„Çø„Ç§„É†': 'time',\n",
    "                'ÁùÄÂ∑Æ': 'margin',\n",
    "                '‰∏ä„Çä': 'last_3f',\n",
    "                'ÈÄöÈÅé': 'passing',\n",
    "                'È¶¨‰ΩìÈáç': 'horse_weight',\n",
    "                'run_style_val': 'run_style',\n",
    "                'ÂçòÂãù': 'odds',\n",
    "                '„Ç™„ÉÉ„Ç∫': 'odds',\n",
    "                'Ë∑ùÈõ¢': 'raw_distance' # e.g. \"Ëäù1600\"\n",
    "            }\n",
    "            \n",
    "            # Rename available columns\n",
    "            df.rename(columns=column_map, inplace=True)\n",
    "            \n",
    "            # Extract Surface and Distance from 'raw_distance'\n",
    "            if 'raw_distance' in df.columns:\n",
    "                def parse_dist(x):\n",
    "                    if not isinstance(x, str): return None, None\n",
    "                    # \"Ëäù1600\", \"„ÉÄ1200\", \"Èöú3000\"\n",
    "                    # Sometimes \"Ëäù1600\" or just \"1600\"\n",
    "                    surf = None\n",
    "                    dist = None\n",
    "                    if 'Ëäù' in x: surf = 'Ëäù'\n",
    "                    elif '„ÉÄ' in x: surf = '„ÉÄ'\n",
    "                    elif 'Èöú' in x: surf = 'Èöú'\n",
    "                    \n",
    "                    # Extract number\n",
    "                    match = re.search(r'(\\d+)', x)\n",
    "                    if match:\n",
    "                        dist = int(match.group(1))\n",
    "                    return surf, dist\n",
    "\n",
    "                parsed = df['raw_distance'].apply(parse_dist)\n",
    "                df['course_type'] = parsed.apply(lambda x: x[0])\n",
    "                df['distance'] = parsed.apply(lambda x: x[1])\n",
    "            else:\n",
    "                df['course_type'] = None\n",
    "                df['distance'] = None\n",
    "\n",
    "            # Coerce numeric\n",
    "            if 'rank' in df.columns:\n",
    "                df['rank'] = pd.to_numeric(df['rank'], errors='coerce')\n",
    "            \n",
    "            if 'odds' in df.columns:\n",
    "                 df['odds'] = pd.to_numeric(df['odds'], errors='coerce')\n",
    "            \n",
    "            # Fill missing\n",
    "            for target_col in list(column_map.values()) + ['course_type', 'distance']:\n",
    "                if target_col not in df.columns:\n",
    "                    df[target_col] = None\n",
    "                \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing past races for {horse_id}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def extract_run_style(self, passing_str):\n",
    "        \"\"\"\n",
    "        Converts passing order string (e.g., \"1-1-1\", \"10-10-12\") to run style (1,2,3,4).\n",
    "        1: Nige (Escape) - Lead at 1st corner\n",
    "        2: Senkou (Leader) - Within first ~4 or so\n",
    "        3: Sashi (Mid) - Midpack\n",
    "        4: Oikomi (Chaser) - Back\n",
    "        Returns integer code.\n",
    "        \"\"\"\n",
    "        if not isinstance(passing_str, str):\n",
    "            return 3 # Default to Mid\n",
    "            \n",
    "        # Clean string \"1-1-1\" -> [1, 1, 1]\n",
    "        try:\n",
    "            cleaned = re.sub(r'[^0-9-]', '', passing_str)\n",
    "            parts = [int(p) for p in cleaned.split('-') if p]\n",
    "            \n",
    "            if not parts:\n",
    "                return 3\n",
    "                \n",
    "            first_corner = parts[0]\n",
    "            \n",
    "            # Heuristics\n",
    "            if first_corner == 1:\n",
    "                return 1 # Nige\n",
    "            elif first_corner <= 4:\n",
    "                return 2 # Senkou\n",
    "            elif first_corner <= 9: # Assuming standard field size of 10-16, 9 is mid-ish limit? \n",
    "                # Actually \"Sashi\" is usually mid-rear. \n",
    "                # Let's say: 1=Lead, 2-4=Front, 5-10=Mid, >10=Back\n",
    "                return 3 # Sashi\n",
    "            else:\n",
    "                return 4 # Oikomi\n",
    "                \n",
    "        except:\n",
    "            return 3\n",
    "\n",
    "    def scrape_race_with_history(self, race_id):\n",
    "        \"\"\"\n",
    "        Detailed scraper that enters a race_result page, finding horse IDs, \n",
    "        then fetches history for each horse.\n",
    "        Returns a dictionary or structured object with the race result + history.\n",
    "        \"\"\"\n",
    "        url = f\"https://race.netkeiba.com/race/result.html?race_id={race_id}\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "            \n",
    "        # 0. Extract Race Date for Leakage Prevention\n",
    "        race_date = None\n",
    "        try:\n",
    "            # Try to find date in .RaceData01 or similar\n",
    "            # Example text: \"10:10ÊõáËâØ2021Âπ¥1Êúà5Êó•...\"\n",
    "            # Netkeiba often puts date in the title tag too like \"2021Âπ¥1Êúà5Êó•...\"\n",
    "            \n",
    "            # Strategy 1: Title\n",
    "            if soup.title:\n",
    "                title_text = soup.title.text\n",
    "                match = re.search(r'(\\d{4})Âπ¥(\\d{1,2})Êúà(\\d{1,2})Êó•', title_text)\n",
    "                if match:\n",
    "                    y, m, d = match.groups()\n",
    "                    race_date = datetime(int(y), int(m), int(d))\n",
    "            \n",
    "            # Strategy 2: .RaceData01 (Common in Result page)\n",
    "            if not race_date:\n",
    "                rd1 = soup.find(\"div\", class_=\"RaceData01\")\n",
    "                if rd1:\n",
    "                    match = re.search(r'(\\d{4})Âπ¥(\\d{1,2})Êúà(\\d{1,2})Êó•', rd1.text)\n",
    "                    if match:\n",
    "                        y, m, d = match.groups()\n",
    "                        race_date = datetime(int(y), int(m), int(d))\n",
    "            \n",
    "            # Strategy 3: URL (kaisai_date=YYYYMMDD) - Though URL input is race_id, result page might link to kaisai\n",
    "            if not race_date:\n",
    "                # Some list links contain kaisai_date, but here we only have race_id\n",
    "                pass\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not extract race date: {e}\")\n",
    "\n",
    "        # 1. Parse Main Result Table\n",
    "        result_data = []\n",
    "        table = soup.find(\"table\", id=\"All_Result_Table\")\n",
    "        if not table:\n",
    "            return None\n",
    "            \n",
    "        rows = table.find_all(\"tr\", class_=\"HorseList\")\n",
    "        print(f\"Found {len(rows)} horses in race {race_id} ({race_date.date() if race_date else 'Unknown Date'}). Fetching histories...\")\n",
    "        \n",
    "        for row in rows:\n",
    "            # Extract basic info\n",
    "            rank_elem = row.select_one(\".Rank\")\n",
    "            rank = rank_elem.text.strip() if rank_elem else \"\"\n",
    "            \n",
    "            horse_name_elem = row.select_one(\".Horse_Name a\")\n",
    "            horse_name = horse_name_elem.text.strip() if horse_name_elem else \"\"\n",
    "            horse_url = horse_name_elem.get(\"href\") if horse_name_elem else \"\"\n",
    "            \n",
    "            # Extract ID from URL\n",
    "            horse_id = None\n",
    "            if horse_url:\n",
    "                match = re.search(r'/horse/(\\d+)', horse_url)\n",
    "                if match:\n",
    "                    horse_id = match.group(1)\n",
    "            \n",
    "            if not horse_id:\n",
    "                print(f\"  Skipping {horse_name} (No ID)\")\n",
    "                continue\n",
    "\n",
    "            # print(f\"  Fetching history for {horse_name} ({horse_id})...\")\n",
    "            \n",
    "            # 2. Get Past History (with Leakage Prevention)\n",
    "            df_past = self.get_past_races(horse_id, target_date=race_date, n_samples=5)\n",
    "            \n",
    "            # 3. Structure Data\n",
    "            # converting df_past to a list of dicts or flattened fields\n",
    "            history = []\n",
    "            if not df_past.empty:\n",
    "                for idx, r in df_past.iterrows():\n",
    "                    # Extract relevant columns\n",
    "                    # We need at least: Rank, RunStyle, Time(Seconds?), Pace?\n",
    "                    # For now just dump raw-ish data\n",
    "                    hist_item = {\n",
    "                        \"date\": r.get('Êó•‰ªò'),\n",
    "                        \"race_name\": r.get('„É¨„Éº„ÇπÂêç'),\n",
    "                        \"rank\": r.get('ÁùÄÈ†Ü'),\n",
    "                        \"passing\": r.get('ÈÄöÈÅé'),\n",
    "                        \"run_style\": r.get('run_style_val'),\n",
    "                        \"time\": r.get('„Çø„Ç§„É†'),\n",
    "                        # Add more as needed for Feature Engineering\n",
    "                    }\n",
    "                    history.append(hist_item)\n",
    "            \n",
    "            entry = {\n",
    "                \"race_id\": race_id,\n",
    "                \"horse_id\": horse_id,\n",
    "                \"horse_name\": horse_name,\n",
    "                \"rank\": rank,\n",
    "                \"history\": history\n",
    "            }\n",
    "            result_data.append(entry)\n",
    "            \n",
    "        return result_data\n",
    "\n",
    "    def get_horse_profile(self, horse_id):\n",
    "        \"\"\"\n",
    "        Fetches horse profile to get pedigree (Father, Mother, Grandfather(BMS)).\n",
    "        Returns a dictionary or None.\n",
    "        \"\"\"\n",
    "        # Use pedigree page for reliable bloodline data\n",
    "        url = f\"https://db.netkeiba.com/horse/ped/{horse_id}/\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        # Parse Blood Table\n",
    "        # table class=\"blood_table\"\n",
    "        \n",
    "        data = {\n",
    "            \"father\": \"\",\n",
    "            \"mother\": \"\",\n",
    "            \"bms\": \"\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            table = soup.select_one(\"table.blood_table\")\n",
    "            if table:\n",
    "                rows = table.find_all(\"tr\")\n",
    "                # 5-generation table has 32 rows usually\n",
    "                # Father at Row 0 (rowspan 16)\n",
    "                # Mother at Row 16 (rowspan 16)\n",
    "                \n",
    "                if len(rows) >= 17:\n",
    "                    # Father: Row 0, Col 0\n",
    "                    r0 = rows[0].find_all(\"td\")\n",
    "                    if r0:\n",
    "                        txt = r0[0].text.strip()\n",
    "                        # Clean: \"„Çπ„ÇØ„É™„Éº„É≥„Éí„Éº„É≠„Éº\\n2004 Ê†óÊØõ...\" -> \"„Çπ„ÇØ„É™„Éº„É≥„Éí„Éº„É≠„Éº\"\n",
    "                        # Take first line\n",
    "                        data[\"father\"] = txt.split('\\n')[0].strip()\n",
    "                        \n",
    "                    # Mother & BMS: Row 16\n",
    "                    r16 = rows[16].find_all(\"td\")\n",
    "                    if len(r16) >= 2:\n",
    "                        # Mother\n",
    "                        m_txt = r16[0].text.strip()\n",
    "                        data[\"mother\"] = m_txt.split('\\n')[0].strip()\n",
    "                        \n",
    "                        # BMS (Mother's Father)\n",
    "                        bms_txt = r16[1].text.strip()\n",
    "                        data[\"bms\"] = bms_txt.split('\\n')[0].strip()\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing profile for {horse_id}: {e}\")\n",
    "            \n",
    "        return data\n",
    "\n",
    "    def get_race_metadata(self, race_id):\n",
    "        \"\"\"\n",
    "        Fetches metadata for a specific race ID from Netkeiba.\n",
    "        Returns dict with: race_name, date, venue, course_type, distance, weather, condition, turn\n",
    "        \"\"\"\n",
    "        url = f\"https://race.netkeiba.com/race/result.html?race_id={race_id}\"\n",
    "        soup = self._get_soup(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "            \n",
    "        data = {\n",
    "            \"race_name\": \"\",\n",
    "            \"date\": \"\",\n",
    "            \"venue\": \"\",\n",
    "            \"course_type\": \"\",\n",
    "            \"distance\": \"\",\n",
    "            \"weather\": \"\",\n",
    "            \"condition\": \"\",\n",
    "            \"turn\": \"\", # New: Dictionary key for turn direction\n",
    "            \"race_id\": race_id\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Race Name\n",
    "            title_elem = soup.select_one(\".RaceName\")\n",
    "            if title_elem:\n",
    "                data[\"race_name\"] = title_elem.text.strip()\n",
    "                \n",
    "            # Date & Venue & Conditions\n",
    "            # <div class=\"RaceData01\">... 2023Âπ¥1Êúà5Êó• ... 1Âõû‰∏≠Â±±1Êó• ...</div>\n",
    "            # Content: \"15:35Áô∫Ëµ∞ / Ëäù1600m (Âè≥ Â§ñ) / Â§©ÂÄô:Êô¥ / È¶¨Â†¥:ËâØ\"\n",
    "            \n",
    "            rd1 = soup.select_one(\".RaceData01\")\n",
    "            \n",
    "            if rd1:\n",
    "                txt = rd1.text.strip()\n",
    "                \n",
    "                # Weather\n",
    "                if \"Â§©ÂÄô:Êô¥\" in txt: data[\"weather\"] = \"Êô¥\"\n",
    "                elif \"Â§©ÂÄô:Êõá\" in txt: data[\"weather\"] = \"Êõá\"\n",
    "                elif \"Â§©ÂÄô:Â∞èÈõ®\" in txt: data[\"weather\"] = \"Â∞èÈõ®\"\n",
    "                elif \"Â§©ÂÄô:Èõ®\" in txt: data[\"weather\"] = \"Èõ®\"\n",
    "                elif \"Â§©ÂÄô:Èõ™\" in txt: data[\"weather\"] = \"Èõ™\"\n",
    "                \n",
    "                # Condition\n",
    "                if \"È¶¨Â†¥:ËâØ\" in txt: data[\"condition\"] = \"ËâØ\"\n",
    "                elif \"È¶¨Â†¥:Á®ç\" in txt: data[\"condition\"] = \"Á®çÈáç\" # Covers Á®çÈáç\n",
    "                elif \"È¶¨Â†¥:Èáç\" in txt: data[\"condition\"] = \"Èáç\"\n",
    "                elif \"È¶¨Â†¥:‰∏çËâØ\" in txt: data[\"condition\"] = \"‰∏çËâØ\"\n",
    "                \n",
    "                # Course & Distance (\"Ëäù1600m\")\n",
    "                # Regex for \"Ëäù\", \"„ÉÄ\", \"Èöú\" followed by digits\n",
    "                match = re.search(r'(Ëäù|„ÉÄ|Èöú)(\\d+)m', txt)\n",
    "                if match:\n",
    "                    ctype_raw = match.group(1)\n",
    "                    if ctype_raw == \"Ëäù\": data[\"course_type\"] = \"Ëäù\"\n",
    "                    elif ctype_raw == \"„ÉÄ\": data[\"course_type\"] = \"„ÉÄ„Éº„Éà\"\n",
    "                    elif ctype_raw == \"Èöú\": data[\"course_type\"] = \"ÈöúÂÆ≥\"\n",
    "                    \n",
    "                    data[\"distance\"] = match.group(2)\n",
    "                \n",
    "                # Turn Direction (\"Âè≥\", \"Â∑¶\", \"Áõ¥Á∑ö\")\n",
    "                # Usually in parentheses like \"(Âè≥)\" or \"(Â∑¶)\" or \"(Ëäù Â∑¶)\"\n",
    "                if \"Âè≥\" in txt: data[\"turn\"] = \"Âè≥\"\n",
    "                elif \"Â∑¶\" in txt: data[\"turn\"] = \"Â∑¶\"\n",
    "                elif \"Áõ¥Á∑ö\" in txt: data[\"turn\"] = \"Áõ¥\"\n",
    "\n",
    "            # Date\n",
    "            # Try finding date in Title or dedicated element\n",
    "            date_elem = soup.select_one(\"dl#RaceList_DateList dd.Active\") \n",
    "            if date_elem:\n",
    "                 # Usually \"1Êúà5Êó•(Èáë)\" - needs Year\n",
    "                 # We can rely on the fact that race_id contains year (2025...)\n",
    "                 # But let's look for YYYYÂπ¥ in the whole text or title\n",
    "                 pass\n",
    "            \n",
    "            # Fallback Date from Title Tag or Meta\n",
    "            if not data[\"date\"]:\n",
    "                 meta_title = soup.title.text if soup.title else \"\"\n",
    "                 match_date = re.search(r'(\\d{4}Âπ¥\\d{1,2}Êúà\\d{1,2}Êó•)', meta_title)\n",
    "                 if match_date:\n",
    "                     data[\"date\"] = match_date.group(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing metadata for {race_id}: {e}\")\n",
    "            \n",
    "        return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test\n",
    "    scraper = RaceScraper()\n",
    "    print(\"Running test...\")\n",
    "    # Example: Do Deuce (2019105219)\n",
    "    # url = \"https://db.netkeiba.com/horse/2019105219/\"\n",
    "    # print(f\"Fetching {url}\")\n",
    "    df = scraper.get_past_races(\"2019105219\")\n",
    "    if df.empty:\n",
    "        print(\"DF is empty. Checking raw soup for 'db_h_race_results'...\")\n",
    "        soup = scraper._get_soup(f\"https://db.netkeiba.com/horse/result/2019105219/\")\n",
    "        if soup:\n",
    "             t = soup.select_one(\"table.db_h_race_results\")\n",
    "             print(f\"Selector 'table.db_h_race_results' found: {t is not None}\")\n",
    "             if not t:\n",
    "                 print(\"Trying fallback 'table' with 'ÁùÄÈ†Ü'...\")\n",
    "                 tables = soup.find_all(\"table\")\n",
    "                 found = False\n",
    "                 for i, tbl in enumerate(tables):\n",
    "                     print(f\"Table {i} classes: {tbl.get('class')}\")\n",
    "                     if \"ÁùÄÈ†Ü\" in tbl.text or \"ÁùÄ È†Ü\" in tbl.text or \"Êó•‰ªò\" in tbl.text:\n",
    "                         print(\"Found a table with 'ÁùÄÈ†Ü/Êó•‰ªò'.\")\n",
    "                         # print(str(tbl)[:200])\n",
    "                         t = tbl\n",
    "                         found = True\n",
    "                         break\n",
    "                 if not found:\n",
    "                     print(\"No table with 'ÁùÄÈ†Ü' found in soup.\")\n",
    "                     print(\"Soup snippet:\", soup.text[:500])\n",
    "                 else:\n",
    "                    # Retry parsing with found table\n",
    "                     try:\n",
    "                        df = pd.read_html(str(t))[0]\n",
    "                        print(\"Retry DF Head:\")\n",
    "                        print(df.head())\n",
    "                     except Exception as e:\n",
    "                        print(f\"Retry parsing failed: {e}\")\n",
    "        else:\n",
    "            print(\"Soup is None.\")\n",
    "    else:\n",
    "        print(df.head())\n",
    "        print(\"Columns:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add project root to path to ensure scraper imports work\n",
    "# Assumes this script is in 'scripts/' and 'scraper/' is in root\n",
    "\n",
    "try:\n",
    "    pass # Replaced import\n",
    "except ImportError:\n",
    "    print(\"‚ùå Could not import RaceScraper. Make sure you are running this from the repository root or 'ids' folder structure is correct.\")\n",
    "    # Fallback/Mock for testing if needed, or exit\n",
    "    sys.exit(1)\n",
    "\n",
    "def fill_bloodline_data(df_path, mode=\"JRA\"):\n",
    "    \"\"\"\n",
    "    Backfills missing bloodline data (father, mother, bms).\n",
    "    \"\"\"\n",
    "    print(f\"\\nüê¥ Starting Bloodline Backfill for {mode} ({os.path.basename(df_path)})\")\n",
    "    \n",
    "    if not os.path.exists(df_path):\n",
    "        print(f\"‚ùå File not found: {df_path}\")\n",
    "        return\n",
    "\n",
    "    # Load Data\n",
    "    try:\n",
    "        if df_path.endswith('.parquet'):\n",
    "            df = pd.read_parquet(df_path)\n",
    "        else:\n",
    "            df = pd.read_csv(df_path, low_memory=False)\n",
    "            # Ensure IDs are strings\n",
    "            if 'horse_id' in df.columns:\n",
    "                df['horse_id'] = df['horse_id'].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
    "            if 'race_id' in df.columns:\n",
    "                df['race_id'] = df['race_id'].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Ensure Columns Exist\n",
    "    for col in ['father', 'mother', 'bms']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Identify Missing\n",
    "    # Criteria: 'father' is null/empty AND 'horse_id' is valid\n",
    "    mask_missing = (df['father'].isna()) | (df['father'] == '') | (df['father'] == 'nan')\n",
    "    \n",
    "    if 'horse_id' not in df.columns:\n",
    "        print(\"‚ùå 'horse_id' column missing.\")\n",
    "        return\n",
    "\n",
    "    target_ids = df.loc[mask_missing, 'horse_id'].dropna().unique()\n",
    "    target_ids = [hid for hid in target_ids if str(hid).isdigit()] # Filter valid IDs\n",
    "    \n",
    "    total_targets = len(target_ids)\n",
    "    print(f\"üéØ Found {total_targets} horses with missing bloodline data.\")\n",
    "    \n",
    "    if total_targets == 0:\n",
    "        print(\"‚úÖ No missing bloodline data found.\")\n",
    "        return\n",
    "\n",
    "    # Scraper Setup\n",
    "    scraper = RaceScraper()\n",
    "    \n",
    "    # Worker Function\n",
    "    def fetch_pedigree(hid):\n",
    "        # random sleep to avoid rate limiting\n",
    "        time.sleep(0.1) \n",
    "        try:\n",
    "            return (hid, scraper.get_horse_profile(hid))\n",
    "        except Exception as e:\n",
    "            return (hid, None)\n",
    "\n",
    "    # Sequential Execution (No Parallel)\n",
    "    print(f\"üöÄ Fetching data for {total_targets} horses (Sequential)...\")\n",
    "    \n",
    "    # Chunking to save progress\n",
    "    CHUNK_SIZE = 1000\n",
    "    \n",
    "    results = {}\n",
    "    for i in range(0, total_targets, CHUNK_SIZE):\n",
    "        chunk = target_ids[i:i+CHUNK_SIZE]\n",
    "        print(f\"  Processing chunk {i}-{i+len(chunk)}...\")\n",
    "        \n",
    "        # Sequential Loop\n",
    "        for hid in tqdm(chunk, leave=False):\n",
    "            try:\n",
    "                # Random sleep to be gentle\n",
    "                time.sleep(0.5) \n",
    "                data = scraper.get_horse_profile(hid)\n",
    "                if data:\n",
    "                    results[hid] = data\n",
    "            except Exception as e:\n",
    "                # print(f\"Error fetching {hid}: {e}\")\n",
    "                pass\n",
    "        \n",
    "        # Apply Logic\n",
    "        if len(results) > 0:\n",
    "            print(\"  Applying updates to DataFrame...\")\n",
    "            # Create Maps\n",
    "            f_map = {h: d.get('father') for h, d in results.items() if d}\n",
    "            m_map = {h: d.get('mother') for h, d in results.items() if d}\n",
    "            b_map = {h: d.get('bms') for h, d in results.items() if d}\n",
    "            \n",
    "            # Update only rows that match these IDs\n",
    "            mask_chunk = df['horse_id'].isin(results.keys())\n",
    "            \n",
    "            # Efficient Map Update\n",
    "            df.loc[mask_chunk, 'father'] = df.loc[mask_chunk, 'horse_id'].map(f_map).fillna(df.loc[mask_chunk, 'father'])\n",
    "            df.loc[mask_chunk, 'mother'] = df.loc[mask_chunk, 'horse_id'].map(m_map).fillna(df.loc[mask_chunk, 'mother'])\n",
    "            df.loc[mask_chunk, 'bms'] = df.loc[mask_chunk, 'horse_id'].map(b_map).fillna(df.loc[mask_chunk, 'bms'])\n",
    "            \n",
    "            # Clear results buffer\n",
    "            results = {}\n",
    "            \n",
    "            # Save\n",
    "            print(f\"  üíæ Saving progress to {df_path}...\")\n",
    "            if df_path.endswith('.parquet'):\n",
    "                df.to_parquet(df_path, index=False)\n",
    "            else:\n",
    "                df.to_csv(df_path, index=False)\n",
    "\n",
    "    print(\"‚úÖ Bloodline backfill complete.\")\n",
    "\n",
    "\n",
    "def fill_history_data(df_path, mode=\"JRA\"):\n",
    "    \"\"\"\n",
    "    Backfills missing past race history (past_1_date, etc.).\n",
    "    Target: Rows where 'past_1_date' is NaN AND race is NOT 'Shinba' (Debut).\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìú Starting History Backfill for {mode} ({os.path.basename(df_path)})\")\n",
    "    \n",
    "    if not os.path.exists(df_path):\n",
    "        print(f\"‚ùå File not found: {df_path}\")\n",
    "        return\n",
    "\n",
    "    # Load Data\n",
    "    try:\n",
    "        if df_path.endswith('.parquet'):\n",
    "            df = pd.read_parquet(df_path)\n",
    "        else:\n",
    "            df = pd.read_csv(df_path, low_memory=False)\n",
    "            if 'horse_id' in df.columns:\n",
    "                df['horse_id'] = df['horse_id'].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Filter Targets\n",
    "    # 1. Not Shinba\n",
    "    if '„É¨„Éº„ÇπÂêç' in df.columns:\n",
    "        mask_shinba = df['„É¨„Éº„ÇπÂêç'].astype(str).str.contains('Êñ∞È¶¨|„É°„Ç§„ÇØ„Éá„Éì„É•„Éº', na=False)\n",
    "    else:\n",
    "        mask_shinba = False\n",
    "        \n",
    "    # 2. Missing History\n",
    "    mask_missing = df['past_1_date'].isna() & (~mask_shinba)\n",
    "    \n",
    "    target_rows = df[mask_missing]\n",
    "    target_ids = target_rows['horse_id'].unique()\n",
    "    target_ids = [hid for hid in target_ids if str(hid).isdigit()]\n",
    "    \n",
    "    total_targets = len(target_ids)\n",
    "    print(f\"üéØ Found {len(target_rows)} rows ({total_targets} unique horses) missing history.\")\n",
    "    \n",
    "    if total_targets == 0:\n",
    "        print(\"‚úÖ No missing history found.\")\n",
    "        return\n",
    "\n",
    "    scraper = RaceScraper()\n",
    "    history_cache = {}\n",
    "\n",
    "    # Worker for simple fetch\n",
    "    def fetch_history(hid):\n",
    "        time.sleep(0.1)\n",
    "        try:\n",
    "            return (hid, scraper.get_past_races(hid, n_samples=None))\n",
    "        except:\n",
    "            return (hid, None)\n",
    "            \n",
    "    # Process\n",
    "    print(f\"üöÄ Fetching history for {total_targets} horses (Sequential)...\")\n",
    "    \n",
    "    CHUNK_SIZE = 500\n",
    "    for i in range(0, total_targets, CHUNK_SIZE):\n",
    "        chunk_ids = target_ids[i:i+CHUNK_SIZE]\n",
    "        \n",
    "        # Sequential Loop\n",
    "        for hid in tqdm(chunk_ids, leave=False, desc=f\"Chunk {i//CHUNK_SIZE+1}\"):\n",
    "             try:\n",
    "                 time.sleep(0.5)\n",
    "                 hist_df = scraper.get_past_races(hid, n_samples=None)\n",
    "                 if hist_df is not None and not hist_df.empty:\n",
    "                     if 'date' in hist_df.columns:\n",
    "                         hist_df['date_dt'] = pd.to_datetime(hist_df['date'], format='%Y/%m/%d', errors='coerce')\n",
    "                     history_cache[hid] = hist_df\n",
    "             except:\n",
    "                 pass\n",
    "        \n",
    "        # Apply to DataFrame iteratively (Complex because depends on Race Date)\n",
    "        print(\"  Applying history to missing rows...\")\n",
    "        \n",
    "        # We need to iterate over the rows in the main DF that correspond to these horses\n",
    "        # This part is slow if not vectorized, but logic is complex (compare dates).\n",
    "        # Optimization: Group by horse_id\n",
    "        \n",
    "        chunk_mask = df['horse_id'].isin(chunk_ids) & mask_missing\n",
    "        affected_indices = df[chunk_mask].index\n",
    "        \n",
    "        updates = [] # List of (index, col, value)\n",
    "        \n",
    "        for idx in tqdm(affected_indices, desc=\"Updating Rows\"):\n",
    "            row = df.loc[idx]\n",
    "            hid = row['horse_id']\n",
    "            race_date_str = str(row['Êó•‰ªò']) # YYYYÂπ¥MMÊúàDDÊó•\n",
    "            \n",
    "            if hid not in history_cache: continue\n",
    "            \n",
    "            hist_df = history_cache[hid]\n",
    "            if hist_df is None or hist_df.empty: continue\n",
    "            \n",
    "            try:\n",
    "                # Parse race date\n",
    "                # Handle 'YYYYÂπ¥MMÊúàDDÊó•' or 'YYYY/MM/DD'\n",
    "                race_date_str = race_date_str.replace('Âπ¥','/').replace('Êúà','/').replace('Êó•','')\n",
    "                current_date = pd.to_datetime(race_date_str, errors='coerce')\n",
    "                \n",
    "                if pd.isna(current_date): continue\n",
    "                \n",
    "                # Filter history < current_date\n",
    "                valid_hist = hist_df[hist_df['date_dt'] < current_date].copy()\n",
    "                \n",
    "                if valid_hist.empty: continue\n",
    "                \n",
    "                # Take top 5\n",
    "                valid_hist = valid_hist.sort_values('date_dt', ascending=False).head(5)\n",
    "                \n",
    "                # Prepare update dict for this row\n",
    "                # Columns: past_1_date, past_1_rank, ...\n",
    "                cols_map = {\n",
    "                    'date': 'date', 'rank': 'rank', 'time': 'time', 'run_style': 'run_style',\n",
    "                    'race_name': 'race_name', 'last_3f': 'last_3f', 'horse_weight': 'horse_weight',\n",
    "                    'jockey': 'jockey', 'condition': 'condition', 'weather': 'weather',\n",
    "                    'distance': 'distance', 'course_type': 'course_type', 'odds': 'odds'\n",
    "                }\n",
    "                \n",
    "                for n, (_, h_row) in enumerate(valid_hist.iterrows()):\n",
    "                    if n >= 5: break\n",
    "                    prefix = f\"past_{n+1}_\"\n",
    "                    \n",
    "                    df.at[idx, prefix + 'date'] = h_row.get('date')\n",
    "                    df.at[idx, prefix + 'rank'] = h_row.get('rank')\n",
    "                    df.at[idx, prefix + 'time'] = h_row.get('time')\n",
    "                    df.at[idx, prefix + 'race_name'] = h_row.get('race_name')\n",
    "                    # ... Add other columns as needed. For brevity, main ones.\n",
    "                    # Note: assign directly to avoid huge list overhead\n",
    "                    \n",
    "                    for key, val_key in cols_map.items():\n",
    "                         df.at[idx, prefix + key] = h_row.get(val_key)\n",
    "\n",
    "            except Exception as e:\n",
    "                # print(f\"Error updating row {idx}: {e}\")\n",
    "                pass\n",
    "\n",
    "        # Clear cache for this chunk to free memory\n",
    "        history_cache = {}\n",
    "        \n",
    "        # Save\n",
    "        print(f\"  üíæ Saving progress to {df_path}...\")\n",
    "        if df_path.endswith('.parquet'):\n",
    "             df.to_parquet(df_path, index=False)\n",
    "        else:\n",
    "             df.to_csv(df_path, index=False)\n",
    "\n",
    "    print(\"‚úÖ History backfill complete.\")\n",
    "\n",
    "\n",
    "def fill_race_metadata(df_path, mode=\"JRA\"):\n",
    "    \"\"\"\n",
    "    Backfills missing race metadata (course_type, distance, weather, condition).\n",
    "    \"\"\"\n",
    "    print(f\"\\nüèüÔ∏è Starting Race Metadata Backfill for {mode} ({os.path.basename(df_path)})\")\n",
    "    \n",
    "    if not os.path.exists(df_path):\n",
    "        print(f\"‚ùå File not found: {df_path}\")\n",
    "        return\n",
    "\n",
    "    # Load Data\n",
    "    try:\n",
    "        if df_path.endswith('.parquet'):\n",
    "            df = pd.read_parquet(df_path)\n",
    "        else:\n",
    "            df = pd.read_csv(df_path, low_memory=False)\n",
    "            if 'race_id' in df.columns:\n",
    "                df['race_id'] = df['race_id'].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Identify missing rows\n",
    "    target_cols = ['„Ç≥„Éº„Çπ„Çø„Ç§„Éó', 'Ë∑ùÈõ¢', 'Â§©ÂÄô', 'È¶¨Â†¥Áä∂ÊÖã']\n",
    "    for c in target_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "    \n",
    "    missing_mask = (df['„Ç≥„Éº„Çπ„Çø„Ç§„Éó'].isna()) | (df['„Ç≥„Éº„Çπ„Çø„Ç§„Éó'] == '') | \\\n",
    "                   (df['Ë∑ùÈõ¢'].isna()) | (df['Ë∑ùÈõ¢'] == '') | \\\n",
    "                   (df['Â§©ÂÄô'].isna()) | (df['Â§©ÂÄô'] == '')\n",
    "                   \n",
    "    target_race_ids = df.loc[missing_mask, 'race_id'].unique()\n",
    "    target_race_ids = [rid for rid in target_race_ids if str(rid).isdigit()]\n",
    "    \n",
    "    total_targets = len(target_race_ids)\n",
    "    print(f\"üéØ Found {total_targets} races with missing metadata.\")\n",
    "    \n",
    "    if total_targets == 0:\n",
    "        print(\"‚úÖ No missing metadata found.\")\n",
    "        return\n",
    "\n",
    "    scraper = RaceScraper()\n",
    "    results = {}\n",
    "    \n",
    "    # Sequential Execution\n",
    "    print(f\"üöÄ Fetching metadata for {total_targets} races (Sequential)...\")\n",
    "    \n",
    "    CHUNK_SIZE = 200\n",
    "    for i in range(0, total_targets, CHUNK_SIZE):\n",
    "        chunk = target_race_ids[i:i+CHUNK_SIZE]\n",
    "        \n",
    "        for rid in tqdm(chunk, leave=False):\n",
    "            try:\n",
    "                time.sleep(0.5)\n",
    "                data = scraper.get_race_metadata(rid)\n",
    "                if data and data.get('course_type'):\n",
    "                    results[rid] = data\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Save Progress\n",
    "        if len(results) > 0:\n",
    "            print(\"  Applying metadata updates...\")\n",
    "            mask = df['race_id'].isin(results.keys())\n",
    "            \n",
    "            c_map = {rid: d['course_type'] for rid, d in results.items() if d.get('course_type')}\n",
    "            d_map = {rid: d['distance'] for rid, d in results.items() if d.get('distance')}\n",
    "            w_map = {rid: d['weather'] for rid, d in results.items() if d.get('weather')}\n",
    "            cond_map = {rid: d['condition'] for rid, d in results.items() if d.get('condition')}\n",
    "            \n",
    "            df.loc[mask, '„Ç≥„Éº„Çπ„Çø„Ç§„Éó'] = df.loc[mask, 'race_id'].map(c_map).fillna(df.loc[mask, '„Ç≥„Éº„Çπ„Çø„Ç§„Éó'])\n",
    "            df.loc[mask, 'Ë∑ùÈõ¢'] = df.loc[mask, 'race_id'].map(d_map).fillna(df.loc[mask, 'Ë∑ùÈõ¢'])\n",
    "            df.loc[mask, 'Â§©ÂÄô'] = df.loc[mask, 'race_id'].map(w_map).fillna(df.loc[mask, 'Â§©ÂÄô'])\n",
    "            df.loc[mask, 'È¶¨Â†¥Áä∂ÊÖã'] = df.loc[mask, 'race_id'].map(cond_map).fillna(df.loc[mask, 'È¶¨Â†¥Áä∂ÊÖã'])\n",
    "            \n",
    "            results = {} # Clear buffer\n",
    "            \n",
    "            print(f\"  üíæ Saving progress to {df_path}...\")\n",
    "            if df_path.endswith('.parquet'):\n",
    "                df.to_parquet(df_path, index=False)\n",
    "            else:\n",
    "                df.to_csv(df_path, index=False)\n",
    "\n",
    "    print(\"‚úÖ Race metadata backfill complete.\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example Usage\n",
    "#     print(\"Usage: Select mode to run.\")\n",
    "#     # Uncomment based on need\n",
    "#     # fill_bloodline_data('data/raw/database.csv', mode=\"JRA\")\n",
    "#     # fill_history_data('data/raw/database.csv', mode=\"JRA\")\n",
    "    \n",
    "#     # fill_bloodline_data('data/raw/database_nar.csv', mode=\"NAR\")\n",
    "#     # fill_history_data('data/raw/database_nar.csv', mode=\"NAR\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ë®≠ÂÆö\n",
    "DATA_DIR = '/content/drive/MyDrive/dai-keiba/data/raw'\n",
    "\n",
    "# ÂÆüË°å„Éñ„É≠„ÉÉ„ÇØ\n",
    "csv_path = os.path.join(DATA_DIR, 'database_nar.csv')\n",
    "if os.path.exists(csv_path):\n",
    "    print(f'Âá¶ÁêÜÂØæË±°: {csv_path}')\n",
    "    fill_bloodline_data(csv_path, mode='NAR')\n",
    "    fill_history_data(csv_path, mode='NAR')\n",
    "    fill_race_metadata(csv_path, mode='NAR')\n",
    "else:\n",
    "    print(f'{csv_path} „ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇ')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}