{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèá NAR Ë©≥Á¥∞ÊÉÖÂ†±„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ (Stage 2/2) v2\n",
    "\n",
    "## üìä ÂèñÂæó„Éá„Éº„Çø\n",
    "- **68„Ç´„É©„É†**: past_1~5„ÅÆÂ±•Ê≠¥„Éá„Éº„Çø(65„Ç´„É©„É†) + Ë°ÄÁµ±„Éá„Éº„Çø(3„Ç´„É©„É†)\n",
    "\n",
    "## üìù ÂâçÊèêÊù°‰ª∂\n",
    "- Stage 1„Åß `database_nar_basic.csv` „Åå‰ΩúÊàêÊ∏à„Åø\n",
    "- horse_id„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Çã\n",
    "\n",
    "## ‚úÖ ÁâπÂæ¥\n",
    "- Ë©≤ÂΩì„É¨„Éº„ÇπÊôÇÁÇπ„Åß„ÅÆÈÅéÂéªËµ∞„Éá„Éº„Çø„ÇíÂèñÂæó\n",
    "- race_date‰ª•Ââç„ÅÆ„É¨„Éº„Çπ„ÅÆ„Åø\n",
    "- „Ç´„É©„É†„Ç∫„É¨„ÇíÂÆåÂÖ®„Å´Èò≤Ê≠¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive Mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "SAVE_DIR = '/content/drive/MyDrive/dai-keiba/data/raw'\n",
    "BASIC_CSV = 'database_nar_basic.csv'\n",
    "DETAILS_CSV = 'database_nar_details.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import io\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"‚úÖ Libraries loaded\")",
    "\n",
    "# --- Helper Functions for Details ---\n",
    "JOCKEY_CACHE = {}\n",
    "def get_jockey_fullname(url, short_name):\n",
    "    if not url: return short_name\n",
    "    if url in JOCKEY_CACHE: return JOCKEY_CACHE[url]\n",
    "    try:\n",
    "        time.sleep(0.5)\n",
    "        if not url.startswith('http'):\n",
    "            url = 'https://db.netkeiba.com' + url\n",
    "        r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        r.encoding = 'EUC-JP'\n",
    "        s = BeautifulSoup(r.text, 'html.parser')\n",
    "        h1 = s.find('h1')\n",
    "        if h1:\n",
    "             title = h1.text.split()[0].strip()\n",
    "             clean = re.sub(r'(„ÅÆË™øÊïôÂ∏´ÊàêÁ∏æ|„ÅÆÈ®éÊâãÊàêÁ∏æ|„ÅÆ„Éó„É≠„Éï„Ç£„Éº„É´|ÔΩú).*', '', title).strip()\n",
    "             JOCKEY_CACHE[url] = clean\n",
    "             return clean\n",
    "    except:\n",
    "        pass\n",
    "    return short_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# È¶¨Â±•Ê≠¥„ÉªË°ÄÁµ±ÂèñÂæóÈñ¢Êï∞\n",
    "\n",
    "def get_horse_details(horse_id, race_date):\n",
    "    \"\"\"\n",
    "    È¶¨„ÅÆÂ±•Ê≠¥„ÉªË°ÄÁµ±„Éá„Éº„Çø„ÇíÂèñÂæó(68„Ç´„É©„É†)\n",
    "    race_date‰ª•Ââç„ÅÆÈÅéÂéª5Ëµ∞„ÅÆ„ÅøÂèñÂæó\n",
    "    \"\"\"\n",
    "    details = {\n",
    "        'race_id': '',\n",
    "        'horse_id': horse_id\n",
    "    }\n",
    "    \n",
    "    # ÈÅéÂéª5Ëµ∞„ÅÆÂàùÊúüÂåñ\n",
    "    for i in range(1, 6):\n",
    "        prefix = f'past_{i}'\n",
    "        for field in ['date', 'rank', 'time', 'run_style', 'race_name', 'last_3f', \n",
    "                      'horse_weight', 'weight_change', 'jockey', 'condition', 'odds', 'weather', 'distance', 'course_type']:\n",
    "            details[f'{prefix}_{field}'] = ''\n",
    "    \n",
    "    # Ë°ÄÁµ±„ÅÆÂàùÊúüÂåñ\n",
    "    details['father'] = ''\n",
    "    details['mother'] = ''\n",
    "    details['bms'] = ''\n",
    "    \n",
    "    try:\n",
    "        time.sleep(0.5)\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        \n",
    "        # È¶¨„Éö„Éº„Ç∏ÂèñÂæó\n",
    "        url = f\"https://db.netkeiba.com/horse/result/{horse_id}/\"\n",
    "        resp = requests.get(url, headers=headers, timeout=15)\n",
    "        resp.encoding = 'EUC-JP'\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        \n",
    "        # Ë°ÄÁµ±ÊÉÖÂ†±(Á∞°ÊòìÁâà - „Éö„Éº„Ç∏ÂÜÖ„ÉÜ„Ç≠„Çπ„Éà„Åã„Çâ)\n",
    "        # ÂÆüÈöõ„ÅÆÂÆüË£Ö„Åß„ÅØË°ÄÁµ±„ÉÜ„Éº„Éñ„É´„Çí„Éë„Éº„Çπ\n",
    "        \n",
    "        # „É¨„Éº„ÇπÂ±•Ê≠¥ÂèñÂæó\n",
    "        table = soup.select_one(\"table.db_h_race_results\")\n",
    "        if not table:\n",
    "            tables = soup.find_all(\"table\")\n",
    "            for t in tables:\n",
    "                if \"ÁùÄÈ†Ü\" in t.text:\n",
    "                    table = t\n",
    "                    break\n",
    "        \n",
    "        if table:\n",
    "            df = pd.read_html(io.StringIO(str(table)))[0]\n",
    "            df = df.dropna(how='all')\n",
    "            df.columns = df.columns.astype(str).str.replace(r'\\s+', '', regex=True)\n",
    "            \n",
    "            # Êó•‰ªò„Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
    "        # --- PEDIGREE FETCH (Dual URL) ---\n",
    "        try:\n",
    "            # Only fetch if we are actually processing this horse\n",
    "            ped_url = f\"https://db.netkeiba.com/horse/ped/{horse_id}/\"\n",
    "            # Simple check to avoid hammer if needed, but we do it every time for now\n",
    "            time.sleep(0.5) # Be polite\n",
    "            r_ped = requests.get(ped_url, headers=headers)\n",
    "            r_ped.encoding = 'EUC-JP'\n",
    "            soup_ped = BeautifulSoup(r_ped.text, 'html.parser')\n",
    "            blood_tbl = soup_ped.select_one('table.blood_table')\n",
    "            if blood_tbl:\n",
    "                tds = blood_tbl.find_all('td')\n",
    "                # Flattened TDs: 0=Father, 1=Unknown(Mother?), but heuristic: 0 is Father\n",
    "                if len(tds) > 0: details['father'] = tds[0].text.strip().split('\\n')[0]\n",
    "                # Mother is tricky. Let's try to get 'bms' (Broodmare Sire) -> Mother's Father\n",
    "                # If table is 5 generation:\n",
    "                # 0: Father, 1: F-Father, 2: F-F-Father, 3: F-F-Mother\n",
    "                # Mother is in the bottom half of the first split.\n",
    "                # Table structure: tr1 has Father (rowspan 16), tr17 has Mother (rowspan 16)\n",
    "                # If we use find_all('td', rowspan='...'):\n",
    "                # Let's try finding the row for Mother.\n",
    "                # Heuristic: The text in the cell with rowspan that appears 2nd in the 'Parents' column.\n",
    "                # Simplified: Just grab Father for now as proof.\n",
    "                pass\n",
    "        except Exception as e: pass\n",
    "        # ---------------------------------\n",
    "\n",
    "            if 'Êó•‰ªò' in df.columns:\n",
    "                df['date_obj'] = pd.to_datetime(df['Êó•‰ªò'].astype(str).str.replace('.', '/'), errors='coerce')\n",
    "                df = df.dropna(subset=['date_obj'])\n",
    "                \n",
    "                # race_date‰ª•Ââç„ÅÆ„É¨„Éº„Çπ„ÅÆ„Åø\n",
    "                current_date = pd.to_datetime(race_date)\n",
    "                df = df[df['date_obj'] < current_date]\n",
    "                df = df.sort_values('date_obj', ascending=False)\n",
    "                df = df.head(5)\n",
    "                \n",
    "                # „Éá„Éº„ÇøÊäΩÂá∫\n",
    "                for i, row in enumerate(df.itertuples(), 1):\n",
    "                    prefix = f'past_{i}'\n",
    "                    \n",
    "                    details[f'{prefix}_date'] = getattr(row, 'Êó•‰ªò', '')\n",
    "                    details[f'{prefix}_rank'] = str(getattr(row, 'ÁùÄÈ†Ü', ''))\n",
    "                    details[f'{prefix}_time'] = str(getattr(row, '„Çø„Ç§„É†', ''))\n",
    "                    details[f'{prefix}_race_name'] = str(getattr(row, '„É¨„Éº„ÇπÂêç', ''))\n",
    "                    details[f'{prefix}_last_3f'] = str(getattr(row, '‰∏ä„Çä', ''))\n",
    "                    details[f'{prefix}_horse_weight'] = str(getattr(row, 'È¶¨‰ΩìÈáç', ''))\n",
    "                    details[f'{prefix}_jockey'] = str(getattr(row, 'È®éÊâã', ''))\n",
    "                    details[f'{prefix}_condition'] = str(getattr(row, 'È¶¨Â†¥', ''))\n",
    "                    details[f'{prefix}_odds'] = str(getattr(row, 'ÂçòÂãù', '') or getattr(row, '„Ç™„ÉÉ„Ç∫', ''))\n",
    "                    details[f'{prefix}_weather'] = str(getattr(row, 'Â§©Ê∞ó', ''))\n",
    "                    \n",
    "                    # Ë∑ùÈõ¢„Éª„Ç≥„Éº„Çπ„Çø„Ç§„Éó\n",
    "                    dist_text = str(getattr(row, 'Ë∑ùÈõ¢', ''))\n",
    "                    dist_match = re.search(r'(Ëäù|„ÉÄ|Èöú)(\\d+)', dist_text)\n",
    "                    if dist_match:\n",
    "                        course_type = dist_match.group(1)\n",
    "                        details[f'{prefix}_course_type'] = 'Ëäù' if course_type == 'Ëäù' else '„ÉÄ„Éº„Éà' if course_type == '„ÉÄ' else 'ÈöúÂÆ≥'\n",
    "                        details[f'{prefix}_distance'] = dist_match.group(2)\n",
    "                    \n",
    "                    # ËÑöË≥™(Á∞°ÊòìÁâà)\n",
    "                    details[f'{prefix}_run_style'] = '3'  # „Éá„Éï„Ç©„É´„Éà\n",
    "        \n",
    "        return details\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è „Ç®„É©„Éº({horse_id}): {e}\")\n",
    "        return details\n",
    "\n",
    "print(\"‚úÖ Horse details function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „É°„Ç§„É≥„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÂÆüË°å\n",
    "\n",
    "def run_details_scraping():\n",
    "    \"\"\"\n",
    "    Stage 1„ÅÆCSV„Åã„Çâhorse_id„ÇíË™≠„ÅøËæº„Åø„ÄÅË©≥Á¥∞„Éá„Éº„Çø„ÇíÂèñÂæó\n",
    "    \"\"\"\n",
    "    basic_path = os.path.join(SAVE_DIR, BASIC_CSV)\n",
    "    details_path = os.path.join(SAVE_DIR, DETAILS_CSV)\n",
    "    \n",
    "    # Stage 1„Éá„Éº„ÇøË™≠„ÅøËæº„Åø\n",
    "    if not os.path.exists(basic_path):\n",
    "        print(f\"‚ùå Stage 1„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {basic_path}\")\n",
    "        return\n",
    "    \n",
    "    df_basic = pd.read_csv(basic_path, dtype=str, on_bad_lines='skip', usecols=['race_id', 'horse_id', 'Êó•‰ªò'])\n",
    "    print(f\"üìã Stage 1„Éá„Éº„Çø: {len(df_basic)}Ë°å\")\n",
    "    \n",
    "    # Êó¢Â≠ò„ÅÆdetails„Éá„Éº„Çø\n",
    "    existing_ids = set()\n",
    "    if os.path.exists(details_path):\n",
    "        df_existing = pd.read_csv(details_path, dtype=str, on_bad_lines='skip')\n",
    "        if 'race_id' in df_existing.columns and 'horse_id' in df_existing.columns:\n",
    "            existing_ids = set(df_existing['race_id'] + '_' + df_existing['horse_id'])\n",
    "        print(f\"üíæ Êó¢Â≠ò„Éá„Éº„Çø: {len(existing_ids)}‰ª∂\")\n",
    "    \n",
    "    # Â∑ÆÂàÜË®àÁÆó\n",
    "    df_basic['key'] = df_basic['race_id'] + '_' + df_basic['horse_id']\n",
    "    df_target = df_basic[~df_basic['key'].isin(existing_ids)]\n",
    "    print(f\"üöÄ ‰ªäÂõûÂèñÂæó: {len(df_target)}‰ª∂\\n\")\n",
    "    \n",
    "    if df_target.empty:\n",
    "        print(\"‚úÖ ÂÖ®„Å¶ÂèñÂæóÊ∏à„Åø„Åß„Åô\")\n",
    "        return\n",
    "    \n",
    "    # „Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞\n",
    "    buffer = []\n",
    "    chunk_size = 50\n",
    "    \n",
    "    for i, row in enumerate(tqdm(df_target.itertuples(), total=len(df_target))):\n",
    "        horse_id = row.horse_id\n",
    "        race_date = row.Êó•‰ªò\n",
    "        race_id = row.race_id\n",
    "        \n",
    "        if not horse_id or not race_date:\n",
    "            continue\n",
    "        \n",
    "        details = get_horse_details(horse_id, race_date)\n",
    "        details['race_id'] = race_id\n",
    "        buffer.append(details)\n",
    "        \n",
    "        # „ÉÅ„É£„É≥„ÇØ‰øùÂ≠ò\n",
    "        if len(buffer) >= chunk_size or (i == len(df_target) - 1 and buffer):\n",
    "            df_chunk = pd.DataFrame(buffer)\n",
    "            \n",
    "            # „Ç´„É©„É†È†ÜÂ∫è„ÇíÊòéÁ§∫\n",
    "            ordered_columns = ['race_id', 'horse_id']\n",
    "            for i in range(1, 6):\n",
    "                prefix = f'past_{i}'\n",
    "                for field in ['date', 'rank', 'time', 'run_style', 'race_name', 'last_3f', \n",
    "                              'horse_weight', 'weight_change', 'jockey', 'condition', 'odds', 'weather', 'distance', 'course_type']:\n",
    "                    ordered_columns.append(f'{prefix}_{field}')\n",
    "            ordered_columns.extend(['father', 'mother', 'bms'])\n",
    "            \n",
    "            df_chunk = df_chunk[ordered_columns]\n",
    "            \n",
    "            if not os.path.exists(details_path):\n",
    "                df_chunk.to_csv(details_path, index=False)\n",
    "            else:\n",
    "                df_chunk.to_csv(details_path, mode='a', header=False, index=False)\n",
    "            \n",
    "            print(f\"  üíæ Saved {len(buffer)} records\")\n",
    "            buffer = []\n",
    "    \n",
    "    print(\"\\n‚úÖ „Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÂÆå‰∫Ü\")\n",
    "\n",
    "print(\"‚úÖ Main function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂÆüË°å\n",
    "run_details_scraping()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}