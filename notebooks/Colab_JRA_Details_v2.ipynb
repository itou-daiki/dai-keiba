{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JRA Details Scraper v2\n\n- **å–å¾—å¯¾è±¡**: ç«¶èµ°é¦¬ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ï¼ˆéå»èµ°ã€è¡€çµ±ï¼‰\n- **ä¸»ãªå¤‰æ›´ç‚¹**:\n  - `/horse/result/` (æˆ¦ç¸¾) ã¨ `/horse/ped/` (è¡€çµ±) ã®Dual URLå–å¾—\n  - éå»5èµ°ã®ãƒ‡ãƒ¼ã‚¿ (æ—¥ä»˜, ç€é †, ã‚¿ã‚¤ãƒ ... ã‚³ãƒ¼ã‚¹, å¤©å€™, ã‚ªãƒƒã‚º)\n  - é¦¬ä½“é‡ã®åˆ†å‰² (`_horse_weight`, `_weight_change`)\n  - é¨æ‰‹åã®ãƒ•ãƒ«ãƒãƒ¼ãƒ åŒ–\n  - è¡€çµ±æƒ…å ± (`father`, `mother`, `bms`)\n- **å–å¾—å¤‰æ•°**: è¨ˆ70å€‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive Mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "SAVE_DIR = '/content/drive/MyDrive/dai-keiba/data/raw'\n",
    "BASIC_CSV = 'database_basic.csv'\n",
    "DETAILS_CSV = 'database_details.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import io\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"âœ… Libraries loaded\")",
    "\n",
    "# --- Helper Functions for Details ---\n",
    "JOCKEY_CACHE = {}\n",
    "def get_jockey_fullname(url, short_name):\n",
    "    if not url: return short_name\n",
    "    if url in JOCKEY_CACHE: return JOCKEY_CACHE[url]\n",
    "    try:\n",
    "        time.sleep(0.5)\n",
    "        if not url.startswith('http'):\n",
    "            url = 'https://db.netkeiba.com' + url\n",
    "        r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        r.encoding = 'EUC-JP'\n",
    "        s = BeautifulSoup(r.text, 'html.parser')\n",
    "        h1 = s.find('h1')\n",
    "        if h1:\n",
    "             title = h1.text.split()[0].strip()\n",
    "             clean = re.sub(r'(ã®èª¿æ•™å¸«æˆç¸¾|ã®é¨æ‰‹æˆç¸¾|ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«|ï½œ).*', '', title).strip()\n",
    "             JOCKEY_CACHE[url] = clean\n",
    "             return clean\n",
    "    except:\n",
    "                    pass\n",
    "    return short_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¦¬å±¥æ­´ãƒ»è¡€çµ±å–å¾—é–¢æ•°\n",
    "\n",
    "def get_horse_details(horse_id, race_date):\n",
    "    \"\"\"\n",
    "    é¦¬ã®å±¥æ­´ãƒ»è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—(68ã‚«ãƒ©ãƒ )\n",
    "    race_dateä»¥å‰ã®éå»5èµ°ã®ã¿å–å¾—\n",
    "    \"\"\"\n",
    "    details = {\n",
    "        'race_id': '',\n",
    "        'horse_id': horse_id\n",
    "    }\n",
    "    \n",
    "    # éå»5èµ°ã®åˆæœŸåŒ–\n",
    "    for i in range(1, 6):\n",
    "        prefix = f'past_{i}'\n",
    "        for field in ['date', 'rank', 'time', 'run_style', 'race_name', 'last_3f', \n",
    "                      'horse_weight', 'weight_change', 'jockey', 'condition', 'odds', 'weather', 'distance', 'course_type']:\n",
    "            details[f'{prefix}_{field}'] = ''\n",
    "    \n",
    "    # è¡€çµ±ã®åˆæœŸåŒ–\n",
    "    details['father'] = ''\n",
    "    details['mother'] = ''\n",
    "    details['bms'] = ''\n",
    "    \n",
    "    try:\n",
    "        time.sleep(0.5)\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        \n",
    "        # é¦¬ãƒšãƒ¼ã‚¸å–å¾—\n",
    "        url = f\"https://db.netkeiba.com/horse/result/{horse_id}/\"\n",
    "        resp = requests.get(url, headers=headers, timeout=15)\n",
    "        resp.encoding = 'EUC-JP'\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        \n",
    "        # è¡€çµ±æƒ…å ±(ç°¡æ˜“ç‰ˆ - ãƒšãƒ¼ã‚¸å†…ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰)\n",
    "        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯è¡€çµ±ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹\n",
    "        \n",
    "        # ãƒ¬ãƒ¼ã‚¹å±¥æ­´å–å¾—\n",
    "        table = soup.select_one(\"table.db_h_race_results\")\n",
    "        if not table:\n",
    "            tables = soup.find_all(\"table\")\n",
    "            for t in tables:\n",
    "                if \"ç€é †\" in t.text:\n",
    "                    table = t\n",
    "                    break\n",
    "        \n",
    "        if table:\n",
    "            df = pd.read_html(io.StringIO(str(table)))[0]\n",
    "            df = df.dropna(how='all')\n",
    "            df.columns = df.columns.astype(str).str.replace(r'\\s+', '', regex=True)\n",
    "            \n",
    "            # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "            # --- PEDIGREE FETCH (Dual URL) ---\n",
    "            try:\n",
    "                ped_url = f\"https://db.netkeiba.com/horse/ped/{horse_id}/\"\n",
    "                time.sleep(0.5)\n",
    "                r_ped = requests.get(ped_url, headers=headers)\n",
    "                r_ped.encoding = 'EUC-JP'\n",
    "                soup_ped = BeautifulSoup(r_ped.text, 'html.parser')\n",
    "                blood_tbl = soup_ped.select_one('table.blood_table')\n",
    "                if blood_tbl:\n",
    "                    trs = blood_tbl.find_all('tr')\n",
    "                    if len(trs) > 0:\n",
    "                        tds0 = trs[0].find_all('td')\n",
    "                        if len(tds0) > 0: details['father'] = tds0[0].text.strip().split('\\n')[0]\n",
    "                    \n",
    "                    mid_idx = int(len(trs) / 2)\n",
    "                    if mid_idx > 0 and len(trs) > mid_idx:\n",
    "                        tds_m = trs[mid_idx].find_all('td')\n",
    "                        if len(tds_m) > 0:\n",
    "                             details['mother'] = tds_m[0].text.strip().split('\\n')[0]\n",
    "                        if len(tds_m) > 1:\n",
    "                             details['bms'] = tds_m[1].text.strip().split('\\n')[0]\n",
    "            except: pass\n",
    "\n",
    "            if 'æ—¥ä»˜' in df.columns:\n",
    "                df['date_obj'] = pd.to_datetime(df['æ—¥ä»˜'].astype(str).str.replace('.', '/'), errors='coerce')\n",
    "                df = df.dropna(subset=['date_obj'])\n",
    "                \n",
    "                # race_dateä»¥å‰ã®ãƒ¬ãƒ¼ã‚¹ã®ã¿\n",
    "                current_date = pd.to_datetime(race_date)\n",
    "                df = df[df['date_obj'] < current_date]\n",
    "                df = df.sort_values('date_obj', ascending=False)\n",
    "                df = df.head(5)\n",
    "                \n",
    "                # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º\n",
    "                for i, row in enumerate(df.itertuples(), 1):\n",
    "                    prefix = f'past_{pi}'\n",
    "                    \n",
    "                    details[f'{prefix}_date'] = getattr(row, 'æ—¥ä»˜', '')\n",
    "                    details[f'{prefix}_rank'] = str(getattr(row, 'ç€é †', ''))\n",
    "                    details[f'{prefix}_time'] = str(getattr(row, 'ã‚¿ã‚¤ãƒ ', ''))\n",
    "                    details[f'{prefix}_race_name'] = str(getattr(row, 'ãƒ¬ãƒ¼ã‚¹å', ''))\n",
    "                    details[f'{prefix}_last_3f'] = str(getattr(row, 'ä¸Šã‚Š', ''))\n",
    "                    # Weight Split\n",
    "                    w_val = str(getattr(row, 'é¦¬ä½“é‡', ''))\n",
    "                    w_num = w_val\n",
    "                    w_chg = ''\n",
    "                    if isinstance(w_val, str):\n",
    "                        wm = re.search(r'(\\d+)\\(([\\+\\-\\d]+)\\)', w_val)\n",
    "                        if wm:\n",
    "                            w_num = wm.group(1)\n",
    "                            w_chg = wm.group(2).replace('+', '')\n",
    "                    details[f'{prefix}_horse_weight'] = w_num\n",
    "                    details[f'{prefix}_weight_change'] = w_chg\n",
    "                    # Jockey Full Name & Link Recovery\n",
    "                    j_text = str(getattr(row, 'é¨æ‰‹', ''))\n",
    "                    r_date = str(getattr(row, 'æ—¥ä»˜', ''))\n",
    "                    try:\n",
    "                        # Find row in soup by Date match\n",
    "                        # table is defined.\n",
    "                        found_link = None\n",
    "                        for tr in table.find_all('tr'):\n",
    "                            if r_date in tr.text:\n",
    "                                # Found match\n",
    "                                links = tr.find_all('a')\n",
    "                                for lk in links:\n",
    "                                    if '/jockey/' in lk.get('href', ''):\n",
    "                                        found_link = lk.get('href')\n",
    "                                        break\n",
    "                                if found_link: break\n",
    "                        \n",
    "                        if found_link:\n",
    "                            j_text = get_jockey_fullname(found_link, j_text)\n",
    "                    except: pass\n",
    "                    details[f'{prefix}_jockey'] = j_text\n",
    "                    details[f'{prefix}_condition'] = str(getattr(row, 'é¦¬å ´', ''))\n",
    "                    details[f'{prefix}_odds'] = str(getattr(row, 'å˜å‹', '') or getattr(row, 'ã‚ªãƒƒã‚º', ''))\n",
    "                    details[f'{prefix}_weather'] = str(getattr(row, 'å¤©æ°—', ''))\n",
    "                    \n",
    "                    # è·é›¢ãƒ»ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—\n",
    "                    dist_text = str(getattr(row, 'è·é›¢', ''))\n",
    "                    dist_match = re.search(r'(èŠ|ãƒ€|éšœ)(\\d+)', dist_text)\n",
    "                    if dist_match:\n",
    "                        course_type = dist_match.group(1)\n",
    "                        details[f'{prefix}_course_type'] = 'èŠ' if course_type == 'èŠ' else 'ãƒ€ãƒ¼ãƒˆ' if course_type == 'ãƒ€' else 'éšœå®³'\n",
    "                        details[f'{prefix}_distance'] = dist_match.group(2)\n",
    "                    \n",
    "                    # è„šè³ª(ç°¡æ˜“ç‰ˆ)\n",
    "                    details[f'{prefix}_run_style'] = '3'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ\n",
    "        \n",
    "        return details\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ ã‚¨ãƒ©ãƒ¼({horse_id}): {e}\")\n",
    "        return details\n",
    "\n",
    "print(\"âœ… Horse details function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ\n",
    "\n",
    "def run_details_scraping():\n",
    "    \"\"\"\n",
    "    Stage 1ã®CSVã‹ã‚‰horse_idã‚’èª­ã¿è¾¼ã¿ã€è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "    \"\"\"\n",
    "    basic_path = os.path.join(SAVE_DIR, BASIC_CSV)\n",
    "    details_path = os.path.join(SAVE_DIR, DETAILS_CSV)\n",
    "    \n",
    "    # Stage 1ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "    if not os.path.exists(basic_path):\n",
    "        print(f\"âŒ Stage 1ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {basic_path}\")\n",
    "        return\n",
    "    \n",
    "    df_basic = pd.read_csv(basic_path, dtype=str, on_bad_lines='skip', usecols=['race_id', 'horse_id', 'æ—¥ä»˜'])\n",
    "    print(f\"ğŸ“‹ Stage 1ãƒ‡ãƒ¼ã‚¿: {len(df_basic)}è¡Œ\")\n",
    "    \n",
    "    # æ—¢å­˜ã®detailsãƒ‡ãƒ¼ã‚¿\n",
    "    existing_ids = set()\n",
    "    if os.path.exists(details_path):\n",
    "        df_existing = pd.read_csv(details_path, dtype=str, on_bad_lines='skip')\n",
    "        if 'race_id' in df_existing.columns and 'horse_id' in df_existing.columns:\n",
    "            # Re-scrape if data is missing (e.g. 'father' is empty)\n",
    "            # If 'father' column exists, use it to filter completed rows\n",
    "            if 'father' in df_existing.columns:\n",
    "                # Robust check: father is not null and not empty string\n",
    "                completed_df = df_existing[df_existing['father'].notna() & (df_existing['father'] != '')]\n",
    "                existing_ids = set(completed_df['race_id'] + '_' + completed_df['horse_id'])\n",
    "            else:\n",
    "                # If column missing, assume all need scraping (or fallback to old logic?)\n",
    "                # Better to clear existing_ids if we can't verify completeness\n",
    "                existing_ids = set() # Force re-scrape if format is old\n",
    "                # existing_ids = set(df_existing['race_id'] + '_' + df_existing['horse_id']) # Old Logic\n",
    "        print(f\"ğŸ’¾ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(existing_ids)}ä»¶\")\n",
    "    \n",
    "    # å·®åˆ†è¨ˆç®—\n",
    "    df_basic['key'] = df_basic['race_id'] + '_' + df_basic['horse_id']\n",
    "    df_target = df_basic[~df_basic['key'].isin(existing_ids)]\n",
    "    print(f\"ğŸš€ ä»Šå›å–å¾—: {len(df_target)}ä»¶\\n\")\n",
    "    \n",
    "    if df_target.empty:\n",
    "        print(\"âœ… å…¨ã¦å–å¾—æ¸ˆã¿ã§ã™\")\n",
    "        return\n",
    "    \n",
    "    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°\n",
    "    buffer = []\n",
    "    chunk_size = 50\n",
    "    \n",
    "    for i, row in enumerate(tqdm(df_target.itertuples(), total=len(df_target))):\n",
    "        horse_id = row.horse_id\n",
    "        race_date = row.æ—¥ä»˜\n",
    "        race_id = row.race_id\n",
    "        \n",
    "        if not horse_id or not race_date:\n",
    "            continue\n",
    "        \n",
    "        details = get_horse_details(horse_id, race_date)\n",
    "        details['race_id'] = race_id\n",
    "        buffer.append(details)\n",
    "        \n",
    "        # ãƒãƒ£ãƒ³ã‚¯ä¿å­˜\n",
    "        # --- Circuit Breaker (Safety Stop) ---\n",
    "        if len(buffer) >= 10:\n",
    "             valid_count = sum(1 for d in buffer if d.get('father', '').strip() != '')\n",
    "             success_rate = valid_count / len(buffer)\n",
    "             if success_rate < 0.2:\n",
    "                 print(f\"\\nâš ï¸ CIRCUIT BREAKER TRIGGERED: Success rate {success_rate:.1%} is below threshold.\")\n",
    "                 print(\"â›” Stopping execution to prevent empty data accumulation (likely IP Ban).\")\n",
    "                 # Save what we have so far (optional, but maybe risky if all empty)\n",
    "                 # Raise exception to stop loop\n",
    "                 raise RuntimeError(\"Circuit Breaker: High failure rate detected.\")\n",
    "        # -------------------------------------\n",
    "\n",
    "        if len(buffer) >= chunk_size or (i == len(df_target) - 1 and buffer):\n",
    "            df_chunk = pd.DataFrame(buffer)\n",
    "            \n",
    "            # ã‚«ãƒ©ãƒ é †åºã‚’æ˜ç¤º\n",
    "            ordered_columns = ['race_id', 'horse_id']\n",
    "            for pi in range(1, 6):\n",
    "                prefix = f'past_{pi}'\n",
    "                for field in ['date', 'rank', 'time', 'run_style', 'race_name', 'last_3f', \n",
    "                              'horse_weight', 'weight_change', 'jockey', 'condition', 'odds', 'weather', 'distance', 'course_type']:\n",
    "                    ordered_columns.append(f'{prefix}_{field}')\n",
    "            ordered_columns.extend(['father', 'mother', 'bms'])\n",
    "            \n",
    "            df_chunk = df_chunk[ordered_columns]\n",
    "            \n",
    "            if not os.path.exists(details_path):\n",
    "                df_chunk.to_csv(details_path, index=False)\n",
    "            else:\n",
    "                df_chunk.to_csv(details_path, mode='a', header=False, index=False)\n",
    "            \n",
    "            print(f\"  ğŸ’¾ Saved {len(buffer)} records\")\n",
    "            buffer = []\n",
    "    \n",
    "    print(\"\\nâœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†\")\n",
    "\n",
    "print(\"âœ… Main function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Ÿè¡Œ\n",
    "run_details_scraping()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}