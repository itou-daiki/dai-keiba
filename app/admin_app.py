import streamlit as st
import subprocess
import os
import sys
import pandas as pd
from datetime import date, datetime
import time
import plotly.graph_objects as go

# Add ml to path
# Add ml to path
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'ml'))
try:
    import train_model
    import feature_engineering
    import importlib
    importlib.reload(train_model)
    importlib.reload(feature_engineering)
except ImportError:
    st.error("Failed to import train_model. Make sure ml/train_model.py exists.")



# Set page config
st.set_page_config(page_title="JRA ãƒ‡ãƒ¼ã‚¿ç®¡ç†ãƒ‘ãƒãƒ«", layout="wide")

st.title("ğŸ‡ JRA ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç®¡ç†ãƒ‘ãƒãƒ«")
st.markdown("ã“ã“ã§JRAå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€`database.csv` ã‚’æ›´æ–°ã—ã¾ã™ã€‚")

# --- Session State for Process Management ---
if 'process' not in st.session_state:
    st.session_state.process = None
if 'logs' not in st.session_state:
    st.session_state.logs = []
if 'is_running' not in st.session_state:
    st.session_state.is_running = False

# --- UI Layout (No Sidebar) ---
st.markdown("### âš™ï¸ è¨­å®š")

col1, col2 = st.columns(2)

with col1:
    year = st.selectbox("å¯¾è±¡å¹´", ["2025", "2024"], index=0, disabled=st.session_state.is_running)
    
    mode_option = st.radio("é–‹å‚¬ãƒ¢ãƒ¼ãƒ‰ (Mode)", ["JRA (ä¸­å¤®ç«¶é¦¬)", "NAR (åœ°æ–¹ç«¶é¦¬)"], index=0, disabled=st.session_state.is_running)
    mode_val = "JRA" if "JRA" in mode_option else "NAR"

    source_option = st.radio(
        "ãƒ‡ãƒ¼ã‚¿å–å¾—å…ƒ",
        ["netkeiba", "JRA"],
        index=0,
        disabled=st.session_state.is_running,
        help="é€šå¸¸ã¯netkeibaã‚’ä½¿ç”¨ã—ã¾ã™ã€‚JRAå…¬å¼ã¯è£œå®Œçš„ã«ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
    )
    source_val = "netkeiba" if source_option == "netkeiba" else "jra"

with col2:
    default_start = date(int(year), 1, 1)
    default_end = date(int(year), 12, 31)
    
    date_range = st.date_input(
        "å–å¾—æœŸé–“ (é–‹å§‹æ—¥ - çµ‚äº†æ—¥)",
        value=(default_start, default_start),
        min_value=date(2020, 1, 1),
        max_value=date(2030, 12, 31),
        disabled=st.session_state.is_running
    )

start_date_str = ""
end_date_str = ""

if isinstance(date_range, tuple):
    if len(date_range) == 2:
        start_d, end_d = date_range
        start_date_str = start_d.strftime("%Y-%m-%d")
        end_date_str = end_d.strftime("%Y-%m-%d")
        st.info(f"é¸æŠç¯„å›²: {start_date_str} ã€œ {end_date_str}")
    elif len(date_range) == 1:
        st.warning("çµ‚äº†æ—¥ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
else:
    st.warning("æ—¥ä»˜ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")

st.markdown("---")

col_btn_1, col_btn_2 = st.columns([1, 1])

with col_btn_1:
    if st.button("ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹", type="primary", disabled=st.session_state.is_running):
        if not start_date_str or not end_date_str:
            st.error("æœ‰åŠ¹ãªæœŸé–“ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        else:
            st.session_state.is_running = True
            st.session_state.logs = []
            
            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            cmd = [
                sys.executable, "-u", "scraper/auto_scraper.py", 
                "--start", start_date_str,
                "--end", end_date_str,
                "--source", source_val,
                "--mode", mode_val
            ]
            
            try:
                # Use Popen
                p = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    bufsize=1,
                    cwd=project_root
                )
                st.session_state.process = p
                st.rerun()
                
            except Exception as e:
                st.error(f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                st.session_state.is_running = False

with col_btn_2:
    if st.button("ğŸ›‘ åœæ­¢", type="secondary", disabled=not st.session_state.is_running):
        if st.session_state.process:
            st.session_state.process.terminate()
            st.session_state.process = None
        st.session_state.is_running = False
        st.error("å‡¦ç†ã‚’åœæ­¢ã—ã¾ã—ãŸã€‚")
        st.rerun()

# --- Log Streaming Area ---
st.markdown("### ğŸ“œ å®Ÿè¡Œãƒ­ã‚°")
log_container = st.empty()

if st.session_state.is_running and st.session_state.process:
    p = st.session_state.process
    # Read output non-blocking? logic in Streamlit loop is tricky.
    # We loop here reading one line at a time then rerun? No, that hangs UI.
    # Streamlit runs top-down. We can read available lines.
    
    import fcntl
    
    # Set non-blocking
    fd = p.stdout.fileno()
    fl = fcntl.fcntl(fd, fcntl.F_GETFL)
    fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)
    
    try:
        # Read all available
        chunk = p.stdout.read()
        if chunk:
            for line in chunk.splitlines():
                if line:
                    st.session_state.logs.append(line)
    except Exception:
        pass
        
    # Check if finished
    if p.poll() is not None:
        st.session_state.is_running = False
        st.session_state.process = None
        st.success("å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        st.rerun()
    else:
        # Rerun to keep updating logs
        time.sleep(0.5)
        st.rerun()

# Display logs
if st.session_state.logs:
    log_text = "\n".join(st.session_state.logs[-20:])
    log_container.code(log_text)

# --- Data Preview ---
st.markdown("### ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
if mode_val == "NAR":
    csv_filename = "database_nar.csv"
else:
    csv_filename = "database.csv"
    
csv_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data", "raw", csv_filename)

col_prev_1, col_prev_2 = st.columns([1, 4])
with col_prev_1:
    if st.button("ğŸ”„ æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"):
        st.rerun()

if os.path.exists(csv_path):
    try:
        df = pd.read_csv(csv_path)
        st.metric("ç·ãƒ‡ãƒ¼ã‚¿æ•° (è¡Œ)", len(df))
        st.dataframe(df, width='stretch')
    except Exception as e:
        st.error(f"{csv_filename} ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ: {e}")
else:
    st.warning(f"{csv_filename} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

st.markdown("---")

# --- Data Maintenance ---
st.markdown("### ğŸ”§ ãƒ‡ãƒ¼ã‚¿ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹")
st.info("ãƒ‡ãƒ¼ã‚¿ã«æ¬ æãŒã‚ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒœã‚¿ãƒ³ã§è£œå®Œã‚’è©¦ã¿ã¾ã™ã€‚")

col_mnt_1, col_mnt_2 = st.columns(2)

with col_mnt_1:
    if st.button("ğŸ”„ è£œå®Œ: ãƒ¬ãƒ¼ã‚¹æƒ…å ± (è·é›¢ãƒ»é¦¬å ´ãªã©-Netkeiba)"):
        with st.spinner("ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’è£œå®Œä¸­... (Netkeibaã‹ã‚‰å–å¾—)"):
             try:
                 # Run script
                 cmd = [sys.executable, "scripts/backfill_metadata.py"]
                 result = subprocess.run(cmd, capture_output=True, text=True)
                 if result.returncode == 0:
                     st.success("å®Œäº†ã—ã¾ã—ãŸï¼")
                     with st.expander("è©³ç´°ãƒ­ã‚°"):
                         st.code(result.stdout)
                 else:
                     st.error("ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
                     with st.expander("ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°"):
                         st.code(result.stderr + "\n" + result.stdout)
             except Exception as e:
                 st.error(f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")

with col_mnt_2:
    if st.button("ğŸ”„ è£œå®Œ: éå»ãƒ‡ãƒ¼ã‚¿ (Horse History)"):
         with st.spinner("éå»ãƒ‡ãƒ¼ã‚¿ã‚’è£œå®Œä¸­..."):
             try:
                 cmd = [sys.executable, "scripts/fill_past_data.py"]
                 result = subprocess.run(cmd, capture_output=True, text=True)
                 if result.returncode == 0:
                     st.success("å®Œäº†ã—ã¾ã—ãŸï¼")
                     with st.expander("è©³ç´°ãƒ­ã‚°"):
                         st.code(result.stdout)
                 else:
                     st.error("ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
                     with st.expander("ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°"):
                         st.code(result.stderr + "\n" + result.stdout)
             except Exception as e:
                 st.error(f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")

st.markdown("---")
st.caption("ä½¿ã„æ–¹: ãƒ‡ãƒ¼ã‚¿å–å¾—å¾Œã€å¿…ãš git ã§å¤‰æ›´ã‚’ã‚³ãƒŸãƒƒãƒˆï¼†ãƒ—ãƒƒã‚·ãƒ¥ã—ã¦å…¬é–‹ã‚µã‚¤ãƒˆã«åæ˜ ã•ã›ã¦ãã ã•ã„ã€‚")

# --- ML Management Section ---
st.markdown("---")
st.markdown("## ğŸ¤– æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ç®¡ç† (MLOps)")

# MLflow Instructions
with st.expander("â„¹ï¸ MLflow (å®Ÿé¨“ç®¡ç†) ã®ä½¿ã„æ–¹"):
    st.markdown("""
    å®Ÿé¨“ã®å±¥æ­´ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ç²¾åº¦ã€ãƒ¢ãƒ‡ãƒ«ï¼‰ã¯ **MLflow** ã§è‡ªå‹•è¨˜éŒ²ã•ã‚Œã¦ã„ã¾ã™ã€‚
    è©³ç´°ã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã€ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã„ã¦ãã ã•ã„ã€‚
    ```bash
    mlflow ui
    ```
    (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒ¼ãƒˆ: http://127.0.0.1:5000)
    """)

tab_ml, tab_data, tab_upload = st.tabs(["ğŸ§  ãƒ¢ãƒ‡ãƒ«å­¦ç¿’", "ğŸ› ï¸ ãƒ‡ãƒ¼ã‚¿ç®¡ç†", "ğŸ“¤ ãƒ‡ãƒ—ãƒ­ã‚¤"])

# --- Tab 1: ML (Training & Tuning) ---
with tab_ml:
    st.markdown("### ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®è¨­å®š")
    
    col_conf_1, col_conf_2 = st.columns(2)
    
    with col_conf_1:
        is_tuning = st.checkbox("ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (Optuna) ã‚’å®Ÿè¡Œã™ã‚‹", value=False)
        
        n_trials = 20
        if is_tuning:
            st.info("AIãŒæœ€é©ãªè¨­å®šã‚’æ¢ç´¢ã—ã¦ã‹ã‚‰å­¦ç¿’ã‚’è¡Œã„ã¾ã™ã€‚")
            n_trials = st.slider("è©¦è¡Œå›æ•°", 5, 100, 20)
        else:
            if 'best_params' in st.session_state:
                st.success("âœ… å‰å›ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ã—ã¾ã™ã€‚")
                if st.checkbox("æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç ´æ£„ã—ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã™"):
                    del st.session_state['best_params']
                    st.rerun()
            else:
                st.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§å­¦ç¿’ã—ã¾ã™ã€‚")

        st.markdown("---")
        is_calibrate = st.checkbox("ç¢ºç‡è¼ƒæ­£ (Calibration) ã‚’è¡Œã†", value=False, help="Brier ScoreãŒé«˜ã„å ´åˆã«æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚")
        st.markdown("---")
        auto_push = st.checkbox("å­¦ç¿’å®Œäº†å¾Œã€ãƒªãƒã‚¸ãƒˆãƒªã‚’è‡ªå‹•æ›´æ–° (Git Push)", value=True, help="å­¦ç¿’æˆåŠŸæ™‚ã«å¤‰æ›´ã‚’è‡ªå‹•çš„ã«ã‚³ãƒŸãƒƒãƒˆï¼†ãƒ—ãƒƒã‚·ãƒ¥ã—ã¾ã™")
        skip_preprocess = st.checkbox("å‰å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ— (æ—¢å­˜ã®processed_data.csvã‚’ä½¿ç”¨)", value=False)

    with col_conf_2:
        st.markdown(f"""
        **å®Ÿè¡Œå†…å®¹:**
        1. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç† (æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®åæ˜ )
        2. {'ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (æœ€é©åŒ–)' if is_tuning else 'è¨­å®šã®ç¢ºèª'}
        3. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ (LightGBM) {' + ç¢ºç‡è¼ƒæ­£' if is_calibrate else ''}
        4. {'ãƒªãƒã‚¸ãƒˆãƒªæ›´æ–° (Git Push)' if auto_push else '(ãƒªãƒã‚¸ãƒˆãƒªæ›´æ–°ãªã—)'}
        """)
        
        btn_label = "ğŸ§ª ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ï¼† å­¦ç¿’é–‹å§‹" if is_tuning else "ğŸ§  å­¦ç¿’é–‹å§‹"
        start_process = st.button(btn_label, type="primary")
        




    if start_process:
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
        # Switch paths based on Mode
        if mode_val == "NAR":
            data_path = os.path.join(project_root, "ml", "processed_data_nar.csv")
            db_path = os.path.join(project_root, "data", "raw", "database_nar.csv")
            model_name = "lgbm_model_nar.pkl"
        else:
            data_path = os.path.join(project_root, "ml", "processed_data.csv")
            db_path = os.path.join(project_root, "data", "raw", "database.csv")
            model_name = "lgbm_model.pkl"

        model_dir = os.path.join(project_root, "ml", "models")
        os.makedirs(model_dir, exist_ok=True)
        model_path = os.path.join(model_dir, model_name)

        # 1. Preprocess
        if not skip_preprocess:
            with st.spinner("1/3 ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ä¸­..."):
                if os.path.exists(db_path):
                   feature_engineering.calculate_features(db_path, data_path)
                   
                   # 1.1 Export Stats for Inference
                   import export_stats
                   importlib.reload(export_stats)
                   if export_stats.export_stats(mode=mode_val):
                       st.success("çµ±è¨ˆãƒ‡ãƒ¼ã‚¿(Inferenceç”¨)ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†")
                   else:
                       st.error("çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                else:
                   st.error("database.csvãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                   st.stop()
        else:
            st.info("â„¹ï¸ å‰å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        
        # 1.5 Auto Save to DB
        with st.spinner("1.5 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹(SQL)ã«ä¿å­˜ä¸­..."):
            try:
                import db_helper
                importlib.reload(db_helper)
                
                # Check if processed file exists
                if os.path.exists(data_path):
                    df_proc = pd.read_csv(data_path)
                    
                    db_path_sql = os.path.join(project_root, "keiba_data.db")
                    # Ensure file creation
                    conn_check = importlib.import_module("sqlite3").connect(db_path_sql)
                    conn_check.close()
                    
                    db = db_helper.KeibaDatabase(db_path_sql)
                    db.save_processed_data(df_proc, mode=mode_val)
                    st.success(f"DBè‡ªå‹•ä¿å­˜å®Œäº†: {len(df_proc)} records")
            except Exception as e:
                st.warning(f"DBä¿å­˜å¤±æ•—: {e} (å­¦ç¿’ã¯ç¶šè¡Œã—ã¾ã™)")

        
        # 2. Tuning (if selected)
        if is_tuning:
            with st.spinner(f"2/3 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ ({n_trials} trials)..."):
                try:
                    opt_res = train_model.optimize_hyperparameters(data_path, n_trials=n_trials)
                    if opt_res:
                        st.success(f"æœ€é©åŒ–å®Œäº†ï¼ Best AUC: {opt_res['best_auc']:.4f}")
                        st.session_state['best_params'] = opt_res['best_params']
                    else:
                        st.error("æœ€é©åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§å­¦ç¿’ã‚’ç¶šè¡Œã—ã¾ã™ã€‚")
                except Exception as e:
                     st.error(f"æœ€é©åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        
        # 3. Training
        step_label = "3/3" if is_tuning else "2/2"
        with st.spinner(f"{step_label} ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­..."):
            params = st.session_state.get('best_params', None)
            
            try:
                results = train_model.train_and_save_model(data_path, model_path, params=params, calibrate=is_calibrate)
                if results:
                    st.success("å­¦ç¿’å®Œäº†ï¼")
                    st.session_state['ml_results'] = results
                    
                    # 4. Auto Push
                    if auto_push:
                        st.markdown("---")
                        with st.spinner("ğŸ”„ ãƒªãƒã‚¸ãƒˆãƒªã‚’æ›´æ–°ä¸­ (Git Push)..."):
                            try:
                                # Relative paths for git
                                if mode_val == "NAR":
                                    model_path_rel = "ml/models/lgbm_model_nar.pkl"
                                else:
                                    model_path_rel = "ml/models/lgbm_model.pkl"
                                
                                meta_path_rel = model_path_rel.replace('.pkl', '_meta.json')
                                
                                commit_msg = f"Auto-update model ({mode_val}): {datetime.now().strftime('%Y-%m-%d %H:%M')}"
                                
                                cmds = [
                                    ["git", "add", model_path_rel, meta_path_rel],
                                    # Add processed data if exists? Maybe too large. Skip for now.
                                    ["git", "commit", "-m", commit_msg],
                                    ["git", "push", "origin", "main"]
                                ]
                                
                                for cmd in cmds:
                                    res = subprocess.run(cmd, cwd=project_root, capture_output=True, text=True)
                                    if res.returncode != 0:
                                        # Ignore clean working tree error
                                        if "nothing to commit" not in (res.stdout + res.stderr).lower():
                                            st.warning(f"Git Warning: {res.stderr}")
                                
                                st.success("âœ… ãƒªãƒã‚¸ãƒˆãƒªæ›´æ–°å®Œäº†ï¼")
                            except Exception as e:
                                st.error(f"Git Push Error: {e}")
                                
                else:
                    st.error("å­¦ç¿’ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            except Exception as e:
                st.error(f"å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    # Display Results
    if 'ml_results' in st.session_state:
        st.markdown("---")
        res = st.session_state['ml_results']
        
        st.markdown("#### ğŸ“Š å­¦ç¿’çµæœãƒ¬ãƒãƒ¼ãƒˆ")
        m_col1, m_col2, m_col3 = st.columns(3)
        m_col1.metric("Accuracy", f"{res['accuracy']:.4f}")
        m_col2.metric("AUC", f"{res['auc']:.4f}")
        m_col3.metric("Positive Rate", f"{res.get('win_rate', 0.0):.2%}")
        
        p_col1, p_col2 = st.columns(2)
        with p_col1:
            if 'evals_result' in res and res['evals_result']:
                evals = res['evals_result']
                if 'train' in evals and 'auc' in evals['train']:
                    fig_lc = go.Figure()
                    fig_lc.add_trace(go.Scatter(y=evals['train']['auc'], mode='lines', name='Train AUC'))
                    if 'valid' in evals and 'auc' in evals['valid']:
                        fig_lc.add_trace(go.Scatter(y=evals['valid']['auc'], mode='lines', name='Valid AUC'))
                    fig_lc.update_layout(title="å­¦ç¿’æ›²ç·š (AUC)", xaxis_title="Rounds", yaxis_title="AUC")
                    st.plotly_chart(fig_lc, width="stretch")
            else:
                st.info("å­¦ç¿’å±¥æ­´ãªã—")

        with p_col2:
            if 'feature_importance' in res:
                fi = pd.DataFrame(res['feature_importance'])
                if not fi.empty:
                    fig_fi = go.Figure(go.Bar(
                        x=fi['Value'], y=fi['Feature'], orientation='h'
                    ))
                    fig_fi.update_layout(
                        title="ç‰¹å¾´é‡é‡è¦åº¦ (Top 20)",
                        yaxis=dict(autorange="reversed"),
                        xaxis_title="Importance (Gain)"
                    )
                    st.plotly_chart(fig_fi, width="stretch")
            else:
                st.info("ç‰¹å¾´é‡é‡è¦åº¦ãªã—")




# --- Tab 2: Data Ops ---
with tab_data:
    st.markdown("### ğŸ› ï¸ ãƒ‡ãƒ¼ã‚¿ç®¡ç† (Data Ops)")
    st.info("ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã§ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã®æ‰‹å‹•ä½œæˆã‚„ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜ã‚’è¡Œãˆã¾ã™ã€‚ï¼ˆé€šå¸¸ã¯ã€Œãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã€æ™‚ã«è‡ªå‹•ã§è¡Œã‚ã‚Œã¾ã™ï¼‰")

    col_ops_1, col_ops_2 = st.columns(2)
    
    with col_ops_1:
        if st.button("âš™ï¸ ãƒ‡ãƒ¼ã‚¿åŠ å·¥ (å‰å‡¦ç†) ã®ã¿å®Ÿè¡Œ", help="rawãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã‚’è¨ˆç®—ã—ã€processed_dataã‚’ä½œæˆã—ã¾ã™"):
            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            # Switch paths based on Mode
            if mode_val == "NAR":
                data_path = os.path.join(project_root, "ml", "processed_data_nar.csv")
                db_path = os.path.join(project_root, "data", "raw", "database_nar.csv")
            else:
                data_path = os.path.join(project_root, "ml", "processed_data.csv")
                db_path = os.path.join(project_root, "data", "raw", "database.csv")
            
            with st.spinner("ãƒ‡ãƒ¼ã‚¿åŠ å·¥ä½œæ¥­ä¸­..."):
                if os.path.exists(db_path):
                   feature_engineering.calculate_features(db_path, data_path)
                   st.success(f"å®Œäº†ï¼ ä¿å­˜å…ˆ: {os.path.basename(data_path)}")
                else:
                   st.error("database.csvãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    with col_ops_2:
        if st.button("ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ (SQL) ã«ä¿å­˜", help="processed_dataã‚’SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã™"):
            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            if mode_val == "NAR":
                data_path = os.path.join(project_root, "ml", "processed_data_nar.csv")
            else:
                data_path = os.path.join(project_root, "ml", "processed_data.csv")
            
            db_path_sql = os.path.join(project_root, "keiba_data.db")
            
            if not os.path.exists(data_path):
                st.error(f"å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {os.path.basename(data_path)}")
                st.info("å…ˆã«ã€Œãƒ‡ãƒ¼ã‚¿åŠ å·¥ã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            else:
                with st.spinner("SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ä¸­..."):
                    try:
                        import db_helper
                        importlib.reload(db_helper)
                        
                        df_proc = pd.read_csv(data_path)
                        conn_check = importlib.import_module("sqlite3").connect(db_path_sql)
                        conn_check.close() # Create file
                        
                        db = db_helper.KeibaDatabase(db_path_sql)
                        db.save_processed_data(df_proc, mode=mode_val)
                        
                        st.success(f"ä¿å­˜å®Œäº†ï¼ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {os.path.basename(db_path_sql)}")
                    except Exception as e:
                        st.error(f"SQLä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    # New Section: Global Stats
    st.markdown("---")
    st.markdown("### ğŸ“Š çµ±è¨ˆã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆç®¡ç†")
    st.info("äºˆæ¸¬æ™‚ã«ä½¿ç”¨ã™ã‚‹é¨æ‰‹ãƒ»ã‚³ãƒ¼ã‚¹ãƒ»å©èˆã®çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« (`feature_stats.pkl`) ã‚’æ›´æ–°ã—ã¾ã™ã€‚ã“ã‚Œã«ã¯**å…¨æœŸé–“ï¼ˆ3å¹´é–“ï¼‰ã®Global Historyãƒ‡ãƒ¼ã‚¿**ï¼ˆé¨æ‰‹ç›¸æ€§ã€é©æ€§ã‚¹ã‚³ã‚¢ï¼‰ãŒå«ã¾ã‚Œã¾ã™ã€‚")
    if st.button("ğŸ“ˆ çµ±è¨ˆã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’æ›´æ–° (Export Stats)", help="ml/export_stats.pyã‚’å®Ÿè¡Œã—ã¾ã™"):
         with st.spinner(f"çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—ãƒ»å‡ºåŠ›ä¸­ ({mode_val})..."):
             try:
                 cmd = [sys.executable, "ml/export_stats.py", "--mode", mode_val]
                 res = subprocess.run(cmd, capture_output=True, text=True, cwd=os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
                 
                 st.markdown("#### å®Ÿè¡Œãƒ­ã‚°")
                 st.code(res.stdout)
                 
                 if res.returncode == 0:
                     st.success("çµ±è¨ˆã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®æ›´æ–°ã«æˆåŠŸã—ã¾ã—ãŸï¼")
                 else:
                     st.error("æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                     st.code(res.stderr)
             except Exception as e:
                 st.error(f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")


# --- Tab 3: Upload ---
with tab_upload:
    st.markdown("### ãƒªãƒã‚¸ãƒˆãƒªã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.warning("âš ï¸ Gitã®è¨­å®šï¼ˆSSHéµãªã©ï¼‰ãŒã‚µãƒ¼ãƒãƒ¼ä¸Šã§æ­£ã—ãè¡Œã‚ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
    
    commit_msg = st.text_input("ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸", value=f"Update model: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    
    if st.button("ğŸ“¤ ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (Git Push)"):
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
        # Upload correct model
        if mode_val == "NAR":
             model_path_rel = "ml/models/lgbm_model_nar.pkl"
        else:
             model_path_rel = "ml/models/lgbm_model.pkl"
             
        # Check if exists
        if not os.path.exists(os.path.join(project_root, model_path_rel)):
             st.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path_rel}")
             st.stop()
        
        # Metadata path
        meta_path_rel = model_path_rel.replace('.pkl', '_meta.json')

        cmds = [
            ["git", "add", model_path_rel, meta_path_rel],
            ["git", "commit", "-m", commit_msg],
            ["git", "push", "origin", "main"] # Start with main
        ]
        
        st.markdown("#### å®Ÿè¡Œãƒ­ã‚°")
        status_area = st.empty()
        
        all_success = True
        for cmd in cmds:
            try:
                result = subprocess.run(cmd, cwd=project_root, capture_output=True, text=True)
                if result.returncode == 0:
                    status_area.success(f"OK: {' '.join(cmd)}")
                else:
                    # git commit returns 1 if nothing to commit
                    # Check both stdout and stderr for common "clean" messages
                    out_err = (result.stdout + result.stderr).lower()
                    if "nothing to commit" in out_err or "working tree clean" in out_err or "no changes added to commit" in out_err:
                         status_area.info(f"Info: No changes to commit.")
                    else:
                        status_area.error(f"Error: {' '.join(cmd)}\n{result.stderr}")
                        all_success = False
                        break
            except Exception as e:
                status_area.error(f"Command failed: {e}")
                all_success = False
                break
        
        if all_success:
            st.success("âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼")

