import streamlit as st
import pandas as pd
import os
import sys
import time
from datetime import datetime

# Add scraper directory to path so we can import auto_scraper
sys.path.append(os.path.dirname(__file__))
import auto_scraper

st.set_page_config(page_title="ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ç®¡ç†è€…ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", layout="wide")

st.title("ðŸ› ï¸ ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ç®¡ç†è€…ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")

# Database Path
DB_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), "database.csv")

def get_db_info():
    if not os.path.exists(DB_PATH):
        return None, 0, "ãƒ•ã‚¡ã‚¤ãƒ«ãªã—"
    
    try:
        df = pd.read_csv(DB_PATH)
        if 'æ—¥ä»˜' in df.columns:
            df['date_obj'] = pd.to_datetime(df['æ—¥ä»˜'], format='%Yå¹´%mæœˆ%dæ—¥', errors='coerce')
            last_date = df['date_obj'].max()
            records = len(df)
            return last_date, records, "æ­£å¸¸"
    except Exception as e:
        return None, 0, f"ã‚¨ãƒ©ãƒ¼: {e}"
    return None, 0, "ãƒ‡ãƒ¼ã‚¿ãªã—"

# --- Status Section ---
st.subheader("ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹")
last_date, count, status = get_db_info()

col1, col2, col3 = st.columns(3)
col1.metric("ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", f"{count} ä»¶")
col2.metric("æœ€çµ‚ãƒ‡ãƒ¼ã‚¿æ—¥æ™‚", last_date.strftime('%Y-%m-%d') if last_date else "-")
col3.metric("ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", status)

if last_date:
    st.info(f"æœ€çµ‚æ›´æ–°ãƒ‡ãƒ¼ã‚¿: {last_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} ã®ãƒ¬ãƒ¼ã‚¹ã¾ã§å–å¾—æ¸ˆã¿ã§ã™ã€‚")

# --- Update Section ---
st.subheader("ãƒ‡ãƒ¼ã‚¿æ›´æ–°")
st.write("netkeiba.comã‹ã‚‰æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ã—ã¾ã™ã€‚")

if st.button("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¦æ›´æ–°", type="primary"):
    status_area = st.empty()
    log_area = st.empty()
    
    # Capture stdout to show logs in Streamlit
    import io
    from contextlib import redirect_stdout
    
    f = io.StringIO()
    with redirect_stdout(f):
        # Run scraper
        with st.spinner("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œä¸­..."):
            try:
                # auto_scraper.main() calls sys.exit() or runs indefinitely? 
                # We need to make sure it doesn't kill streamlit.
                # Modified auto_scraper to be importable.
                
                # We can call main() but we need to ensure it doesn't handle args if called directly,
                # or we can pass args manually if modified to accept them.
                # Currently auto_scraper.main() calls get_start_params() which parses sys.argv.
                # We should clear sys.argv to avoid streamlit args interfering.
                old_argv = sys.argv
                sys.argv = ["auto_scraper.py"] # Reset args
                
                auto_scraper.main()
                
                sys.argv = old_argv # Restore
                
                st.success("æ›´æ–°å®Œäº†ï¼")
            except Exception as e:
                st.error(f"å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                
    # Show logs
    logs = f.getvalue()
    log_area.code(logs)
    
    # Reload info
    time.sleep(1)
    st.rerun()

# --- Preview Section ---
if count > 0:
    st.subheader("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ (æœ€æ–°20ä»¶)")
    df = pd.read_csv(DB_PATH)
    if 'æ—¥ä»˜' in df.columns:
         df['date_obj'] = pd.to_datetime(df['æ—¥ä»˜'], format='%Yå¹´%mæœˆ%dæ—¥', errors='coerce')
         st.dataframe(df.sort_values('date_obj', ascending=False).head(20))
    else:
        st.dataframe(df.tail(20))
