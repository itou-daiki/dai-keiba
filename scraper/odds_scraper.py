"""
ã‚ªãƒƒã‚ºå–å¾—ã®å®‰å®šåŒ–
ãƒªãƒˆãƒ©ã‚¤å‡¦ç†ã¨è¤‡æ•°ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®å–å¾—
"""

import requests
from bs4 import BeautifulSoup
import time
from functools import wraps
import json
import os


def retry_on_failure(max_retries=3, delay=2):
    """
    å¤±æ•—æ™‚ã«è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿

    Args:
        max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
        delay: ãƒªãƒˆãƒ©ã‚¤é–“éš”ï¼ˆç§’ï¼‰
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        print(f"  âŒ {func.__name__} failed after {max_retries} attempts: {e}")
                        raise e

                    wait_time = delay * (2 ** attempt)  # Exponential backoff
                    print(f"  âš ï¸ Attempt {attempt + 1} failed, retrying in {wait_time}s...")
                    time.sleep(wait_time)

            return None
        return wrapper
    return decorator


class OddsScraper:
    """
    ã‚ªãƒƒã‚ºå–å¾—ã‚¯ãƒ©ã‚¹ï¼ˆè¤‡æ•°ã‚½ãƒ¼ã‚¹å¯¾å¿œï¼‰
    """

    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        self.cache_file = "odds_cache.json"
        self.cache = self._load_cache()

    def _load_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def _save_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"Cache save failed: {e}")

    @retry_on_failure(max_retries=3, delay=2)
    def _get_odds_netkeiba(self, race_id):
        """
        netkeiba.comã‹ã‚‰ã‚ªãƒƒã‚ºã‚’å–å¾—

        Args:
            race_id: ãƒ¬ãƒ¼ã‚¹ID (ä¾‹: 202506050101)

        Returns:
            dict: {é¦¬ç•ª: ã‚ªãƒƒã‚º}
        """
        url = f"https://race.netkeiba.com/odds/index.html?race_id={race_id}&type=b1"

        time.sleep(1)  # Be polite

        response = requests.get(url, headers=self.headers, timeout=10)
        response.encoding = response.apparent_encoding

        if response.status_code != 200:
            raise Exception(f"HTTP {response.status_code}")

        soup = BeautifulSoup(response.text, 'html.parser')

        odds_dict = {}

        # Find odds table
        # Format varies, but typically:
        # <td class="Waku1"> ... <td class="Odds">2.5</td>

        odds_elements = soup.select('td.Odds')

        if not odds_elements:
            # Try alternative selector
            odds_elements = soup.find_all('td', string=lambda t: t and '.' in str(t))

        # Extract odds
        umaban = 1
        for elem in odds_elements:
            try:
                odds_text = elem.get_text(strip=True)
                odds_value = float(odds_text)
                odds_dict[umaban] = odds_value
                umaban += 1
            except ValueError:
                continue

        if not odds_dict:
            raise Exception("No odds found in netkeiba")

        return odds_dict

    @retry_on_failure(max_retries=2, delay=3)
    def _get_odds_jra(self, race_id):
        """
        JRAå…¬å¼ã‹ã‚‰ã‚ªãƒƒã‚ºã‚’å–å¾—ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰

        Note: JRAå…¬å¼ã®URLã¯å®Ÿéš›ã«ã¯ç•°ãªã‚‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        ã“ã“ã§ã¯æ¦‚å¿µçš„ãªå®Ÿè£…ä¾‹

        Args:
            race_id: ãƒ¬ãƒ¼ã‚¹ID

        Returns:
            dict: {é¦¬ç•ª: ã‚ªãƒƒã‚º}
        """
        # JRAã®race_idãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯ç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹
        # å®Ÿè£…ä¾‹ã¨ã—ã¦ç°¡ç•¥åŒ–

        # Example: https://www.jra.go.jp/JRADB/accessO.html?CNAME=pw01oli00/...
        # å®Ÿéš›ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¯èª¿æŸ»ãŒå¿…è¦

        print("  â„¹ï¸ JRA official source not fully implemented (placeholder)")

        # Placeholder: return empty
        return {}

    def get_odds_multi_source(self, race_id):
        """
        è¤‡æ•°ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚ªãƒƒã‚ºã‚’å–å¾—

        Args:
            race_id: ãƒ¬ãƒ¼ã‚¹ID

        Returns:
            dict: {é¦¬ç•ª: ã‚ªãƒƒã‚º} or None
        """
        print(f"ğŸ“Š Fetching odds for race {race_id}...")

        # Check cache first
        if race_id in self.cache:
            print("  âœ… Using cached odds")
            return self.cache[race_id]

        odds = None

        # Priority 1: netkeiba
        try:
            print("  Trying netkeiba.com...")
            odds = self._get_odds_netkeiba(race_id)
            if odds:
                print(f"  âœ… Got odds from netkeiba ({len(odds)} horses)")
                self.cache[race_id] = odds
                self._save_cache()
                return odds
        except Exception as e:
            print(f"  âš ï¸ netkeiba failed: {e}")

        # Priority 2: JRA official (placeholder)
        try:
            print("  Trying JRA official...")
            odds = self._get_odds_jra(race_id)
            if odds:
                print(f"  âœ… Got odds from JRA ({len(odds)} horses)")
                self.cache[race_id] = odds
                self._save_cache()
                return odds
        except Exception as e:
            print(f"  âš ï¸ JRA failed: {e}")

        # Priority 3: Use cached odds from previous fetch (if available)
        if race_id in self.cache:
            print("  âš ï¸ Using stale cached odds")
            return self.cache[race_id]

        print("  âŒ All sources failed")
        return None

    def get_odds_batch(self, race_ids):
        """
        è¤‡æ•°ãƒ¬ãƒ¼ã‚¹ã®ã‚ªãƒƒã‚ºã‚’ä¸€æ‹¬å–å¾—

        Args:
            race_ids: ãƒ¬ãƒ¼ã‚¹IDã®ãƒªã‚¹ãƒˆ

        Returns:
            dict: {race_id: {é¦¬ç•ª: ã‚ªãƒƒã‚º}}
        """
        results = {}

        for i, race_id in enumerate(race_ids, 1):
            print(f"\n[{i}/{len(race_ids)}] Processing {race_id}...")

            odds = self.get_odds_multi_source(race_id)
            if odds:
                results[race_id] = odds
            else:
                results[race_id] = {}

            # Polite delay between requests
            if i < len(race_ids):
                time.sleep(2)

        return results


if __name__ == "__main__":
    # Test
    scraper = OddsScraper()

    # Example race ID (adjust to a real one for testing)
    test_race_id = "202506050101"

    odds = scraper.get_odds_multi_source(test_race_id)

    if odds:
        print("\nâœ… Odds retrieved successfully:")
        for umaban, odd in odds.items():
            print(f"  é¦¬ç•ª {umaban}: {odd}å€")
    else:
        print("\nâŒ Failed to retrieve odds")
